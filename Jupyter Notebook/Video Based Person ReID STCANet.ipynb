{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a jupyter notebook file for the implementation of paper **\"Multi-Camera Person Re-Identification using Spatiotemporal Context Modeling\"**<br><br>\n",
    "Author: Fatima Zulfiqar, Usama Ijaz Bajwa, Rana Hammad Raza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fuEfvKPh05W"
   },
   "source": [
    "# Dataset Manager Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlAYSODgnAXP"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import os\n",
    "import sys\n",
    "import errno\n",
    "import shutil\n",
    "import json\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "\n",
    "def mkdir_if_missing(directory):\n",
    "    if not osp.exists(directory):\n",
    "        try:\n",
    "            os.makedirs(directory)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value.\n",
    "       \n",
    "       Code imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def save_checkpoint(state, is_best, fpath='checkpoint.pth.tar'):\n",
    "    mkdir_if_missing(osp.dirname(fpath))\n",
    "    torch.save(state, fpath)\n",
    "    if is_best:\n",
    "        shutil.copy(fpath, osp.join(osp.dirname(fpath), 'best_model.pth.tar'))\n",
    "\n",
    "class Logger(object):\n",
    "    \"\"\"\n",
    "    Write console output to external text file.\n",
    "    Code imported from https://github.com/Cysu/open-reid/blob/master/reid/utils/logging.py.\n",
    "    \"\"\"\n",
    "    def __init__(self, fpath=None, mode='w'):\n",
    "        self.console = sys.stdout\n",
    "        self.file = None\n",
    "        if fpath is not None:\n",
    "            mkdir_if_missing(os.path.dirname(fpath))\n",
    "            self.file = open(fpath, mode)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "    def __enter__(self):\n",
    "        pass\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.close()\n",
    "\n",
    "    def write(self, msg):\n",
    "        self.console.write(msg)\n",
    "        if self.file is not None:\n",
    "            self.file.write(msg)\n",
    "\n",
    "    def flush(self):\n",
    "        self.console.flush()\n",
    "        if self.file is not None:\n",
    "            self.file.flush()\n",
    "            os.fsync(self.file.fileno())\n",
    "\n",
    "    def close(self):\n",
    "        self.console.close()\n",
    "        if self.file is not None:\n",
    "            self.file.close()\n",
    "\n",
    "def read_json(fpath):\n",
    "    with open(fpath, 'r') as f:\n",
    "        obj = json.load(f)\n",
    "    return obj\n",
    "\n",
    "def write_json(obj, fpath):\n",
    "    mkdir_if_missing(osp.dirname(fpath))\n",
    "    with open(fpath, 'w') as f:\n",
    "        json.dump(obj, f, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrD5ZZzah05X"
   },
   "source": [
    "# Load DukeMTMC-VideoReID Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1853,
     "status": "ok",
     "timestamp": 1637082409405,
     "user": {
      "displayName": "Colab Pro",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12358552435453919041"
     },
     "user_tz": -300
    },
    "id": "a2rslJwLh05X",
    "outputId": "91e4a033-8d57-438d-e7b2-55a5d7ab7361"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import sys\n",
    "import urllib\n",
    "import tarfile\n",
    "import zipfile\n",
    "import os.path as osp\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "#from .utils import mkdir_if_missing, write_json, read_json\n",
    "\n",
    "\n",
    "class DukeMTMCVidReID(object):\n",
    "    \"\"\"\n",
    "    DukeMTMCVidReID\n",
    "    Reference:\n",
    "    Wu et al. Exploit the Unknown Gradually: One-Shot Video-Based Person\n",
    "    Re-Identification by Stepwise Learning. CVPR 2018.\n",
    "    URL: https://github.com/Yu-Wu/DukeMTMC-VideoReID\n",
    "    \n",
    "    Dataset statistics:\n",
    "    # identities: 702 (train) + 702 (test)\n",
    "    # tracklets: 2196 (train) + 2636 (test)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root = './DukeMTMC-VideoReID', min_seq_len=0, verbose=True, **kwargs): # set path to dataset in root\n",
    "        self.dataset_dir = root\n",
    "        self.train_dir = osp.join(self.dataset_dir, 'train')\n",
    "        self.query_dir = osp.join(self.dataset_dir, 'query')\n",
    "        self.gallery_dir = osp.join(self.dataset_dir, 'gallery')\n",
    "        self.split_train_json_path = osp.join(self.dataset_dir, 'split_train.json')\n",
    "        self.split_train_dense_json_path = osp.join(self.dataset_dir, 'split_train_dense.json')\n",
    "        self.split_query_json_path = osp.join(self.dataset_dir, 'split_query.json')\n",
    "        self.split_gallery_json_path = osp.join(self.dataset_dir, 'split_gallery.json')\n",
    "\n",
    "        self.min_seq_len = min_seq_len\n",
    "        self._check_before_run()\n",
    "        print(\"Note: if root path is changed, the previously generated json files need to be re-generated (so delete them first)\")\n",
    "\n",
    "        train, num_train_tracklets, num_train_pids, num_imgs_train = \\\n",
    "          self._process_dir(self.train_dir, self.split_train_json_path, relabel=True)\n",
    "        train_dense, num_train_tracklets_dense, num_train_pids_dense, num_imgs_train_dense = \\\n",
    "          self._process_dir_dense(self.train_dir, self.split_train_dense_json_path, relabel=True, sampling_step=32)\n",
    "        query, num_query_tracklets, num_query_pids, num_imgs_query = \\\n",
    "          self._process_dir(self.query_dir, self.split_query_json_path, relabel=False)\n",
    "        gallery, num_gallery_tracklets, num_gallery_pids, num_imgs_gallery = \\\n",
    "          self._process_dir(self.gallery_dir, self.split_gallery_json_path, relabel=False)\n",
    "\n",
    "        print(\"the number of tracklets under dense sampling for train set: {}\".format(num_train_tracklets_dense))\n",
    "\n",
    "        num_imgs_per_tracklet = num_imgs_train + num_imgs_query + num_imgs_gallery\n",
    "        min_num = np.min(num_imgs_per_tracklet)\n",
    "        max_num = np.max(num_imgs_per_tracklet)\n",
    "        avg_num = np.mean(num_imgs_per_tracklet)\n",
    "\n",
    "        num_total_pids = num_train_pids + num_query_pids\n",
    "        num_total_tracklets = num_train_tracklets + num_query_tracklets + num_gallery_tracklets\n",
    "\n",
    "        if verbose:\n",
    "            print(\"=> DukeMTMC-VideoReID loaded\")\n",
    "            print(\"Dataset statistics:\")\n",
    "            print(\"  ------------------------------\")\n",
    "            print(\"  subset         | # ids | # tracklets\")\n",
    "            print(\"  ------------------------------\")\n",
    "            print(\"  train          | {:5d} | {:8d}\".format(num_train_pids, num_train_tracklets))\n",
    "            print(\"  train_dense    | {:5d} | {:8d}\".format(num_train_pids_dense, num_train_tracklets_dense))\n",
    "            print(\"  query          | {:5d} | {:8d}\".format(num_query_pids, num_query_tracklets))\n",
    "            print(\"  gallery        | {:5d} | {:8d}\".format(num_gallery_pids, num_gallery_tracklets))\n",
    "            print(\"  ------------------------------\")\n",
    "            print(\"  total          | {:5d} | {:8d}\".format(num_total_pids, num_total_tracklets))\n",
    "            print(\"  number of images per tracklet: {} ~ {}, average {:.1f}\".format(min_num, max_num, avg_num))\n",
    "            print(\"  ------------------------------\")\n",
    "\n",
    "        self.train = train\n",
    "        self.train_dense = train_dense\n",
    "        self.query = query\n",
    "        self.gallery = gallery\n",
    "\n",
    "        self.num_train_pids = num_train_pids\n",
    "        self.num_query_pids = num_query_pids\n",
    "        self.num_gallery_pids = num_gallery_pids\n",
    "\n",
    "\n",
    "    def _check_before_run(self):\n",
    "        \"\"\"Check if all files are available before going deeper\"\"\"\n",
    "        if not osp.exists(self.dataset_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.dataset_dir))\n",
    "        if not osp.exists(self.train_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.train_dir))\n",
    "        if not osp.exists(self.query_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.query_dir))\n",
    "        if not osp.exists(self.gallery_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.gallery_dir))\n",
    "\n",
    "    def _process_dir(self, dir_path, json_path, relabel):\n",
    "        if osp.exists(json_path):\n",
    "            print(\"=> {} generated before, awesome!\".format(json_path))\n",
    "            split = read_json(json_path)\n",
    "            return split['tracklets'], split['num_tracklets'], split['num_pids'], split['num_imgs_per_tracklet']\n",
    "\n",
    "        print(\"=> Automatically generating split (might take a while for the first time, have a coffe)\")\n",
    "        pdirs = glob.glob(osp.join(dir_path, '*')) # avoid .DS_Store\n",
    "        print(\"Processing {} with {} person identities\".format(dir_path, len(pdirs)))\n",
    "\n",
    "        pid_container = set()\n",
    "        for pdir in pdirs:\n",
    "            pid = int(osp.basename(pdir))\n",
    "            pid_container.add(pid)\n",
    "        pid2label = {pid:label for label, pid in enumerate(pid_container)}\n",
    "\n",
    "        tracklets = []\n",
    "        num_imgs_per_tracklet = []\n",
    "        for pdir in pdirs:\n",
    "            pid = int(osp.basename(pdir))\n",
    "            if relabel: pid = pid2label[pid]\n",
    "            tdirs = glob.glob(osp.join(pdir, '*'))\n",
    "            for tdir in tdirs:\n",
    "                raw_img_paths = glob.glob(osp.join(tdir, '*.jpg'))\n",
    "                num_imgs = len(raw_img_paths)\n",
    "\n",
    "                if num_imgs < self.min_seq_len:\n",
    "                    continue\n",
    "\n",
    "                num_imgs_per_tracklet.append(num_imgs)\n",
    "                img_paths = []\n",
    "                for img_idx in range(num_imgs):\n",
    "                    # some tracklet starts from 0002 instead of 0001\n",
    "                    img_idx_name = 'F' + str(img_idx+1).zfill(4)\n",
    "                    res = glob.glob(osp.join(tdir, '*' + img_idx_name + '*.jpg'))\n",
    "                    if len(res) == 0:\n",
    "                        print(\"Warn: index name {} in {} is missing, jump to next\".format(img_idx_name, tdir))\n",
    "                        continue\n",
    "                    img_paths.append(res[0])\n",
    "                img_name = osp.basename(img_paths[0])\n",
    "                if img_name.find('_') == -1:\n",
    "                    # old naming format: 0001C6F0099X30823.jpg\n",
    "                    camid = int(img_name[5]) - 1\n",
    "                else:\n",
    "                    # new naming format: 0001_C6_F0099_X30823.jpg\n",
    "                    camid = int(img_name[6]) - 1\n",
    "                img_paths = tuple(img_paths)\n",
    "                tracklets.append((img_paths, pid, camid))\n",
    "\n",
    "        num_pids = len(pid_container)\n",
    "        num_tracklets = len(tracklets)\n",
    "\n",
    "        print(\"Saving split to {}\".format(json_path))\n",
    "        split_dict = {\n",
    "            'tracklets': tracklets,\n",
    "            'num_tracklets': num_tracklets,\n",
    "            'num_pids': num_pids,\n",
    "            'num_imgs_per_tracklet': num_imgs_per_tracklet,\n",
    "        }\n",
    "        write_json(split_dict, json_path)\n",
    "\n",
    "        return tracklets, num_tracklets, num_pids, num_imgs_per_tracklet\n",
    "\n",
    "    def _process_dir_dense(self, dir_path, json_path, relabel, sampling_step=32):\n",
    "        if osp.exists(json_path):\n",
    "            print(\"=> {} generated before, awesome!\".format(json_path))\n",
    "            split = read_json(json_path)\n",
    "            return split['tracklets'], split['num_tracklets'], split['num_pids'], split['num_imgs_per_tracklet']\n",
    "\n",
    "        print(\"=> Automatically generating split (might take a while for the first time, have a coffe)\")\n",
    "        pdirs = glob.glob(osp.join(dir_path, '*')) # avoid .DS_Store\n",
    "        print(\"Processing {} with {} person identities\".format(dir_path, len(pdirs)))\n",
    "\n",
    "        pid_container = set()\n",
    "        for pdir in pdirs:\n",
    "            pid = int(osp.basename(pdir))\n",
    "            pid_container.add(pid)\n",
    "        pid2label = {pid:label for label, pid in enumerate(pid_container)}\n",
    "\n",
    "        tracklets = []\n",
    "        num_imgs_per_tracklet = []\n",
    "        for pdir in pdirs:\n",
    "            pid = int(osp.basename(pdir))\n",
    "            if relabel: pid = pid2label[pid]\n",
    "            tdirs = glob.glob(osp.join(pdir, '*'))\n",
    "            for tdir in tdirs:\n",
    "                raw_img_paths = glob.glob(osp.join(tdir, '*.jpg'))\n",
    "                num_imgs = len(raw_img_paths)\n",
    "\n",
    "                if num_imgs < self.min_seq_len:\n",
    "                    continue\n",
    "\n",
    "                num_imgs_per_tracklet.append(num_imgs)\n",
    "                img_paths = []\n",
    "                for img_idx in range(num_imgs):\n",
    "                    # some tracklet starts from 0002 instead of 0001\n",
    "                    img_idx_name = 'F' + str(img_idx+1).zfill(4)\n",
    "                    res = glob.glob(osp.join(tdir, '*' + img_idx_name + '*.jpg'))\n",
    "                    if len(res) == 0:\n",
    "                        print(\"Warn: index name {} in {} is missing, jump to next\".format(img_idx_name, tdir))\n",
    "                        continue\n",
    "                    img_paths.append(res[0])\n",
    "                img_name = osp.basename(img_paths[0])\n",
    "                if img_name.find('_') == -1:\n",
    "                    # old naming format: 0001C6F0099X30823.jpg\n",
    "                    camid = int(img_name[5]) - 1\n",
    "                else:\n",
    "                    # new naming format: 0001_C6_F0099_X30823.jpg\n",
    "                    camid = int(img_name[6]) - 1\n",
    "                img_paths = tuple(img_paths)\n",
    "\n",
    "                # dense sampling\n",
    "                num_sampling = len(img_paths)//sampling_step\n",
    "                if num_sampling == 0:\n",
    "                    tracklets.append((img_paths, pid, camid))\n",
    "                else:\n",
    "                    for idx in range(num_sampling):\n",
    "                        if idx == num_sampling - 1:\n",
    "                            tracklets.append((img_paths[idx*sampling_step:], pid, camid))\n",
    "                        else:\n",
    "                            tracklets.append((img_paths[idx*sampling_step : (idx+1)*sampling_step], pid, camid))\n",
    "\n",
    "        num_pids = len(pid_container)\n",
    "        num_tracklets = len(tracklets)\n",
    "\n",
    "        print(\"Saving split to {}\".format(json_path))\n",
    "        split_dict = {\n",
    "            'tracklets': tracklets,\n",
    "            'num_tracklets': num_tracklets,\n",
    "            'num_pids': num_pids,\n",
    "            'num_imgs_per_tracklet': num_imgs_per_tracklet,\n",
    "        }\n",
    "        write_json(split_dict, json_path)\n",
    "\n",
    "        return tracklets, num_tracklets, num_pids, num_imgs_per_tracklet\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # test\n",
    "    dataset = DukeMTMCVidReID()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4q3SHZoh05a"
   },
   "source": [
    "# Load MARS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4656,
     "status": "ok",
     "timestamp": 1637116171355,
     "user": {
      "displayName": "Colab Pro",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12358552435453919041"
     },
     "user_tz": -300
    },
    "id": "KJDMYyfqh05a",
    "outputId": "1fd4a994-c7d6-4322-82d8-ba3060220991"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import sys\n",
    "import urllib\n",
    "import tarfile\n",
    "import zipfile\n",
    "import os.path as osp\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"Dataset classes\"\"\"\n",
    "\n",
    "\n",
    "class Mars(object):\n",
    "    \"\"\"\n",
    "    MARS\n",
    "    Reference:\n",
    "    Zheng et al. MARS: A Video Benchmark for Large-Scale Person Re-identification. ECCV 2016.\n",
    "    \n",
    "    Dataset statistics:\n",
    "    # identities: 1261\n",
    "    # tracklets: 8298 (train) + 1980 (query) + 11310 (gallery)\n",
    "    # cameras: 6\n",
    "    Note: \n",
    "    # gallery set must contain query set, otherwise 140 query imgs will not have ground truth.\n",
    "    # gallery imgs with label=-1 can be remove, which do not influence on final performance.\n",
    "    Args:\n",
    "        min_seq_len (int): tracklet with length shorter than this value will be discarded (default: 0).\n",
    "    \"\"\"\n",
    "    def __init__(self, root='./content', min_seq_len=0, **kwargs): # set path to dataset in root\n",
    "\n",
    "        self.root = root\n",
    "        self.train_name_path = osp.join(self.root, 'info/train_name.txt')\n",
    "        self.test_name_path = osp.join(self.root, 'info/test_name.txt')\n",
    "        self.track_train_info_path = osp.join(self.root, 'info/tracks_train_info.mat')\n",
    "        self.track_test_info_path = osp.join(self.root, 'info/tracks_test_info.mat')\n",
    "        self.query_IDX_path = osp.join(self.root, 'info/query_IDX.mat')\n",
    "\n",
    "        self._check_before_run()\n",
    "\n",
    "        # prepare meta data\n",
    "        train_names = self._get_names(self.train_name_path)\n",
    "        test_names = self._get_names(self.test_name_path)\n",
    "        track_train = loadmat(self.track_train_info_path)['track_train_info'] # numpy.ndarray (8298, 4)\n",
    "        track_test = loadmat(self.track_test_info_path)['track_test_info'] # numpy.ndarray (12180, 4)\n",
    "        query_IDX = loadmat(self.query_IDX_path)['query_IDX'].squeeze() # numpy.ndarray (1980,)\n",
    "        query_IDX -= 1 # index from 0\n",
    "        track_query = track_test[query_IDX,:]\n",
    "        gallery_IDX = [i for i in range(track_test.shape[0]) if i not in query_IDX]\n",
    "        track_gallery = track_test[gallery_IDX,:]\n",
    "        # track_gallery = track_test\n",
    "\n",
    "        train, num_train_tracklets, num_train_pids, num_train_imgs = \\\n",
    "          self._process_data(train_names, track_train, home_dir='bbox_train', relabel=True, min_seq_len=min_seq_len)\n",
    "\n",
    "        query, num_query_tracklets, num_query_pids, num_query_imgs = \\\n",
    "          self._process_data(test_names, track_query, home_dir='bbox_test', relabel=False, min_seq_len=min_seq_len)\n",
    "\n",
    "        gallery, num_gallery_tracklets, num_gallery_pids, num_gallery_imgs = \\\n",
    "          self._process_data(test_names, track_gallery, home_dir='bbox_test', relabel=False, min_seq_len=min_seq_len)\n",
    "\n",
    "        num_imgs_per_tracklet = num_train_imgs + num_gallery_imgs + num_query_imgs\n",
    "        min_num = np.min(num_imgs_per_tracklet)\n",
    "        max_num = np.max(num_imgs_per_tracklet)\n",
    "        avg_num = np.mean(num_imgs_per_tracklet)\n",
    "\n",
    "        num_total_pids = num_train_pids + num_gallery_pids\n",
    "        num_total_tracklets = num_train_tracklets + num_gallery_tracklets + num_query_tracklets\n",
    "\n",
    "        print(\"=> MARS loaded\")\n",
    "        print(\"Dataset statistics:\")\n",
    "        print(\"  ------------------------------\")\n",
    "        print(\"  subset   | # ids | # tracklets\")\n",
    "        print(\"  ------------------------------\")\n",
    "        print(\"  train    | {:5d} | {:8d}\".format(num_train_pids, num_train_tracklets))\n",
    "        print(\"  query    | {:5d} | {:8d}\".format(num_query_pids, num_query_tracklets))\n",
    "        print(\"  gallery  | {:5d} | {:8d}\".format(num_gallery_pids, num_gallery_tracklets))\n",
    "        print(\"  ------------------------------\")\n",
    "        print(\"  total    | {:5d} | {:8d}\".format(num_total_pids, num_total_tracklets))\n",
    "        print(\"  number of images per tracklet: {} ~ {}, average {:.1f}\".format(min_num, max_num, avg_num))\n",
    "        print(\"  ------------------------------\")\n",
    "\n",
    "        self.train = train\n",
    "        self.query = query\n",
    "        self.gallery = gallery\n",
    "\n",
    "        self.num_train_pids = num_train_pids\n",
    "        self.num_query_pids = num_query_pids\n",
    "        self.num_gallery_pids = num_gallery_pids\n",
    "\n",
    "    def _check_before_run(self):\n",
    "        \"\"\"Check if all files are available before going deeper\"\"\"\n",
    "        if not osp.exists(self.root):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.root))\n",
    "        if not osp.exists(self.train_name_path):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.train_name_path))\n",
    "        if not osp.exists(self.test_name_path):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.test_name_path))\n",
    "        if not osp.exists(self.track_train_info_path):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.track_train_info_path))\n",
    "        if not osp.exists(self.track_test_info_path):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.track_test_info_path))\n",
    "        if not osp.exists(self.query_IDX_path):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.query_IDX_path))\n",
    "\n",
    "    def _get_names(self, fpath):\n",
    "        names = []\n",
    "        with open(fpath, 'r') as f:\n",
    "            for line in f:\n",
    "                new_line = line.rstrip()\n",
    "                names.append(new_line)\n",
    "        return names\n",
    "\n",
    "    def _process_data(self, names, meta_data, home_dir=None, relabel=False, min_seq_len=0):\n",
    "        assert home_dir in ['bbox_train', 'bbox_test']\n",
    "        num_tracklets = meta_data.shape[0]\n",
    "        pid_list = list(set(meta_data[:,2].tolist()))\n",
    "        num_pids = len(pid_list)\n",
    "\n",
    "        if relabel: pid2label = {pid:label for label, pid in enumerate(pid_list)}\n",
    "        tracklets = []\n",
    "        num_imgs_per_tracklet = []\n",
    "\n",
    "        for tracklet_idx in range(num_tracklets):\n",
    "            data = meta_data[tracklet_idx,...]\n",
    "            start_index, end_index, pid, camid = data\n",
    "            if pid == -1: continue # junk images are just ignored\n",
    "            assert 1 <= camid <= 6\n",
    "            if relabel: pid = pid2label[pid]\n",
    "            camid -= 1 # index starts from 0\n",
    "            img_names = names[start_index-1:end_index]\n",
    "\n",
    "            # make sure image names correspond to the same person\n",
    "            pnames = [img_name[:4] for img_name in img_names]\n",
    "            assert len(set(pnames)) == 1, \"Error: a single tracklet contains different person images\"\n",
    "\n",
    "            # make sure all images are captured under the same camera\n",
    "            camnames = [img_name[5] for img_name in img_names]\n",
    "            assert len(set(camnames)) == 1, \"Error: images are captured under different cameras!\"\n",
    "\n",
    "            # append image names with directory information\n",
    "            img_paths = [osp.join(self.root, home_dir, home_dir, img_name[:4], img_name) for img_name in img_names]\n",
    "            if len(img_paths) >= min_seq_len:\n",
    "                img_paths = tuple(img_paths)\n",
    "                tracklets.append((img_paths, pid, camid))\n",
    "                num_imgs_per_tracklet.append(len(img_paths))\n",
    "\n",
    "        num_tracklets = len(tracklets)\n",
    "\n",
    "        return tracklets, num_tracklets, num_pids, num_imgs_per_tracklet\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Mars()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DQv5SR9h05i"
   },
   "source": [
    "# Dataset Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3n9z4tkh05j"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "__vidreid_factory = {\n",
    "    'mars': Mars\n",
    "    #'duke': DukeMTMCVidReID\n",
    "}\n",
    "\n",
    "\n",
    "def get_names():\n",
    "    return list(__vidreid_factory.keys())\n",
    "\n",
    "def init_dataset(name, **kwargs):\n",
    "    if name not in list(__vidreid_factory.keys()):\n",
    "        raise KeyError(\"Invalid dataset, got '{}', but expected to be one of {}\".format(name, list(__vidreid_factory.keys())))\n",
    "    return __vidreid_factory[name](**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEjVnrZGh05j"
   },
   "source": [
    "# models Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bBUrMBuohsG"
   },
   "source": [
    "## Spatio-temporal Attention Module (STAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9rTFZ6dGokOf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class Gconv(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Gconv, self).__init__()\n",
    "        fsm_blocks = []\n",
    "        fsm_blocks.append(nn.Conv2d(in_channels * 2, in_channels, 1))\n",
    "        fsm_blocks.append(nn.BatchNorm2d(in_channels))\n",
    "        fsm_blocks.append(nn.ReLU(inplace=True))\n",
    "        self.fsm = nn.Sequential(*fsm_blocks)\n",
    "        # init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, W, x):\n",
    "        bs, n, c = x.size()\n",
    "\n",
    "        x_neighbor = torch.bmm(W, x) \n",
    "        x = torch.cat([x, x_neighbor], 2) \n",
    "        x = x.view(-1, x.size(2), 1, 1) \n",
    "        x = self.fsm(x) \n",
    "        x = x.view(bs, n, c)\n",
    "        return x \n",
    "\n",
    "\n",
    "class Wcompute(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Wcompute, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        edge_block = []\n",
    "        edge_block.append(nn.Conv2d(in_channels * 2, 1, 1))\n",
    "        edge_block.append(nn.BatchNorm2d(1))\n",
    "        self.relation = nn.Sequential(*edge_block)\n",
    "\n",
    "        #init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "        \n",
    "\n",
    "    def forward(self, x, W_id, y):\n",
    "        bs, N, C = x.size()\n",
    "\n",
    "        W1 = x.unsqueeze(2) \n",
    "        W2 = torch.transpose(W1, 1, 2) \n",
    "        W_new = torch.abs(W1 - W2) \n",
    "        W_new = torch.transpose(W_new, 1, 3) \n",
    "        y = y.view(bs, C, 1, 1).expand_as(W_new)\n",
    "        W_new = torch.cat((W_new, y), 1) \n",
    "\n",
    "        W_new = self.relation(W_new) \n",
    "        W_new = torch.transpose(W_new, 1, 3)  \n",
    "        W_new = W_new.squeeze(3) \n",
    "\n",
    "        W_new = W_new - W_id.expand_as(W_new) * 1e8\n",
    "        W_new = F.softmax(W_new, dim=2)\n",
    "        return W_new\n",
    "\n",
    "\n",
    "class STAModule(nn.Module):\n",
    "    def __init__(self, in_channels, T, N=4):\n",
    "        super(STAModule, self).__init__()\n",
    "        self.T = T\n",
    "        self.in_channels = in_channels\n",
    "        self.module_w = Wcompute(in_channels)\n",
    "        self.module_l = Gconv(in_channels)\n",
    "\n",
    "        W0 = torch.eye(N)\n",
    "        W = (1 - W0).repeat(T, T)\n",
    "        for i in range(T):\n",
    "            W[i*N: (i+1)*N, i*N: (i+1)*N] = W0\n",
    "        self.W_init = W\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        bs, t, N, C = x.size()\n",
    "        x = x.view(bs, -1, C)\n",
    "\n",
    "        if t == self.T:\n",
    "            W_init = self.W_init \n",
    "        else:\n",
    "            W_init = self.W_init[:t*N, :t*N]\n",
    "\n",
    "        W = self.module_w(x, W_init.unsqueeze(0).cuda(), y) \n",
    "        s = self.module_l(W, x) \n",
    "        s = s.view(bs, t, N, C)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZuZiFLKh05k"
   },
   "source": [
    "# Interconnection, Accumulation Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzGkvPmEh05k"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_grid(h, w):\n",
    "    x = np.linspace(0, w-1, w)\n",
    "    y = np.linspace(0, h-1, h)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    xv = xv.flatten()\n",
    "    yv = yv.flatten()\n",
    "    return xv, yv\n",
    "\n",
    "def generate_gaussian(height, width, alpha_x, alpha_y):\n",
    "    Dis = np.zeros((height*width, height*width))\n",
    "    xv, yv = generate_grid(height, width)\n",
    "    for i in range(0, width):\n",
    "        for j in range(0, height):\n",
    "            d = (np.square(xv - i))/ (2 * alpha_x**2)  + (np.square(yv - j)) / (2 * alpha_y**2)\n",
    "            Dis[i+j*width] = -1 *  d \n",
    "    Dis = torch.from_numpy(Dis).float()\n",
    "    Dis = F.softmax(Dis, dim=-1)\n",
    "    return Dis\n",
    "\n",
    "# Interonnection and accumulation operation\n",
    "class _IABlockND(nn.Module):\n",
    "    def __init__(self, in_channels, height, width,\n",
    "            alpha_x, alpha_y):\n",
    "        super(_IABlockND, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        conv_nd = nn.Conv2d\n",
    "        max_pool = nn.MaxPool2d\n",
    "        bn = nn.BatchNorm2d\n",
    "\n",
    "        self.Dis = generate_gaussian(height=height, width=width, alpha_x=alpha_x, alpha_y=alpha_y)\n",
    "        self.W1 = bn(self.in_channels)\n",
    "        self.W2 = bn(self.in_channels)\n",
    "        \n",
    "        # init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, conv_nd):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, bn):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        nn.init.constant_(self.W1.weight.data, 0.0)\n",
    "        nn.init.constant_(self.W1.bias.data, 0.0)\n",
    "        nn.init.constant_(self.W2.weight.data, 0.0)\n",
    "        nn.init.constant_(self.W2.bias.data, 0.0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: (b, c, h, w)\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        g_x = x.view(batch_size, self.in_channels, -1)\n",
    "        g_x = g_x.permute(0, 2, 1)\n",
    "\n",
    "        f_cluster = []\n",
    "        f_loc = torch.unsqueeze(self.Dis.cuda(), 0)\n",
    "        f_loc = f_loc.expand(batch_size, -1, -1)\n",
    "        f_cluster.append(torch.unsqueeze(f_loc, 1))\n",
    "\n",
    "        theta_x = x.view(batch_size, self.in_channels, -1)\n",
    "        theta_x = theta_x.permute(0, 2, 1) #[B, H*W, C]\n",
    "        phi_x = x.view(batch_size, self.in_channels, -1)\n",
    "        f = torch.matmul(theta_x, phi_x) #[B, H*W, H*W]\n",
    "        f = f / np.sqrt(self.in_channels)\n",
    "        f = F.softmax(f, dim=-1)\n",
    "        f_cluster.append(torch.unsqueeze(f, 1))\n",
    "        \n",
    "        f_cluster = torch.cat(f_cluster, 1)\n",
    "        f = torch.prod(f_cluster, dim=1)\n",
    "        f = F.softmax(f, dim=-1)\n",
    "\n",
    "        y = torch.matmul(f, g_x)\n",
    "        y = y.permute(0, 2, 1).contiguous()\n",
    "        y = y.view(batch_size, self.in_channels, *x.size()[2:])\n",
    "        y = self.W1(y)\n",
    "        z = y + x\n",
    "\n",
    "        x = z\n",
    "        g_x = x.view(batch_size, self.in_channels, -1) #[B, c, h*w]\n",
    "        theta_x = g_x #[B, c, h*w]\n",
    "        phi_x = g_x.permute(0, 2, 1) #[B, h*w, c]\n",
    "        f = torch.matmul(theta_x, phi_x) #[B, c, c]\n",
    "        f = F.softmax(f, dim=-1)\n",
    "        y = torch.matmul(f, g_x)\n",
    "        y = y.view(batch_size, self.in_channels, *x.size()[2:])\n",
    "        y = self.W2(y)\n",
    "        z = y + x\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "\n",
    "class IABlock2D(_IABlockND):\n",
    "    def __init__(self, in_channels, height, width, alpha_x, alpha_y, **kwargs):\n",
    "        super(IABlock2D, self).__init__(in_channels, height=height, width=width,\n",
    "                                alpha_x=alpha_x, alpha_y=alpha_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULD9qLHzoLrn"
   },
   "source": [
    "## STCA Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhUWbt8Toa3w"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Basic convolutional block\"\"\"\n",
    "    def __init__(self, in_c, out_c, k, s=1, p=0):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_c, out_c, k, stride=s, padding=p)\n",
    "        self.bn = nn.BatchNorm3d(out_c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bn(self.conv(x))\n",
    "\n",
    "\n",
    "class SpatialAttn(nn.Module):\n",
    "    \"\"\"Spatial Attention \"\"\"\n",
    "    def __init__(self, in_channels, number):\n",
    "        super(SpatialAttn, self).__init__()\n",
    "        self.conv = ConvBlock(in_channels, number, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x) #[bs, 4, t, h, w]\n",
    "        a = torch.sigmoid(x)\n",
    "        return a\n",
    "\n",
    "\n",
    "class STCABlock3D(nn.Module):\n",
    "    def __init__(self, in_channels, seq_len=4):\n",
    "        super(STCABlock3D, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        conv_nd = nn.Conv3d\n",
    "        bn = nn.BatchNorm3d\n",
    "        self.inter_channels = in_channels // 2\n",
    "\n",
    "        self.SA = SpatialAttn(in_channels, number=4)\n",
    "        self.g = nn.Conv2d(self.in_channels, self.inter_channels, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.STAM = STAModule(self.inter_channels, T=seq_len)\n",
    "\n",
    "        self.W1 = nn.Sequential(\n",
    "                conv_nd(self.in_channels, self.in_channels,\n",
    "                    kernel_size=1, stride=1, padding=0, bias=True),\n",
    "                bn(self.in_channels)\n",
    "            )\n",
    "\n",
    "        self.W2 = nn.Sequential(\n",
    "                conv_nd(self.in_channels, self.in_channels,\n",
    "                    kernel_size=1, stride=1, padding=0, bias=True),\n",
    "                bn(self.in_channels)\n",
    "            )\n",
    "        \n",
    "        # init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, conv_nd) or isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, bn):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        nn.init.constant_(self.W1[1].weight.data, 0.0)\n",
    "        nn.init.constant_(self.W1[1].bias.data, 0.0)\n",
    "        nn.init.constant_(self.W2[1].weight.data, 0.0)\n",
    "        nn.init.constant_(self.W2[1].bias.data, 0.0)\n",
    "\n",
    "    def apply_attention(self, x, a):\n",
    "        b, c, t, h, w = x.size()\n",
    "        a = a.view(a.size(0), a.size(1), h * w) \n",
    "        x = x.transpose(1, 2).contiguous().view(b * t, -1, h * w) \n",
    "        y = torch.matmul(a, x.transpose(1, 2)) \n",
    "        y = y.view(b, t, -1, c)\n",
    "        return y\n",
    "\n",
    "    def reduce_dimension(self, x, u):\n",
    "        bs, t, n, c = x.size()\n",
    "\n",
    "        x = x.view(bs * t * n, c)\n",
    "        x = torch.cat((x, u), 0) \n",
    "        x = self.g(x.view(x.size(0), x.size(1), 1, 1))\n",
    "        x = x.view(x.size(0), x.size(1))\n",
    "        \n",
    "        u = x[bs * t * n:, :] \n",
    "        x = x[: bs * t * n, :].view(bs, t, n, -1)\n",
    "        return x, u\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CAM\n",
    "        batch_size, t = x.size(0), x.size(2)\n",
    "\n",
    "        g_x = x.view(batch_size, self.in_channels * t, -1) \n",
    "\n",
    "        theta_x = g_x \n",
    "        phi_x = g_x.permute(0, 2, 1) \n",
    "        f = torch.matmul(theta_x, phi_x) \n",
    "        f = F.softmax(f, dim=-1)\n",
    "\n",
    "        y = torch.matmul(f, g_x) \n",
    "        y = y.view(batch_size, self.in_channels, *x.size()[2:])\n",
    "        y = self.W1(y)\n",
    "        z = y + x\n",
    "\n",
    "        # STAM\n",
    "        x = z\n",
    "        inputs = x\n",
    "\n",
    "        b, c, t, h, w = x.size()\n",
    "        u = x.view(b, c, -1).mean(2) \n",
    "\n",
    "        a = self.SA(x) \n",
    "        a = a.transpose(1, 2).contiguous()\n",
    "        a = a.view(b * t, -1, h, w) \n",
    "\n",
    "        x = self.apply_attention(x, a)\n",
    "        x, u = self.reduce_dimension(x, u)\n",
    "        y = self.STAM(x, u) \n",
    "\n",
    "        y = torch.mean(y, 2) \n",
    "        u = u.unsqueeze(1).expand_as(y)\n",
    "        u = torch.cat((y, u), 2) \n",
    "\n",
    "        y = self.W2(u.transpose(1, 2).unsqueeze(-1).unsqueeze(-1))\n",
    "        z = y + inputs\n",
    "        return z, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UoqyMQktMqL"
   },
   "source": [
    "## inflate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2ivSPNhtPFi"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def inflate_conv(conv2d,\n",
    "                 time_dim=3,\n",
    "                 time_padding=0,\n",
    "                 time_stride=1,\n",
    "                 time_dilation=1,\n",
    "                 center=False):\n",
    "    # To preserve activations, padding should be by continuity and not zero\n",
    "    # or no padding in time dimension\n",
    "    kernel_dim = (time_dim, conv2d.kernel_size[0], conv2d.kernel_size[1])\n",
    "    padding = (time_padding, conv2d.padding[0], conv2d.padding[1])\n",
    "    stride = (time_stride, conv2d.stride[0], conv2d.stride[0])\n",
    "    dilation = (time_dilation, conv2d.dilation[0], conv2d.dilation[1])\n",
    "    conv3d = nn.Conv3d(\n",
    "        conv2d.in_channels,\n",
    "        conv2d.out_channels,\n",
    "        kernel_dim,\n",
    "        padding=padding,\n",
    "        dilation=dilation,\n",
    "        stride=stride)\n",
    "    # Repeat filter time_dim times along time dimension\n",
    "    weight_2d = conv2d.weight.data\n",
    "    if center:\n",
    "        weight_3d = torch.zeros(*weight_2d.shape)\n",
    "        weight_3d = weight_3d.unsqueeze(2).repeat(1, 1, time_dim, 1, 1)\n",
    "        middle_idx = time_dim // 2\n",
    "        weight_3d[:, :, middle_idx, :, :] = weight_2d\n",
    "    else:\n",
    "        weight_3d = weight_2d.unsqueeze(2).repeat(1, 1, time_dim, 1, 1)\n",
    "        weight_3d = weight_3d / time_dim\n",
    "\n",
    "    # Assign new params\n",
    "    conv3d.weight = nn.Parameter(weight_3d)\n",
    "    conv3d.bias = conv2d.bias\n",
    "    return conv3d\n",
    "\n",
    "\n",
    "def inflate_linear(linear2d, time_dim):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        time_dim: final time dimension of the features\n",
    "    \"\"\"\n",
    "    linear3d = nn.Linear(linear2d.in_features * time_dim,\n",
    "                               linear2d.out_features)\n",
    "    weight3d = linear2d.weight.data.repeat(1, time_dim)\n",
    "    weight3d = weight3d / time_dim\n",
    "\n",
    "    linear3d.weight = nn.Parameter(weight3d)\n",
    "    linear3d.bias = linear2d.bias\n",
    "    return linear3d\n",
    "\n",
    "\n",
    "def inflate_batch_norm(batch2d):\n",
    "    # In pytorch 0.2.0 the 2d and 3d versions of batch norm\n",
    "    # work identically except for the check that verifies the\n",
    "    # input dimensions\n",
    "\n",
    "    batch3d = nn.BatchNorm3d(batch2d.num_features)\n",
    "    # retrieve 3d _check_input_dim function\n",
    "    batch2d._check_input_dim = batch3d._check_input_dim\n",
    "    return batch2d\n",
    "\n",
    "\n",
    "def inflate_pool(pool2d,\n",
    "                 time_dim=1,\n",
    "                 time_padding=0,\n",
    "                 time_stride=None,\n",
    "                 time_dilation=1):\n",
    "    kernel_dim = (time_dim, pool2d.kernel_size, pool2d.kernel_size)\n",
    "    padding = (time_padding, pool2d.padding, pool2d.padding)\n",
    "    if time_stride is None:\n",
    "        time_stride = time_dim\n",
    "    stride = (time_stride, pool2d.stride, pool2d.stride)\n",
    "    if isinstance(pool2d, nn.MaxPool2d):\n",
    "        dilation = (time_dilation, pool2d.dilation, pool2d.dilation)\n",
    "        pool3d = nn.MaxPool3d(\n",
    "            kernel_dim,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            stride=stride,\n",
    "            ceil_mode=pool2d.ceil_mode)\n",
    "    elif isinstance(pool2d, nn.AvgPool2d):\n",
    "        pool3d = nn.AvgPool3d(kernel_dim, stride=stride)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            '{} is not among known pooling classes'.format(type(pool2d)))\n",
    "    return pool3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dIBExdhh05l"
   },
   "source": [
    "# ResNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oALHI4xDh05l"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50_s1', 'resnet101',\n",
    "           'resnet152']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3])\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet18_s1(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34_s1(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50_s1(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet101_s1(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet152(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUB2erNGrXYA"
   },
   "source": [
    "## STCANet3D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-c2pYsqJgke"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import copy\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def weights_init_kaiming(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # print(classname)\n",
    "    if classname.find('Conv') != -1:\n",
    "        # init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n",
    "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_out')\n",
    "        init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_out')\n",
    "        init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def weights_init_classifier(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        init.normal_(m.weight.data, std=0.001)\n",
    "        init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "class Bottleneck3d(nn.Module):\n",
    "\n",
    "    def __init__(self, bottleneck2d, inflate_time=False):\n",
    "        super(Bottleneck3d, self).__init__()\n",
    "\n",
    "        if inflate_time == True:\n",
    "            self.conv1 = inflate_conv(bottleneck2d.conv1, time_dim=3, time_padding=1, center=True)\n",
    "        else:\n",
    "            self.conv1 = inflate_conv(bottleneck2d.conv1, time_dim=1)\n",
    "        self.bn1 = inflate_batch_norm(bottleneck2d.bn1)\n",
    "        self.conv2 = inflate_conv(bottleneck2d.conv2, time_dim=1)\n",
    "        self.bn2 = inflate_batch_norm(bottleneck2d.bn2)\n",
    "        self.conv3 = inflate_conv(bottleneck2d.conv3, time_dim=1)\n",
    "        self.bn3 = inflate_batch_norm(bottleneck2d.bn3)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if bottleneck2d.downsample is not None:\n",
    "            self.downsample = self._inflate_downsample(bottleneck2d.downsample)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def _inflate_downsample(self, downsample2d, time_stride=1):\n",
    "        downsample3d = nn.Sequential(\n",
    "            inflate_conv(downsample2d[0], time_dim=1, \n",
    "                                 time_stride=time_stride),\n",
    "            inflate_batch_norm(downsample2d[1]))\n",
    "        return downsample3d\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class STCANet3D(nn.Module):\n",
    "    def __init__(self, num_classes, use_gpu, loss={'xent', 'htri'}):\n",
    "        super(STCANet3D, self).__init__()\n",
    "        \n",
    "        self.loss = loss\n",
    "        self.use_gpu = use_gpu\n",
    "        resnet2d = resnet50_s1(pretrained=True)\n",
    "\n",
    "      \n",
    "        self.conv1 = inflate_conv(resnet2d.conv1, time_dim=1)\n",
    "        self.bn1 = inflate_batch_norm(resnet2d.bn1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = inflate_pool(resnet2d.maxpool, time_dim=1)\n",
    "\n",
    "        self.layer1 = self._inflate_reslayer(resnet2d.layer1)\n",
    "        self.layer2 = self._inflate_reslayer(resnet2d.layer2)\n",
    "        \n",
    "        self.layer3 = self._inflate_reslayer(resnet2d.layer3)\n",
    "        self.layer4 = self._inflate_reslayer(resnet2d.layer4)\n",
    "\n",
    "        \n",
    "        self.STCABlock3D2 = STCABlock3D(512)\n",
    "        self.feat_dim = 2048\n",
    "\n",
    "        # fc using random initialization\n",
    "        add_block = nn.BatchNorm1d(self.feat_dim)\n",
    "        add_block.apply(weights_init_kaiming)\n",
    "        self.bn = add_block\n",
    "        \n",
    "        # classifier using Random initialization\n",
    "        classifier = nn.Linear(self.feat_dim, num_classes)\n",
    "        classifier.apply(weights_init_classifier)\n",
    "        self.classifier = classifier\n",
    "      \n",
    "        \n",
    "    def _inflate_reslayer(self, reslayer2d, enhance_idx=[], channels=512):\n",
    "        reslayers3d = []\n",
    "        for i, layer2d in enumerate(reslayer2d):\n",
    "            layer3d = Bottleneck3d(layer2d)\n",
    "            reslayers3d.append(layer3d)\n",
    "            \n",
    "            \n",
    "        return nn.Sequential(*reslayers3d)\n",
    "        \n",
    "    def pool(self, x):\n",
    "        kernel_size = x.size()[2:]\n",
    "       # print(\"Kernel size in pool\",kernel_size)\n",
    "        x = F.max_pool3d(x, kernel_size = kernel_size) \n",
    "       # f = F.avg_pool3d(x4,x4.size()[2:])\n",
    "        x = x.view(x.size(0), -1) #[b, c]\n",
    "        return x\n",
    "\n",
    "    def pooling(self, x):\n",
    "        b, c, t, h, w = x.size()\n",
    "        x = x.permute(0, 2, 1, 3, 4).contiguous()\n",
    "        x = x.view(b*t, c, h, w)\n",
    "        #kernel_size = x.size()[2:]\n",
    "        x = F.max_pool2d(x, x.size()[2:])\n",
    "        x = x.view(b, t, -1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.conv1(x)\n",
    "      x = self.bn1(x)\n",
    "      x = self.relu(x)\n",
    "      x = self.maxpool(x)\n",
    "\n",
    "      x = self.layer1(x)\n",
    "      x = self.layer2(x) \n",
    "\n",
    "\n",
    "      x2,a = self.STCABlock3D2(x)\n",
    "      x = self.layer3(x2)\n",
    "      x = self.layer4(x)\n",
    "      b, c, t, h, w = x.size()\n",
    "\n",
    "     \n",
    "      x = self.pool(x)\n",
    "    \n",
    "      if not self.training:\n",
    "            return x\n",
    "\n",
    "      #x = x.mean(1)\n",
    "      f = self.bn(x)\n",
    "      y = self.classifier(f)\n",
    "      \n",
    "      \n",
    "       \n",
    "\n",
    "      if self.loss == {'xent'}:\n",
    "        return y\n",
    "      elif self.loss == {'xent', 'htri'}:\n",
    "        return y, f\n",
    "      else:\n",
    "        raise KeyError(\"Unsupported loss: {}\".format(self.loss))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8hpeKCGh05m"
   },
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alOw4Tbih05n"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "\n",
    "__factory = {\n",
    "        'STCANet3D': STCANet3D\n",
    "}\n",
    "\n",
    "def get_names():\n",
    "    return __factory.keys()\n",
    "\n",
    "def init_model(name, *args, **kwargs):\n",
    "    if name not in __factory.keys():\n",
    "        raise KeyError(\"Unknown model: {}\".format(name))\n",
    "    return __factory[name](*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hlo_ojIQqkyC"
   },
   "source": [
    "# Tools Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0TiOvq1rU9G"
   },
   "source": [
    "## Utility Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-l4nFMkrVXX"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import os\n",
    "import sys\n",
    "import errno\n",
    "import shutil\n",
    "import json\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "\n",
    "def mkdir_if_missing(directory):\n",
    "    if not osp.exists(directory):\n",
    "        try:\n",
    "            os.makedirs(directory)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value.\n",
    "       \n",
    "       Code imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def save_checkpoint(state, is_best, fpath='checkpoint.pth.tar'):\n",
    "    mkdir_if_missing(osp.dirname(fpath))\n",
    "    torch.save(state, fpath)\n",
    "    if is_best:\n",
    "        shutil.copy(fpath, osp.join(osp.dirname(fpath), 'best_model.pth.tar'))\n",
    "\n",
    "class Logger(object):\n",
    "    \"\"\"\n",
    "    Write console output to external text file.\n",
    "    Code imported from https://github.com/Cysu/open-reid/blob/master/reid/utils/logging.py.\n",
    "    \"\"\"\n",
    "    def __init__(self, fpath=None, mode='w'):\n",
    "        self.console = sys.stdout\n",
    "        self.file = None\n",
    "        if fpath is not None:\n",
    "            mkdir_if_missing(os.path.dirname(fpath))\n",
    "            self.file = open(fpath, mode)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "    def __enter__(self):\n",
    "        pass\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.close()\n",
    "\n",
    "    def write(self, msg):\n",
    "        self.console.write(msg)\n",
    "        if self.file is not None:\n",
    "            self.file.write(msg)\n",
    "\n",
    "    def flush(self):\n",
    "        self.console.flush()\n",
    "        if self.file is not None:\n",
    "            self.file.flush()\n",
    "            os.fsync(self.file.fileno())\n",
    "\n",
    "    def close(self):\n",
    "        self.console.close()\n",
    "        if self.file is not None:\n",
    "            self.file.close()\n",
    "\n",
    "def read_json(fpath):\n",
    "    with open(fpath, 'r') as f:\n",
    "        obj = json.load(f)\n",
    "    return obj\n",
    "\n",
    "def write_json(obj, fpath):\n",
    "    mkdir_if_missing(osp.dirname(fpath))\n",
    "    with open(fpath, 'w') as f:\n",
    "        json.dump(obj, f, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-adNMIjqnKX"
   },
   "source": [
    "## Metrics Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 522,
     "status": "ok",
     "timestamp": 1637116180485,
     "user": {
      "displayName": "Colab Pro",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12358552435453919041"
     },
     "user_tz": -300
    },
    "id": "bbfX3jAWqslm",
    "outputId": "a0d5524e-f408-4136-b197-c2ae8a05d1d0"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from eval_lib.cython_eval import eval_market1501_wrap\n",
    "    CYTHON_EVAL_AVAI = True\n",
    "    print(\"Cython evaluation is AVAILABLE\")\n",
    "except ImportError:\n",
    "    CYTHON_EVAL_AVAI = False\n",
    "    print(\"Warning: Cython evaluation is UNAVAILABLE\")\n",
    "\n",
    "\n",
    "def eval_cuhk03(distmat, q_pids, g_pids, q_camids, g_camids, max_rank, N=100):\n",
    "    \"\"\"Evaluation with cuhk03 metric\n",
    "    Key: one image for each gallery identity is randomly sampled for each query identity.\n",
    "    Random sampling is performed N times (default: N=100).\n",
    "    \"\"\"\n",
    "    num_q, num_g = distmat.shape\n",
    "    if num_g < max_rank:\n",
    "        max_rank = num_g\n",
    "        print(\"Note: number of gallery samples is quite small, got {}\".format(num_g))\n",
    "    indices = np.argsort(distmat, axis=1)\n",
    "    matches = (g_pids[indices] == q_pids[:, np.newaxis]).astype(np.int32)\n",
    "\n",
    "    # compute cmc curve for each query\n",
    "    all_cmc = []\n",
    "    all_AP = []\n",
    "    num_valid_q = 0. # number of valid query\n",
    "    for q_idx in range(num_q):\n",
    "        # get query pid and camid\n",
    "        q_pid = q_pids[q_idx]\n",
    "        q_camid = q_camids[q_idx]\n",
    "\n",
    "        # remove gallery samples that have the same pid and camid with query\n",
    "        order = indices[q_idx]\n",
    "        remove = (g_pids[order] == q_pid) & (g_camids[order] == q_camid)\n",
    "        keep = np.invert(remove)\n",
    "\n",
    "        # compute cmc curve\n",
    "        orig_cmc = matches[q_idx][keep] # binary vector, positions with value 1 are correct matches\n",
    "        if not np.any(orig_cmc):\n",
    "            # this condition is true when query identity does not appear in gallery\n",
    "            continue\n",
    "\n",
    "        kept_g_pids = g_pids[order][keep]\n",
    "        g_pids_dict = defaultdict(list)\n",
    "        for idx, pid in enumerate(kept_g_pids):\n",
    "            g_pids_dict[pid].append(idx)\n",
    "\n",
    "        cmc, AP = 0., 0.\n",
    "        for repeat_idx in range(N):\n",
    "            mask = np.zeros(len(orig_cmc), dtype=np.bool)\n",
    "            for _, idxs in g_pids_dict.items():\n",
    "                # randomly sample one image for each gallery person\n",
    "                rnd_idx = np.random.choice(idxs)\n",
    "                mask[rnd_idx] = True\n",
    "            masked_orig_cmc = orig_cmc[mask]\n",
    "            _cmc = masked_orig_cmc.cumsum()\n",
    "            _cmc[_cmc > 1] = 1\n",
    "            cmc += _cmc[:max_rank].astype(np.float32)\n",
    "            # compute AP\n",
    "            num_rel = masked_orig_cmc.sum()\n",
    "            tmp_cmc = masked_orig_cmc.cumsum()\n",
    "            tmp_cmc = [x / (i+1.) for i, x in enumerate(tmp_cmc)]\n",
    "            tmp_cmc = np.asarray(tmp_cmc) * masked_orig_cmc\n",
    "            AP += tmp_cmc.sum() / num_rel\n",
    "        cmc /= N\n",
    "        AP /= N\n",
    "        all_cmc.append(cmc)\n",
    "        all_AP.append(AP)\n",
    "        num_valid_q += 1.\n",
    "\n",
    "    assert num_valid_q > 0, \"Error: all query identities do not appear in gallery\"\n",
    "\n",
    "    all_cmc = np.asarray(all_cmc).astype(np.float32)\n",
    "    all_cmc = all_cmc.sum(0) / num_valid_q\n",
    "    mAP = np.mean(all_AP)\n",
    "\n",
    "    return all_cmc, mAP\n",
    "\n",
    "\n",
    "def eval_market1501(distmat, q_pids, g_pids, q_camids, g_camids, max_rank):\n",
    "    \"\"\"Evaluation with market1501 metric\n",
    "    Key: for each query identity, its gallery images from the same camera view are discarded.\n",
    "    \"\"\"\n",
    "    num_q, num_g = distmat.shape\n",
    "    if num_g < max_rank:\n",
    "        max_rank = num_g\n",
    "        print(\"Note: number of gallery samples is quite small, got {}\".format(num_g))\n",
    "    indices = np.argsort(distmat, axis=1)\n",
    "    matches = (g_pids[indices] == q_pids[:, np.newaxis]).astype(np.int32)\n",
    "\n",
    "    # compute cmc curve for each query\n",
    "    all_cmc = []\n",
    "    all_AP = []\n",
    "    num_valid_q = 0. # number of valid query\n",
    "    for q_idx in range(num_q):\n",
    "        # get query pid and camid\n",
    "        q_pid = q_pids[q_idx]\n",
    "        q_camid = q_camids[q_idx]\n",
    "\n",
    "        # remove gallery samples that have the same pid and camid with query\n",
    "        order = indices[q_idx]\n",
    "        remove = (g_pids[order] == q_pid) & (g_camids[order] == q_camid)\n",
    "        keep = np.invert(remove)\n",
    "\n",
    "        # compute cmc curve\n",
    "        orig_cmc = matches[q_idx][keep] # binary vector, positions with value 1 are correct matches\n",
    "        if not np.any(orig_cmc):\n",
    "            # this condition is true when query identity does not appear in gallery\n",
    "            continue\n",
    "\n",
    "        cmc = orig_cmc.cumsum()\n",
    "        cmc[cmc > 1] = 1\n",
    "\n",
    "        all_cmc.append(cmc[:max_rank])\n",
    "        num_valid_q += 1.\n",
    "\n",
    "        # compute average precision\n",
    "        # reference: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision\n",
    "        num_rel = orig_cmc.sum()\n",
    "        tmp_cmc = orig_cmc.cumsum()\n",
    "        tmp_cmc = [x / (i+1.) for i, x in enumerate(tmp_cmc)]\n",
    "        tmp_cmc = np.asarray(tmp_cmc) * orig_cmc\n",
    "        AP = tmp_cmc.sum() / num_rel\n",
    "        all_AP.append(AP)\n",
    "\n",
    "    assert num_valid_q > 0, \"Error: all query identities do not appear in gallery\"\n",
    "\n",
    "    all_cmc = np.asarray(all_cmc).astype(np.float32)\n",
    "    all_cmc = all_cmc.sum(0) / num_valid_q\n",
    "    mAP = np.mean(all_AP)\n",
    "\n",
    "    return all_cmc, mAP\n",
    "\n",
    "\n",
    "def evaluate(distmat, q_pids, g_pids, q_camids, g_camids, max_rank=50, use_metric_cuhk03=False, use_cython=True):\n",
    "    if use_metric_cuhk03:\n",
    "        return eval_cuhk03(distmat, q_pids, g_pids, q_camids, g_camids, max_rank)\n",
    "    else:\n",
    "        if use_cython and CYTHON_EVAL_AVAI:\n",
    "            return eval_market1501_wrap(distmat, q_pids, g_pids, q_camids, g_camids, max_rank)\n",
    "        else:\n",
    "            return eval_market1501(distmat, q_pids, g_pids, q_camids, g_camids, max_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVqXe8BRqv9O"
   },
   "source": [
    "## loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMunFba1q9IQ"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\"\"\"\n",
    "Shorthands for loss:\n",
    "- CrossEntropyLabelSmooth: xent\n",
    "- TripletLoss: htri\n",
    "\"\"\"\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"Triplet loss with hard positive/negative mining.\n",
    "    Reference:\n",
    "    Hermans et al. In Defense of the Triplet Loss for Person Re-Identification. arXiv:1703.07737.\n",
    "    Code imported from https://github.com/Cysu/open-reid/blob/master/reid/loss/triplet.py.\n",
    "    Args:\n",
    "        margin (float): margin for triplet.\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=0.3, distance='consine', use_gpu=True):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        if distance not in ['euclidean', 'consine']:\n",
    "            raise KeyError(\"Unsupported distance: {}\".format(distance))\n",
    "        self.distance = distance\n",
    "        self.margin = margin\n",
    "        self.use_gpu = use_gpu\n",
    "        self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: feature matrix with shape (batch_size, feat_dim)\n",
    "            targets: ground truth labels with shape (num_classes)\n",
    "        \"\"\"\n",
    "        n = inputs.size(0)\n",
    "\n",
    "        # Compute pairwise distance, replace by the official when merged\n",
    "        if self.distance == 'euclidean':\n",
    "            dist = torch.pow(inputs, 2).sum(dim=1, keepdim=True).expand(n, n)\n",
    "            dist = dist + dist.t()\n",
    "            dist.addmm_(1, -2, inputs, inputs.t())\n",
    "            dist = dist.clamp(min=1e-12).sqrt()  # for numerical stability\n",
    "        elif self.distance == 'consine':\n",
    "            fnorm = torch.norm(inputs, p=2, dim=1, keepdim=True)\n",
    "            l2norm = inputs.div(fnorm.expand_as(inputs))\n",
    "            dist = - torch.mm(l2norm, l2norm.t())\n",
    "\n",
    "        if self.use_gpu: targets = targets.cuda()\n",
    "        # For each anchor, find the hardest positive and negative\n",
    "        mask = targets.expand(n, n).eq(targets.expand(n, n).t())\n",
    "        dist_ap, dist_an = [], []\n",
    "        for i in range(n):\n",
    "            dist_ap.append(dist[i][mask[i]].max().unsqueeze(0))\n",
    "            dist_an.append(dist[i][mask[i] == 0].min().unsqueeze(0))\n",
    "        dist_ap = torch.cat(dist_ap)\n",
    "        dist_an = torch.cat(dist_an)\n",
    "\n",
    "        # Compute ranking hinge loss\n",
    "        y = torch.ones_like(dist_an)\n",
    "        loss = self.ranking_loss(dist_an, dist_ap, y)\n",
    "        return loss\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJP9nBYpq-J5"
   },
   "source": [
    "## Samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jogw-yNgrFau"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "\n",
    "\"\"\" Deprecated\n",
    "class RandomIdentitySampler(Sampler):\n",
    "    def __init__(self, data_source, num_instances=4):\n",
    "        self.data_source = data_source\n",
    "        self.num_instances = num_instances\n",
    "        self.index_dic = defaultdict(list)\n",
    "        for index, (_, pid, _) in enumerate(data_source):\n",
    "            self.index_dic[pid].append(index)\n",
    "        self.pids = list(self.index_dic.keys())\n",
    "        self.num_identities = len(self.pids)\n",
    "    def __iter__(self):\n",
    "        indices = torch.randperm(self.num_identities)\n",
    "        ret = []\n",
    "        for i in indices:\n",
    "            pid = self.pids[i]\n",
    "            t = self.index_dic[pid]\n",
    "            replace = False if len(t) >= self.num_instances else True\n",
    "            t = np.random.choice(t, size=self.num_instances, replace=replace)\n",
    "            ret.extend(t)\n",
    "        return iter(ret)\n",
    "    def __len__(self):\n",
    "        return self.num_identities * self.num_instances\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class RandomIdentitySampler(Sampler):\n",
    "    \"\"\"\n",
    "    Randomly sample N identities, then for each identity,\n",
    "    randomly sample K instances, therefore batch size is N*K.\n",
    "    Args:\n",
    "    - data_source (Dataset): dataset to sample from.\n",
    "    - num_instances (int): number of instances per identity.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_source, num_instances=4):\n",
    "        self.data_source = data_source\n",
    "        self.num_instances = num_instances\n",
    "        self.index_dic = defaultdict(list)\n",
    "        for index, (_, pid, _) in enumerate(data_source):\n",
    "            self.index_dic[pid].append(index)\n",
    "        self.pids = list(self.index_dic.keys())\n",
    "        self.num_identities = len(self.pids)\n",
    "\n",
    "        # compute number of examples in an epoch\n",
    "        self.length = 0\n",
    "        for pid in self.pids:\n",
    "            idxs = self.index_dic[pid]\n",
    "            num = len(idxs)\n",
    "            if num < self.num_instances:\n",
    "                num = self.num_instances\n",
    "            self.length += num - num % self.num_instances\n",
    "\n",
    "    def __iter__(self):\n",
    "        list_container = []\n",
    "\n",
    "        for pid in self.pids:\n",
    "            idxs = copy.deepcopy(self.index_dic[pid])\n",
    "            if len(idxs) < self.num_instances:\n",
    "                idxs = np.random.choice(idxs, size=self.num_instances, replace=True)\n",
    "            random.shuffle(idxs)\n",
    "            batch_idxs = []\n",
    "            for idx in idxs:\n",
    "                batch_idxs.append(idx)\n",
    "                if len(batch_idxs) == self.num_instances:\n",
    "                    list_container.append(batch_idxs)\n",
    "                    batch_idxs = []\n",
    "\n",
    "        random.shuffle(list_container)\n",
    "\n",
    "        ret = []\n",
    "        for batch_idxs in list_container:\n",
    "            ret.extend(batch_idxs)\n",
    "\n",
    "        return iter(ret)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ay_jX761rqRA"
   },
   "source": [
    "## Video Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ew9xfTmnrsh_"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import functools\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def pil_loader(path, mode):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert(mode)\n",
    "\n",
    "def accimage_loader(path):\n",
    "    try:\n",
    "        import accimage\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "def get_default_image_loader():\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader\n",
    "    else:\n",
    "        return pil_loader\n",
    "\n",
    "def image_loader(path):\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)\n",
    "\n",
    "def video_loader(img_paths, mode, image_loader):\n",
    "    video = []\n",
    "    for image_path in img_paths:\n",
    "        if os.path.exists(image_path):\n",
    "            video.append(image_loader(image_path, mode))\n",
    "        else:\n",
    "            return video\n",
    "    return video\n",
    "\n",
    "def get_default_video_loader():\n",
    "    image_loader = get_default_image_loader()\n",
    "    return functools.partial(video_loader, image_loader=image_loader)\n",
    "\n",
    "\n",
    "class VideoDataset(data.Dataset):\n",
    "    \"\"\"Video Person ReID Dataset.\n",
    "    Note:\n",
    "        Batch data has shape N x C x T x H x W\n",
    "    Args:\n",
    "        dataset (list): List with items (img_paths, pid, camid)\n",
    "        temporal_transform (callable, optional): A function/transform that  takes in a list of frame indices\n",
    "            and returns a transformed version\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        loader (callable, optional): A function to load an video given its path and frame indices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 dataset, \n",
    "                 spatial_transform=None,\n",
    "                 temporal_transform=None,\n",
    "                 get_loader=get_default_video_loader):\n",
    "        self.dataset = dataset\n",
    "        self.spatial_transform = spatial_transform\n",
    "        self.temporal_transform = temporal_transform\n",
    "        self.loader = get_loader()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (clip, pid, camid) where pid is identity of the clip.\n",
    "        \"\"\"\n",
    "        img_paths, pid, camid = self.dataset[index]\n",
    "\n",
    "        if self.temporal_transform is not None:\n",
    "            img_paths = self.temporal_transform(img_paths)\n",
    "\n",
    "        clip = self.loader(img_paths, mode='RGB')\n",
    "\n",
    "        if self.spatial_transform is not None:\n",
    "            self.spatial_transform.randomize_parameters()\n",
    "            clip = [self.spatial_transform(img) for img in clip]\n",
    "\n",
    "        # trans T x C x H x W to C x T x H x W\n",
    "        clip = torch.stack(clip, 0).permute(1, 0, 2, 3)\n",
    "\n",
    "        return clip, pid, camid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0SLWeRzh05p"
   },
   "source": [
    "# Torchtool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyZoOxGbh05q"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, base_lr, epoch, stepsize, gamma=0.1):\n",
    "    # decay learning rate by 'gamma' for every 'stepsize'\n",
    "    lr = base_lr * (gamma ** (epoch // stepsize))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def set_bn_to_eval(m):\n",
    "    # 1. no update for running mean and var\n",
    "    # 2. scale and shift parameters are still trainable\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('BatchNorm') != -1:\n",
    "        m.eval()\n",
    "\n",
    "\n",
    "def count_num_param(model):\n",
    "    num_param = sum(p.numel() for p in model.parameters()) / 1e+06\n",
    "    if hasattr(model, 'classifier') and isinstance(model.classifier, nn.Module):\n",
    "        # we ignore the classifier because it is unused at test time\n",
    "        num_param -= sum(p.numel() for p in model.classifier.parameters()) / 1e+06\n",
    "    return num_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVKPBySCs1cF"
   },
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsOO1lDPs4_T"
   },
   "source": [
    "## Spatial Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hvFtN3us4Td"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import random\n",
    "import math\n",
    "import numbers\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageOps\n",
    "try:\n",
    "    import accimage\n",
    "except ImportError:\n",
    "    accimage = None\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    \"\"\"Composes several transforms together.\n",
    "    Args:\n",
    "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>> ])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms:\n",
    "            img = t(img)\n",
    "        return img\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        for t in self.transforms:\n",
    "            t.randomize_parameters()\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert a ``PIL.Image`` or ``numpy.ndarray`` to tensor.\n",
    "    Converts a PIL.Image or numpy.ndarray (H x W x C) in the range\n",
    "    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, norm_value=255):\n",
    "        self.norm_value = norm_value\n",
    "\n",
    "    def __call__(self, pic):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pic (PIL.Image or numpy.ndarray): Image to be converted to tensor.\n",
    "        Returns:\n",
    "            Tensor: Converted image.\n",
    "        \"\"\"\n",
    "        if isinstance(pic, np.ndarray):\n",
    "            # handle numpy array\n",
    "            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n",
    "            # backward compatibility\n",
    "            return img.float().div(self.norm_value)\n",
    "\n",
    "        if accimage is not None and isinstance(pic, accimage.Image):\n",
    "            nppic = np.zeros(\n",
    "                [pic.channels, pic.height, pic.width], dtype=np.float32)\n",
    "            pic.copyto(nppic)\n",
    "            return torch.from_numpy(nppic)\n",
    "\n",
    "        # handle PIL Image\n",
    "        if pic.mode == 'I':\n",
    "            img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
    "        elif pic.mode == 'I;16':\n",
    "            img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n",
    "        else:\n",
    "            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
    "        # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n",
    "        if pic.mode == 'YCbCr':\n",
    "            nchannel = 3\n",
    "        elif pic.mode == 'I;16':\n",
    "            nchannel = 1\n",
    "        else:\n",
    "            nchannel = len(pic.mode)\n",
    "        img = img.view(pic.size[1], pic.size[0], nchannel)\n",
    "        # put it from HWC to CHW format\n",
    "        # yikes, this transpose takes 80% of the loading time/CPU\n",
    "        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
    "        if isinstance(img, torch.ByteTensor):\n",
    "            return img.float().div(self.norm_value)\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    \"\"\"Normalize an tensor image with mean and standard deviation.\n",
    "    Given mean: (R, G, B) and std: (R, G, B),\n",
    "    will normalize each channel of the torch.*Tensor, i.e.\n",
    "    channel = (channel - mean) / std\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n",
    "        std (sequence): Sequence of standard deviations for R, G, B channels\n",
    "            respecitvely.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        # TODO: make efficient\n",
    "        if tensor.size(0) == 1:\n",
    "            return tensor\n",
    "\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class NormalizeSub(object):\n",
    "    \"\"\"Normalize an tensor image with mean and standard deviation.\n",
    "    Given mean: (R, G, B) and std: (R, G, B),\n",
    "    will normalize each channel of the torch.*Tensor, i.e.\n",
    "    channel = (channel - mean) / std\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n",
    "        std (sequence): Sequence of standard deviations for R, G, B channels\n",
    "            respecitvely.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        # TODO: make efficient\n",
    "        if tensor.size(0) == 1:\n",
    "            mean = [0.5]\n",
    "            std = [0.25]\n",
    "            for t, m, s in zip(tensor, mean, std):\n",
    "                t.sub_(m).div_(s)\n",
    "            return tensor\n",
    "\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Scale(object):\n",
    "    \"\"\"Rescale the input PIL.Image to the given size.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size. If size is a sequence like\n",
    "            (w, h), output size will be matched to this. If size is an int,\n",
    "            smaller edge of the image will be matched to this number.\n",
    "            i.e, if height > width, then image will be rescaled to\n",
    "            (size * height / width, size)\n",
    "        interpolation (int, optional): Desired interpolation. Default is\n",
    "            ``PIL.Image.BILINEAR``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        assert isinstance(size,\n",
    "                          int) or (isinstance(size, collections.Iterable) and\n",
    "                                   len(size) == 2)\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be scaled.\n",
    "        Returns:\n",
    "            PIL.Image: Rescaled image.\n",
    "        \"\"\"\n",
    "        if isinstance(self.size, int):\n",
    "            w, h = img.size\n",
    "            if (w <= h and w == self.size) or (h <= w and h == self.size):\n",
    "                return img\n",
    "            if w < h:\n",
    "                ow = self.size\n",
    "                oh = int(self.size * h / w)\n",
    "                return img.resize((ow, oh), self.interpolation)\n",
    "            else:\n",
    "                oh = self.size\n",
    "                ow = int(self.size * w / h)\n",
    "                return img.resize((ow, oh), self.interpolation)\n",
    "        else:\n",
    "            return img.resize(self.size[::-1], self.interpolation)\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crops the given PIL.Image at a random location.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an\n",
    "            int instead of sequence like (h, w), a square crop (size, size) is\n",
    "            made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be cropped.\n",
    "        Returns:\n",
    "            PIL.Image: Cropped image.\n",
    "        \"\"\"\n",
    "        w, h = img.size\n",
    "        th, tw = self.size\n",
    "\n",
    "        x1 = int(round(self.tl_x * (w - tw)))\n",
    "        y1 = int(round(self.tl_y * (h - th)))\n",
    "        return img.crop((x1, y1, x1 + tw, y1 + th))\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        self.tl_x = random.random()\n",
    "        self.tl_y = random.random()\n",
    "\n",
    "        \n",
    "class CenterCrop(object):\n",
    "    \"\"\"Crops the given PIL.Image at the center.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an\n",
    "            int instead of sequence like (h, w), a square crop (size, size) is\n",
    "            made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be cropped.\n",
    "        Returns:\n",
    "            PIL.Image: Cropped image.\n",
    "        \"\"\"\n",
    "        w, h = img.size\n",
    "        th, tw = self.size\n",
    "        x1 = int(round((w - tw) / 2.))\n",
    "        y1 = int(round((h - th) / 2.))\n",
    "        return img.crop((x1, y1, x1 + tw, y1 + th))\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class CornerCrop(object):\n",
    "\n",
    "    def __init__(self, size, crop_position=None):\n",
    "        self.size = size\n",
    "        if crop_position is None:\n",
    "            self.randomize = True\n",
    "        else:\n",
    "            self.randomize = False\n",
    "        self.crop_position = crop_position\n",
    "        self.crop_positions = ['c', 'tl', 'tr', 'bl', 'br']\n",
    "\n",
    "    def __call__(self, img):\n",
    "        image_width = img.size[0]\n",
    "        image_height = img.size[1]\n",
    "\n",
    "        if self.crop_position == 'c':\n",
    "            th, tw = (self.size, self.size)\n",
    "            x1 = int(round((image_width - tw) / 2.))\n",
    "            y1 = int(round((image_height - th) / 2.))\n",
    "            x2 = x1 + tw\n",
    "            y2 = y1 + th\n",
    "        elif self.crop_position == 'tl':\n",
    "            x1 = 0\n",
    "            y1 = 0\n",
    "            x2 = self.size\n",
    "            y2 = self.size\n",
    "        elif self.crop_position == 'tr':\n",
    "            x1 = image_width - self.size\n",
    "            y1 = 0\n",
    "            x2 = image_width\n",
    "            y2 = self.size\n",
    "        elif self.crop_position == 'bl':\n",
    "            x1 = 0\n",
    "            y1 = image_height - self.size\n",
    "            x2 = self.size\n",
    "            y2 = image_height\n",
    "        elif self.crop_position == 'br':\n",
    "            x1 = image_width - self.size\n",
    "            y1 = image_height - self.size\n",
    "            x2 = image_width\n",
    "            y2 = image_height\n",
    "\n",
    "        img = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "        return img\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        if self.randomize:\n",
    "            self.crop_position = self.crop_positions[random.randint(\n",
    "                0,\n",
    "                len(self.crop_positions) - 1)]\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    \"\"\"Horizontally flip the given PIL.Image randomly with a probability of 0.5.\"\"\"\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be flipped.\n",
    "        Returns:\n",
    "            PIL.Image: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        if self.p < 0.5:\n",
    "            return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        return img\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        self.p = random.random()\n",
    "\n",
    "\n",
    "class MultiScaleCornerCrop(object):\n",
    "    \"\"\"Crop the given PIL.Image to randomly selected size.\n",
    "    A crop of size is selected from scales of the original size.\n",
    "    A position of cropping is randomly selected from 4 corners and 1 center.\n",
    "    This crop is finally resized to given size.\n",
    "    Args:\n",
    "        scales: cropping scales of the original size\n",
    "        size: size of the smaller edge\n",
    "        interpolation: Default: PIL.Image.BILINEAR\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 scales,\n",
    "                 size,\n",
    "                 interpolation=Image.BILINEAR,\n",
    "                 crop_positions=['c', 'tl', 'tr', 'bl', 'br']):\n",
    "        self.scales = scales\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "        self.crop_positions = crop_positions\n",
    "\n",
    "    def __call__(self, img):\n",
    "        min_length = min(img.size[0], img.size[1])\n",
    "        crop_size = int(min_length * self.scale)\n",
    "\n",
    "        image_width = img.size[0]\n",
    "        image_height = img.size[1]\n",
    "\n",
    "        if self.crop_position == 'c':\n",
    "            center_x = image_width // 2\n",
    "            center_y = image_height // 2\n",
    "            box_half = crop_size // 2\n",
    "            x1 = center_x - box_half\n",
    "            y1 = center_y - box_half\n",
    "            x2 = center_x + box_half\n",
    "            y2 = center_y + box_half\n",
    "        elif self.crop_position == 'tl':\n",
    "            x1 = 0\n",
    "            y1 = 0\n",
    "            x2 = crop_size\n",
    "            y2 = crop_size\n",
    "        elif self.crop_position == 'tr':\n",
    "            x1 = image_width - crop_size\n",
    "            y1 = 0\n",
    "            x2 = image_width\n",
    "            y2 = crop_size\n",
    "        elif self.crop_position == 'bl':\n",
    "            x1 = 0\n",
    "            y1 = image_height - crop_size\n",
    "            x2 = crop_size\n",
    "            y2 = image_height\n",
    "        elif self.crop_position == 'br':\n",
    "            x1 = image_width - crop_size\n",
    "            y1 = image_height - crop_size\n",
    "            x2 = image_width\n",
    "            y2 = image_height\n",
    "\n",
    "        img = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "        return img.resize((self.size, self.size), self.interpolation)\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        self.scale = self.scales[random.randint(0, len(self.scales) - 1)]\n",
    "        self.crop_position = self.crop_positions[random.randint(\n",
    "            0,\n",
    "            len(self.scales) - 1)]\n",
    "\n",
    "\n",
    "class MultiScaleRandomCrop(object):\n",
    "\n",
    "    def __init__(self, scales, size, interpolation=Image.BILINEAR):\n",
    "        self.scales = scales\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img):\n",
    "        min_length = min(img.size[0], img.size[1])\n",
    "        crop_size = int(min_length * self.scale)\n",
    "\n",
    "        image_width = img.size[0]\n",
    "        image_height = img.size[1]\n",
    "\n",
    "        x1 = self.tl_x * (image_width - crop_size)\n",
    "        y1 = self.tl_y * (image_height - crop_size)\n",
    "        x2 = x1 + crop_size\n",
    "        y2 = y1 + crop_size\n",
    "\n",
    "        img = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "        return img.resize((self.size, self.size), self.interpolation)\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        self.scale = self.scales[random.randint(0, len(self.scales) - 1)]\n",
    "        self.tl_x = random.random()\n",
    "        self.tl_y = random.random()\n",
    "\n",
    "\n",
    "class Random2DTranslation(object):\n",
    "    \"\"\"\n",
    "    With a probability, first increase image size to (1 + 1/8), and then perform random crop.\n",
    "    Args:\n",
    "        height (int): target height.\n",
    "        width (int): target width.\n",
    "        p (float): probability of performing this transformation. Default: 0.5.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, p=0.5, interpolation=Image.BILINEAR):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "        self.height, self.width = self.size\n",
    "        self.p = p\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be cropped.\n",
    "        Returns:\n",
    "            PIL Image: Cropped image.\n",
    "        \"\"\"\n",
    "        if not self.cropping:\n",
    "            return img.resize((self.width, self.height), self.interpolation)\n",
    "        \n",
    "        new_width, new_height = int(round(self.width * 1.125)), int(round(self.height * 1.125))\n",
    "        resized_img = img.resize((new_width, new_height), self.interpolation)\n",
    "        x_maxrange = new_width - self.width\n",
    "        y_maxrange = new_height - self.height\n",
    "        x1 = int(round(self.tl_x * x_maxrange))\n",
    "        y1 = int(round(self.tl_y * y_maxrange))\n",
    "        return resized_img.crop((x1, y1, x1 + self.width, y1 + self.height))\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        self.cropping = random.uniform(0, 1) < self.p\n",
    "        self.tl_x = random.random()\n",
    "        self.tl_y = random.random()\n",
    "\n",
    "\n",
    "\n",
    "class RandomErasing(object):\n",
    "    \"\"\" Randomly selects a rectangle region in an image and erases its pixels.\n",
    "        'Random Erasing Data Augmentation' by Zhong et al.\n",
    "        See https://arxiv.org/pdf/1708.04896.pdf\n",
    "    Args:\n",
    "         probability: The probability that the Random Erasing operation will be performed.\n",
    "         sl: Minimum proportion of erased area against input image.\n",
    "         sh: Maximum proportion of erased area against input image.\n",
    "         r1: Minimum aspect ratio of erased area.\n",
    "         mean: Erasing value. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, probability = 0.5, height=256, width=128, sl = 0.02, sh = 0.4, r1 = 0.3, mean=[0.4914, 0.4822, 0.4465]):\n",
    "        self.probability = probability\n",
    "        self.mean = mean\n",
    "        self.sl = sl\n",
    "        self.sh = sh\n",
    "        self.r1 = r1\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "       \n",
    "    def __call__(self, img):\n",
    "\n",
    "        if self.p > self.probability or not self.erasing:\n",
    "            return img\n",
    "\n",
    "        x1 = self.x1\n",
    "        y1 = self.y1\n",
    "        h, w = self.h, self.w\n",
    "        img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n",
    "        img[1, x1:x1+h, y1:y1+w] = self.mean[1]\n",
    "        img[2, x1:x1+h, y1:y1+w] = self.mean[2]\n",
    "        return img\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        self.p = random.uniform(0, 1)\n",
    "        if self.p > self.probability:\n",
    "            return\n",
    "        self.erasing = False\n",
    "\n",
    "        for attempt in range(100):\n",
    "            area = self.height * self.width\n",
    "       \n",
    "            target_area = random.uniform(self.sl, self.sh) * area\n",
    "            aspect_ratio = random.uniform(self.r1, 1/self.r1)\n",
    "\n",
    "            h = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "            w = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "\n",
    "            if w < self.width and h < self.height:\n",
    "                self.x1 = random.randint(0, self.height - h)\n",
    "                self.y1 = random.randint(0, self.width - w)\n",
    "                self.h = h\n",
    "                self.w = w\n",
    "                self.erasing = True\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2loYNSDss_QN"
   },
   "source": [
    "## Temporal Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afhDjzo_tH3O"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LoopPadding(object):\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, frame_indices):\n",
    "        out = list(frame_indices)\n",
    "\n",
    "        while len(out) < self.size:\n",
    "            for index in out:\n",
    "                if len(out) >= self.size:\n",
    "                    break\n",
    "                out.append(index)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class TemporalBeginCrop(object):\n",
    "    \"\"\"Temporally crop the given frame indices at a beginning.\n",
    "    If the number of frames is less than the size,\n",
    "    loop the indices as many times as necessary to satisfy the size.\n",
    "    Args:\n",
    "        size (int): Desired output size of the crop.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size=4):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, frame_indices):\n",
    "        frame_indices = list(frame_indices)\n",
    "        size = self.size\n",
    "\n",
    "        if len(frame_indices) >= (size - 1) * 8 + 1:\n",
    "            out = frame_indices[0: (size - 1) * 8 + 1: 8]\n",
    "        elif len(frame_indices) >= (size - 1) * 4 + 1:\n",
    "            out = frame_indices[0: (size - 1) * 4 + 1: 4]\n",
    "        elif len(frame_indices) >= (size - 1) * 2 + 1:\n",
    "            out = frame_indices[0: (size - 1) * 2 + 1: 2]\n",
    "        elif len(frame_indices) >= size:\n",
    "            out = frame_indices[0:size:1]\n",
    "        else:\n",
    "            out = frame_indices[0:size]\n",
    "            while len(out) < size:\n",
    "                for index in out:\n",
    "                    if len(out) >= size:\n",
    "                        break\n",
    "                    out.append(index)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class TemporalRandomCrop(object):\n",
    "    \"\"\"Temporally crop the given frame indices at a random location.\n",
    "    If the number of frames is less than the size,\n",
    "    loop the indices as many times as necessary to satisfy the size.\n",
    "    Args:\n",
    "        size (int): Desired output size of the crop.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size=4, stride=8):\n",
    "        self.size = size\n",
    "        self.stride = stride\n",
    "\n",
    "    def __call__(self, frame_indices):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            frame_indices (list): frame indices to be cropped.\n",
    "        Returns:\n",
    "            list: Cropped frame indices.\n",
    "        \"\"\"\n",
    "        frame_indices = list(frame_indices)\n",
    "\n",
    "        if len(frame_indices) >= self.size * self.stride:\n",
    "            rand_end = len(frame_indices) - (self.size - 1) * self.stride - 1\n",
    "            begin_index = random.randint(0, rand_end)\n",
    "            end_index = begin_index + (self.size - 1) * self.stride + 1\n",
    "            out = frame_indices[begin_index:end_index:self.stride]\n",
    "        elif len(frame_indices) >= self.size:\n",
    "            index = np.random.choice(len(frame_indices), size=self.size, replace=False)\n",
    "            index.sort()\n",
    "            out = [frame_indices[index[i]] for i in range(self.size)]\n",
    "        else:\n",
    "            index = np.random.choice(len(frame_indices), size=self.size, replace=True)\n",
    "            index.sort()\n",
    "            out = [frame_indices[index[i]] for i in range(self.size)]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7G6yAVEh05q"
   },
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CJCSlpHtuz_"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import h5py\n",
    "import scipy\n",
    "import datetime\n",
    "import argparse\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "\n",
    "def extract(model, args, vids, use_gpu):\n",
    "    n, c, t, h, w = vids.size()\n",
    "    assert(n == 1)\n",
    "    k = args.test_frames\n",
    "\n",
    "    if t % k != 0:\n",
    "        inputs = vids.clone()\n",
    "        while(inputs.size(2) % k != 0):\n",
    "            for idx in range(t):\n",
    "                if (inputs.size(2) % k == 0):\n",
    "                    break\n",
    "                inputs = torch.cat((inputs, vids[:,:,idx:idx+1]), 2)\n",
    "        vids = inputs\n",
    "    t = vids.size(2)\n",
    "    assert (t % k == 0)\n",
    "\n",
    "    vids = vids.view(c, t//k, k, h, w).contiguous()\n",
    "    vids = vids.transpose(0, 1) #[t//k, c, k, h, w]\n",
    "    vids = vids.cuda()\n",
    "\n",
    "    num_clips = vids.size(0)\n",
    "    batch_size = 32\n",
    "    feat = torch.cuda.FloatTensor()\n",
    "    for i in range(int(math.ceil(num_clips * 1.0 / batch_size))):\n",
    "        clip = vids[i*batch_size: (i+1)*batch_size] #[batch_size, c, k, h, w]\n",
    "        output = model(clip) #[batch_size, k/1, c]\n",
    "        output = output.view(-1, output.size(-1)) #[batch_size*k, c]\n",
    "        feat = torch.cat((feat, output), 0)\n",
    "\n",
    "    feat = feat.mean(0, keepdim=True)\n",
    "    feat = model.module.bn(feat)\n",
    "\n",
    "    return feat\n",
    "\n",
    "def evaluation(model, args, queryloader, galleryloader, use_gpu, ranks=[1, 5, 10, 20]):\n",
    "    since = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    qf, q_pids, q_camids = [], [], []\n",
    "    for batch_idx, (vids, pids, camids) in enumerate(queryloader):\n",
    "        if (batch_idx + 1) % 1000==0:\n",
    "            print(\"{}/{}\".format(batch_idx+1, len(queryloader)))\n",
    "\n",
    "        qf.append(extract(model, args, vids, use_gpu).squeeze())\n",
    "        q_pids.extend(pids)\n",
    "        q_camids.extend(camids)\n",
    "\n",
    "    qf = torch.stack(qf)\n",
    "    q_pids = np.asarray(q_pids)\n",
    "    q_camids = np.asarray(q_camids)\n",
    "    print(\"Extracted features for query set, obtained {} matrix\".format(qf.shape))\n",
    "\n",
    "    gf, g_pids, g_camids = [], [], []\n",
    "    for batch_idx, (vids, pids, camids) in enumerate(galleryloader):\n",
    "        if (batch_idx + 1) % 1000==0:\n",
    "            print(\"{}/{}\".format(batch_idx+1, len(galleryloader)))\n",
    "\n",
    "        gf.append(extract(model, args, vids, use_gpu).squeeze())\n",
    "        g_pids.extend(pids)\n",
    "        g_camids.extend(camids)\n",
    "\n",
    "    gf = torch.stack(gf)\n",
    "    g_pids = np.asarray(g_pids)\n",
    "    g_camids = np.asarray(g_camids)\n",
    "\n",
    "    if args.dataset == 'mars' or args.dataset == 'lsvid':\n",
    "        print('process the dataset {}!'.format(args.dataset))\n",
    "        # gallery set must contain query set, otherwise 140 query imgs will not have ground truth.\n",
    "        gf = torch.cat((qf, gf), 0)\n",
    "        g_pids = np.append(q_pids, g_pids)\n",
    "        g_camids = np.append(q_camids, g_camids)\n",
    "\n",
    "    print(\"Extracted features for gallery set, obtained {} matrix\".format(gf.shape))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Extracting features complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print(\"Computing distance matrix\")\n",
    "    m, n = qf.size(0), gf.size(0)\n",
    "    distmat = torch.zeros((m,n))\n",
    "\n",
    "    if args.distance == 'euclidean':\n",
    "        distmat = torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\n",
    "                  torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\n",
    "        distmat.addmm_(1, -2, qf, gf.t())\n",
    "    else:\n",
    "        q_norm = torch.norm(qf, p=2, dim=1, keepdim=True)\n",
    "        g_norm = torch.norm(gf, p=2, dim=1, keepdim=True)\n",
    "        qf = qf.div(q_norm.expand_as(qf))\n",
    "        gf = gf.div(g_norm.expand_as(gf))\n",
    "        distmat = - torch.mm(qf, gf.t())\n",
    "    distmat = distmat.data.cpu()\n",
    "    distmat = distmat.numpy()\n",
    "\n",
    "    print(\"Computing CMC and mAP\")\n",
    "    cmc, mAP = evaluate(distmat, q_pids, g_pids, q_camids, g_camids)\n",
    "\n",
    "    print(\"Results ----------\")\n",
    "    print(\"mAP: {:.2%}\".format(mAP))\n",
    "    print(\"CMC curve\")\n",
    "    for r in ranks:\n",
    "        print(\"Rank-{:<3}: {:.2%}\".format(r, cmc[r-1]))\n",
    "    print(\"------------------\")\n",
    "\n",
    "    elapsed = round(time.time() - since)\n",
    "    elapsed = str(datetime.timedelta(seconds=elapsed))\n",
    "    print(\"Finished. Total elapsed time (h:m:s): {}.\".format(elapsed))\n",
    "\n",
    "    return cmc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYbj9cfUh05s"
   },
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baxtINPch05s"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def init_optim(optim, params, lr, weight_decay):\n",
    "    if optim == 'adam':\n",
    "        return torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
    "    elif optim == 'amsgrad':\n",
    "        return torch.optim.Adam(params, lr=lr, weight_decay=weight_decay, amsgrad=True)\n",
    "    elif optim == 'sgd':\n",
    "        return torch.optim.SGD(params, lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    elif optim == 'rmsprop':\n",
    "        return torch.optim.RMSprop(params, lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise KeyError(\"Unsupported optimizer: {}\".format(optim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk9RkwycayKr"
   },
   "source": [
    "## Video_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12342,
     "status": "ok",
     "timestamp": 1636911812658,
     "user": {
      "displayName": "Colab Pro",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12358552435453919041"
     },
     "user_tz": -300
    },
    "id": "MBwoxcQYi6Je",
    "outputId": "7aa00415-eb87-4b8a-dd72-4aaa44a87c71"
   },
   "outputs": [],
   "source": [
    "!pip install torchnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "executionInfo": {
     "elapsed": 576,
     "status": "error",
     "timestamp": 1636911900271,
     "user": {
      "displayName": "Colab Pro",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12358552435453919041"
     },
     "user_tz": -300
    },
    "id": "8_eoV8M6a09g",
    "outputId": "99cac1d2-227f-4529-e173-f2e4f6ab20bd"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import functools\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def pil_loader(path, mode):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert(mode)\n",
    "\n",
    "def accimage_loader(path):\n",
    "    try:\n",
    "        import accimage\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "def get_default_image_loader():\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader\n",
    "    else:\n",
    "        return pil_loader\n",
    "\n",
    "def image_loader(path):\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)\n",
    "\n",
    "def video_loader(img_paths, mode, image_loader):\n",
    "    video = []\n",
    "    for image_path in img_paths:\n",
    "        if os.path.exists(image_path):\n",
    "            video.append(image_loader(image_path, mode))\n",
    "        else:\n",
    "            return video\n",
    "    return video\n",
    "\n",
    "def get_default_video_loader():\n",
    "    image_loader = get_default_image_loader()\n",
    "    return functools.partial(video_loader, image_loader=image_loader)\n",
    "\n",
    "\n",
    "class VideoDataset(data.Dataset):\n",
    "    \"\"\"Video Person ReID Dataset.\n",
    "    Note:\n",
    "        Batch data has shape N x C x T x H x W\n",
    "    Args:\n",
    "        dataset (list): List with items (img_paths, pid, camid)\n",
    "        temporal_transform (callable, optional): A function/transform that  takes in a list of frame indices\n",
    "            and returns a transformed version\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        loader (callable, optional): A function to load an video given its path and frame indices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 dataset, \n",
    "                 spatial_transform=None,\n",
    "                 temporal_transform=None,\n",
    "                 get_loader=get_default_video_loader):\n",
    "        self.dataset = dataset\n",
    "        self.spatial_transform = spatial_transform\n",
    "        self.temporal_transform = temporal_transform\n",
    "        self.loader = get_loader()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (clip, pid, camid) where pid is identity of the clip.\n",
    "        \"\"\"\n",
    "        img_paths, pid, camid = self.dataset[index]\n",
    "\n",
    "        if self.temporal_transform is not None:\n",
    "            img_paths = self.temporal_transform(img_paths)\n",
    "\n",
    "        clip = self.loader(img_paths, mode='RGB')\n",
    "\n",
    "        if self.spatial_transform is not None:\n",
    "            self.spatial_transform.randomize_parameters()\n",
    "            clip = [self.spatial_transform(img) for img in clip]\n",
    "\n",
    "        # trans T x C x H x W to C x T x H x W\n",
    "        clip = torch.stack(clip, 0).permute(1, 0, 2, 3)\n",
    "\n",
    "        return clip, pid, camid\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #import data_manager\n",
    "    #import transforms.spatial_transforms as ST\n",
    "    #import transforms.temporal_transforms as TT\n",
    "    import torchvision.utils as tu\n",
    "    from torchnet.logger import VisdomLogger\n",
    "    import numpy as np\n",
    "    np.set_printoptions(threshold='nan')\n",
    "\n",
    "    def show(logger, clips):\n",
    "        \"\"\"clips: [T, C, h, w]\n",
    "        \"\"\"\n",
    "        clips = clips.detach().cpu()\n",
    "        if clips.size(2) != 256:\n",
    "            clips = F.interpolate(clips, (256, 128), mode='bilinear', align_corners=True)\n",
    "        elif clips.size(1) == 3:\n",
    "            mean=torch.tensor([0.485, 0.456, 0.406])\n",
    "            std=torch.tensor([0.229, 0.224, 0.225])\n",
    "            clips.mul_(std.view(1, 3, 1, 1)).add_(mean.view(1, 3, 1, 1))\n",
    "\n",
    "        clips = tu.make_grid(clips, clips.size(0)).numpy()\n",
    "        clips = np.array(clips * 255, dtype=np.uint8)\n",
    "        logger.log(clips)\n",
    "        return\n",
    "\n",
    "    dataset = data_manager.init_dataset(name='duke_seg')\n",
    "\n",
    "    spatial_transform_train = ST.Compose([\n",
    "                ST.Scale((256, 128), interpolation=3),\n",
    "                ST.RandomHorizontalFlip(),\n",
    "                ST.ToTensor(),\n",
    "                ST.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "    temporal_transform_train = TT.TemporalRandomCropSeg(size=8, stride=4)\n",
    "\n",
    "    dataset = VideoDataset_seg(dataset.train_dense, spatial_transform_train, temporal_transform_train)\n",
    "    clip, pids, camid = dataset[2000]\n",
    "    print(clip.size()) #[8, T, H, W]\n",
    "    clip = clip.transpose(0, 1) #[T, 8, H, W]\n",
    "\n",
    "    vis_img_logger = VisdomLogger('image', port=8000, opts={'title' : 'img'})\n",
    "    show(vis_img_logger, clip[:,:3])\n",
    "\n",
    "    vis_img_logger = VisdomLogger('image', port=8000, opts={'title' : 'head'})\n",
    "    show(vis_img_logger, clip[:,3:4])\n",
    "\n",
    "    vis_img_logger = VisdomLogger('image', port=8000, opts={'title' : 'upper'})\n",
    "    show(vis_img_logger, clip[:,4:5])\n",
    "\n",
    "    vis_img_logger = VisdomLogger('image', port=8000, opts={'title' : 'lower'})\n",
    "    show(vis_img_logger, clip[:,5:6])\n",
    "\n",
    "    vis_img_logger = VisdomLogger('image', port=8000, opts={'title' : 'shoes'})\n",
    "    show(vis_img_logger, clip[:,6:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEbZbNQIh05w"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "36e90f56917d4698a992137ef668a671"
     ]
    },
    "executionInfo": {
     "elapsed": 12148541,
     "status": "ok",
     "timestamp": 1637175318207,
     "user": {
      "displayName": "Colab Pro",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12358552435453919041"
     },
     "user_tz": -300
    },
    "id": "93v6LbYucdOj",
    "outputId": "474ee74a-4b43-46bf-f235-40aa85d82181"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import h5py\n",
    "import scipy\n",
    "import datetime\n",
    "import argparse\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "#change hyperparameter values in default argument.\n",
    "parser = argparse.ArgumentParser(description='Train video model with cross entropy loss')\n",
    "# Datasets\n",
    "parser.add_argument('-f')\n",
    "parser.add_argument('-d', '--dataset', type=str, default='mars', #duke\n",
    "                    choices=get_names())\n",
    "parser.add_argument('-j', '--workers', default=4, type=int,\n",
    "                    help=\"number of data loading workers (default: 4)\")\n",
    "parser.add_argument('--height', type=int, default=256,\n",
    "                    help=\"height of an image (default: 256)\")\n",
    "parser.add_argument('--width', type=int, default=128,\n",
    "                    help=\"width of an image (default: 128)\")\n",
    "# Augment\n",
    "parser.add_argument('--seq_len', type=int, default=4, help=\"number of images to sample in a tracklet\")\n",
    "parser.add_argument('--sample_stride', type=int, default=8, help=\"stride of images to sample in a tracklet\")\n",
    "parser.add_argument('--test_frames', default=4, type=int, help='frames/clip for test')\n",
    "# Optimization options\n",
    "parser.add_argument('--max_epoch', default=150, type=int,\n",
    "                    help=\"maximum epochs to run\")\n",
    "parser.add_argument('--start_epoch', default=0, type=int,\n",
    "                    help=\"manual epoch number (useful on restarts)\")\n",
    "parser.add_argument('--train_batch', default=32, type=int,\n",
    "                    help=\"train batch size\")\n",
    "parser.add_argument('--test_batch', default=1, type=int, help=\"has to be 1\")\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.0003, type=float,\n",
    "                    help=\"initial learning rate, use 0.0001 for rnn, use 0.0003 for pooling and attention\")\n",
    "parser.add_argument('--stepsize', default=[40, 80, 120], nargs='+', type=int,\n",
    "                    help=\"stepsize to decay learning rat-e\")\n",
    "parser.add_argument('--gamma', default=0.1, type=float,\n",
    "                    help=\"learning rate decay\")\n",
    "parser.add_argument('--weight_decay', default=5e-04, type=float,\n",
    "                    help=\"weight decay (default: 5e-04)\")\n",
    "parser.add_argument('--margin', type=float, default=0.3, help=\"margin for triplet loss\")\n",
    "parser.add_argument('--distance', type=str, default='consine', help=\"euclidean or consine\")\n",
    "parser.add_argument('--num_instances', type=int, default=4, help=\"number of instances per identity\")\n",
    "# Architecture\n",
    "parser.add_argument('-a', '--arch', type=str, default='STCANet3D')\n",
    "parser.add_argument('--save_dir', type=str, default='')\n",
    "parser.add_argument('--resume', type=str, default='', metavar='PATH')\n",
    "# Spatial Attention\n",
    "parser.add_argument('--alpha', default=0.01, type=float)\n",
    "# Miscs\n",
    "parser.add_argument('--seed', type=int, default=1, help=\"manual seed\")\n",
    "parser.add_argument('--evaluate', action='store_true', help=\"evaluation only\")\n",
    "parser.add_argument('--eval_step', type=int, default=10,\n",
    "                    help=\"run evaluation for every N epochs (set to -1 to test after training)\")\n",
    "parser.add_argument('--start_eval', type=int, default=1, help=\"start to evaluate after specific epoch\")\n",
    "parser.add_argument('--use_cpu', action='store_true', help=\"use cpu\")\n",
    "parser.add_argument('--gpu_devices', default='2', type=str, help='gpu device ids for CUDA_VISIBLE_DEVICES')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_devices\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    print(use_gpu)\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_devices\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    if args.use_cpu: use_gpu = False\n",
    "\n",
    "    if not args.evaluate:\n",
    "        sys.stdout = Logger(osp.join(args.save_dir, 'log_train.txt'))\n",
    "    else:\n",
    "        sys.stdout = Logger(osp.join(args.save_dir, 'log_test.txt'))\n",
    "    print(\"==========\\nArgs:{}\\n==========\".format(args))\n",
    "\n",
    "    if use_gpu:\n",
    "        print(\"Currently using GPU {}\".format(args.gpu_devices))\n",
    "        #cudnn.benchmark = True\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "    else:\n",
    "        print(\"Currently using CPU (GPU is highly recommended)\")\n",
    "\n",
    "    print(\"Initializing dataset {}\".format(args.dataset))\n",
    "    dataset = init_dataset(name=args.dataset)\n",
    "\n",
    "    # Data augmentation\n",
    "    spatial_transform_train = Compose([\n",
    "                Scale((args.height, args.width), interpolation=3),\n",
    "                RandomHorizontalFlip(),\n",
    "                ToTensor(),\n",
    "                Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                RandomErasing(),\n",
    "            ])\n",
    "\n",
    "    temporal_transform_train = TemporalRandomCrop(size=args.seq_len, stride=args.sample_stride)\n",
    "\n",
    "    spatial_transform_test = Compose([\n",
    "                Scale((args.height, args.width), interpolation=3),\n",
    "                ToTensor(),\n",
    "                Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "    temporal_transform_test = TemporalBeginCrop(size=args.test_frames)\n",
    "\n",
    "    pin_memory = True if use_gpu else False\n",
    "\n",
    "    dataset_train = dataset.train\n",
    "    if args.dataset != 'mars':\n",
    "        dataset_train = dataset.train_dense\n",
    "        print('process {} dataset'.format(args.dataset))\n",
    "\n",
    "    trainloader = DataLoader(\n",
    "        VideoDataset(dataset_train, spatial_transform=spatial_transform_train, temporal_transform=temporal_transform_train),\n",
    "        sampler=RandomIdentitySampler(dataset_train, num_instances=args.num_instances),\n",
    "        batch_size=args.train_batch, num_workers=args.workers,\n",
    "        pin_memory=pin_memory, drop_last=True,\n",
    "    )\n",
    "\n",
    "    queryloader = DataLoader(\n",
    "        VideoDataset(dataset.query, spatial_transform=spatial_transform_test, temporal_transform=temporal_transform_test),\n",
    "        batch_size=args.test_batch, shuffle=False, num_workers=args.workers,\n",
    "        pin_memory=pin_memory, drop_last=False\n",
    "    )\n",
    "\n",
    "    galleryloader = DataLoader(\n",
    "        VideoDataset(dataset.gallery, spatial_transform=spatial_transform_test, temporal_transform=temporal_transform_test),\n",
    "        batch_size=args.test_batch, shuffle=False, num_workers=args.workers,\n",
    "        pin_memory=pin_memory, drop_last=False\n",
    "    )\n",
    "\n",
    "    print(\"Initializing model: {}\".format(args.arch))\n",
    "    model = init_model(name=args.arch, num_classes=dataset.num_train_pids,use_gpu = use_gpu, loss={'xent', 'htri'})\n",
    "    print(model)\n",
    "    print(\"Model size: {:.5f}M\".format(sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "\n",
    "    criterion_xent = nn.CrossEntropyLoss() \n",
    "    if args.dataset == 'lsvid':\n",
    "        print('process lsvid with contrastive loss!')\n",
    "        criterion_htri = ContrastiveLoss()\n",
    "    else:\n",
    "        print('process {} with triplet loss!'.format(args.dataset))\n",
    "        criterion_htri = TripletLoss(margin=args.margin, distance=args.distance, use_gpu=use_gpu)\n",
    "     \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=args.stepsize, gamma=args.gamma)\n",
    "    start_epoch = args.start_epoch\n",
    "\n",
    "    if args.resume:\n",
    "        print(\"Loading checkpoint from '{}'\".format(args.resume))\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "\n",
    "    if use_gpu:\n",
    "        model = nn.DataParallel(model).cuda()\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_time = 0\n",
    "    best_mAP = -np.inf\n",
    "    best_epoch = 0\n",
    "    print(\"==> Start training\")\n",
    "\n",
    "    for epoch in range(start_epoch, args.max_epoch):\n",
    "\n",
    "        start_train_time = time.time()\n",
    "        train(epoch, model, criterion_xent, criterion_htri, optimizer, trainloader, use_gpu)\n",
    "        # torch.cuda.empty_cache()\n",
    "        train_time += round(time.time() - start_train_time)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if (epoch+1) >= args.start_eval and (epoch+1) % args.eval_step == 0 or epoch == 0:\n",
    "            print(\"==> Test\")\n",
    "            with torch.no_grad():\n",
    "                mAP = test(model, queryloader, galleryloader, use_gpu)\n",
    "                # torch.cuda.empty_cache()\n",
    "            is_best = mAP >= best_mAP\n",
    "            if is_best: \n",
    "                best_mAP = mAP\n",
    "                best_epoch = epoch + 1\n",
    "\n",
    "            if use_gpu:\n",
    "                state_dict = model.module.state_dict()\n",
    "            else:\n",
    "                state_dict = model.state_dict()\n",
    "            save_checkpoint({\n",
    "                'state_dict': state_dict,\n",
    "                'mAP': mAP,\n",
    "                'epoch': epoch,\n",
    "            }, is_best, osp.join(args.save_dir, 'checkpoint_ep' + str(epoch+1) + '.pth.tar'))\n",
    "\n",
    "    print(\"==> Best mAP {:.1%}, achieved at epoch {}\".format(best_mAP, best_epoch))\n",
    "\n",
    "    elapsed = round(time.time() - start_time)\n",
    "    elapsed = str(datetime.timedelta(seconds=elapsed))\n",
    "    train_time = str(datetime.timedelta(seconds=train_time))\n",
    "    print(\"Finished. Total elapsed time (h:m:s): {}. Training time (h:m:s): {}.\".format(elapsed, train_time))\n",
    "\n",
    "\n",
    "def train(epoch, model, criterion_xent, criterion_htri, optimizer, trainloader, use_gpu):\n",
    "    batch_xent_loss = AverageMeter()\n",
    "    batch_htri_loss = AverageMeter()\n",
    "    \n",
    "    batch_loss = AverageMeter()\n",
    "    batch_corrects = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for batch_idx, (vids, pids, _) in enumerate(trainloader):\n",
    "        if use_gpu:\n",
    "            vids, pids = vids.cuda(), pids.cuda()\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        outputs, features = model(vids)\n",
    "\n",
    "        # combine hard triplet loss with cross entropy loss\n",
    "        xent_loss = criterion_xent(outputs, pids)\n",
    "        htri_loss = criterion_htri(features, pids)\n",
    "        \n",
    "        loss = xent_loss + htri_loss\n",
    "\n",
    "        # backward + optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        batch_corrects.update(torch.sum(preds == pids.data).float()/pids.size(0), pids.size(0))\n",
    "        \n",
    "        batch_xent_loss.update(xent_loss.item(), pids.size(0))\n",
    "        batch_htri_loss.update(htri_loss.item(), pids.size(0))\n",
    "        batch_loss.update(loss.item(), pids.size(0)) \n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "    print('Epoch{0} '\n",
    "          'Time:{batch_time.sum:.1f}s '\n",
    "          'Data:{data_time.sum:.1f}s '\n",
    "          'Loss:{loss.avg:.4f} '\n",
    "          'Xent:{xent.avg:.4f} '\n",
    "          'Htri:{htri.avg:.4f} '\n",
    "          'Acc:{acc.avg:.2%} '.format(\n",
    "          epoch+1, batch_time=batch_time,\n",
    "          data_time=data_time, loss=batch_loss,\n",
    "          xent=batch_xent_loss, htri=batch_htri_loss,\n",
    "          acc=batch_corrects))\n",
    "\n",
    "\n",
    "def test(model, queryloader, galleryloader, use_gpu, ranks=[1, 5, 10, 20]):\n",
    "    since = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    qf, q_pids, q_camids = [], [], []\n",
    "    for batch_idx, (vids, pids, camids) in enumerate(queryloader):\n",
    "        if use_gpu:\n",
    "            vids = vids.cuda()\n",
    "        feat = model(vids)\n",
    "        #feat = feat.mean(1)\n",
    "        feat = model.module.bn(feat)\n",
    "        feat = feat.data.cpu()\n",
    "\n",
    "        qf.append(feat)\n",
    "        q_pids.extend(pids)\n",
    "        q_camids.extend(camids)\n",
    "\n",
    "    qf = torch.cat(qf, 0)\n",
    "    q_pids = np.asarray(q_pids)\n",
    "    q_camids = np.asarray(q_camids)\n",
    "    print(\"Extracted features for query set, obtained {} matrix\".format(qf.shape))\n",
    "\n",
    "    gf, g_pids, g_camids = [], [], []\n",
    "    for batch_idx, (vids, pids, camids) in enumerate(galleryloader):\n",
    "        if use_gpu:\n",
    "            vids = vids.cuda()\n",
    "        feat = model(vids)\n",
    "       # feat = feat.mean(1)\n",
    "        feat = model.module.bn(feat)\n",
    "        feat = feat.data.cpu()\n",
    "\n",
    "        gf.append(feat)\n",
    "        g_pids.extend(pids)\n",
    "        g_camids.extend(camids)\n",
    "\n",
    "    gf = torch.cat(gf, 0)\n",
    "    g_pids = np.asarray(g_pids)\n",
    "    g_camids = np.asarray(g_camids)\n",
    "\n",
    "    if args.dataset == 'mars' or args.dataset == 'lsvid':\n",
    "        print('process the dataset {}!'.format(args.dataset))\n",
    "        # gallery set must contain query set, otherwise 140 query imgs will not have ground truth.\n",
    "        gf = torch.cat((qf, gf), 0)\n",
    "        g_pids = np.append(q_pids, g_pids)\n",
    "        g_camids = np.append(q_camids, g_camids)\n",
    "\n",
    "    print(\"Extracted features for gallery set, obtained {} matrix\".format(gf.shape))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Extracting features complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print(\"Computing distance matrix\")\n",
    "    m, n = qf.size(0), gf.size(0)\n",
    "    distmat = torch.zeros((m,n))\n",
    "\n",
    "    if args.distance == 'euclidean':\n",
    "        distmat = torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\n",
    "                  torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\n",
    "        distmat.addmm_(1, -2, qf, gf.t())\n",
    "    else:\n",
    "        q_norm = torch.norm(qf, p=2, dim=1, keepdim=True)\n",
    "        g_norm = torch.norm(gf, p=2, dim=1, keepdim=True)\n",
    "        qf = qf.div(q_norm.expand_as(qf))\n",
    "        gf = gf.div(g_norm.expand_as(gf))\n",
    "        distmat = - torch.mm(qf, gf.t())\n",
    "    distmat = distmat.numpy()\n",
    "\n",
    "    print(\"Computing CMC and mAP\")\n",
    "    cmc, mAP = evaluate(distmat, q_pids, g_pids, q_camids, g_camids)\n",
    "\n",
    "    print(\"Results ----------\")\n",
    "    print(\"mAP: {:.2%}\".format(mAP))\n",
    "    print(\"CMC curve\")\n",
    "    for r in ranks:\n",
    "        print(\"Rank-{:<3}: {:.2%}\".format(r, cmc[r-1]))\n",
    "    print(\"------------------\")\n",
    "\n",
    "    return mAP\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rs-C8Dbay0iM"
   },
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579,
     "referenced_widgets": [
      "253fc2ff27fe4065be4aa5baeb449a73",
      "d61552d8f96444239dd8cd85e3617ced",
      "6206218c04254c9a8e64fcd487436ea5",
      "047d1e8b1beb42a9b0702cf6e1685e69",
      "5466dd887a804eb783ee110a2be4336a",
      "64844084f50b4b099b91c45f8e2124ba",
      "c28a5b74ed674b5daf0527337dbc25fb",
      "ce2ccc5169084fa68ee16cba96c3c791",
      "fb0e1eb4f92f496e853e99c3c1ca942a",
      "3bf977baad4049a99efa216113229993",
      "92f71c0ac6e34b27abeaafb1d8224978"
     ]
    },
    "id": "NQT2YpAny2mU",
    "outputId": "1d766a04-ccb9-4bfd-eea7-2a9b246f3089"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import h5py\n",
    "import scipy\n",
    "import datetime\n",
    "import argparse\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "#chnage hyper-parameters in default argument\n",
    "parser = argparse.ArgumentParser(description='Train video model with cross entropy loss')\n",
    "# Datasets\n",
    "parser.add_argument('-f')\n",
    "parser.add_argument('-d', '--dataset', type=str, default='mars', #mars\n",
    "                    choices=get_names())\n",
    "parser.add_argument('-j', '--workers', default=4, type=int,\n",
    "                    help=\"number of data loading workers (default: 4)\")\n",
    "parser.add_argument('--height', type=int, default=256,\n",
    "                    help=\"height of an image (default: 256)\")\n",
    "parser.add_argument('--width', type=int, default=128,\n",
    "                    help=\"width of an image (default: 128)\")\n",
    "# Augment\n",
    "parser.add_argument('--seq_len', type=int, default=4, help=\"number of images to sample in a tracklet\")\n",
    "parser.add_argument('--sample_stride', type=int, default=8, help=\"stride of images to sample in a tracklet\")\n",
    "parser.add_argument('--test_frames', default=4, type=int, help='frames/clip for test')\n",
    "# Optimization options\n",
    "parser.add_argument('--distance', type=str, default='consine', help=\"euclidean or consine\")\n",
    "# Architecture\n",
    "parser.add_argument('-a', '--arch', type=str, default='STCANet3D')\n",
    "parser.add_argument('--save-dir', type=str, default='')\n",
    "parser.add_argument('--resume', type=str, default='', metavar='PATH')\n",
    "# Miscs\n",
    "parser.add_argument('--seed', type=int, default=1, help=\"manual seed\")\n",
    "parser.add_argument('--use_cpu', action='store_true', help=\"use cpu\")\n",
    "parser.add_argument('--gpu_devices', default='2', type=str, help='gpu device ids for CUDA_VISIBLE_DEVICES')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    torch.manual_seed(args.seed)\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_devices\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    if args.use_cpu: use_gpu = False\n",
    "\n",
    "    sys.stdout = Logger(osp.join(args.save_dir, 'log_test.txt'))\n",
    "    print(\"==========\\nArgs:{}\\n==========\".format(args))\n",
    "\n",
    "    if use_gpu:\n",
    "        print(\"Currently using GPU {}\".format(args.gpu_devices))\n",
    "        #cudnn.benchmark = True\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "    else:\n",
    "        print(\"Currently using CPU (GPU is highly recommended)\")\n",
    "\n",
    "    print(\"Initializing dataset {}\".format(args.dataset))\n",
    "    dataset = init_dataset(name=args.dataset)\n",
    "\n",
    "    # Data augmentation\n",
    "    spatial_transform_test = Compose([\n",
    "                Scale((args.height, args.width), interpolation=3),\n",
    "                ToTensor(),\n",
    "                Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "    temporal_transform_test = None\n",
    "\n",
    "    queryloader = DataLoader(\n",
    "        VideoDataset(dataset.query, spatial_transform=spatial_transform_test, temporal_transform=temporal_transform_test),\n",
    "        batch_size=1, shuffle=False, num_workers=1, pin_memory=True, drop_last=False\n",
    "    )\n",
    "\n",
    "    galleryloader = DataLoader(\n",
    "        VideoDataset(dataset.gallery, spatial_transform=spatial_transform_test, temporal_transform=temporal_transform_test),\n",
    "        batch_size=1, shuffle=False, num_workers=1, pin_memory=True, drop_last=False\n",
    "    )\n",
    "\n",
    "    print(\"Initializing model: {}\".format(args.arch))\n",
    "    model = init_model(name=args.arch, num_classes=dataset.num_train_pids,use_gpu = use_gpu)\n",
    "    model.cuda()\n",
    "    print(\"Model size: {:.5f}M\".format(sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "\n",
    "    if args.resume:\n",
    "        print(\"Loading checkpoint from '{}'\".format(args.resume))\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        test_all_frames(model, queryloader, galleryloader, use_gpu)\n",
    "\n",
    "    if not args.resume:\n",
    "        for epoch in [150, 140, 130, 120]:\n",
    "            weights = os.path.join(args.save_dir, 'checkpoint_ep'+str(epoch)+'.pth.tar')\n",
    "            if not os.path.isfile(weights):\n",
    "                continue\n",
    "            print(\"Loading checkpoint from {}\".format(weights))\n",
    "            checkpoint = torch.load(weights)\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            test_all_frames(model, queryloader, galleryloader, use_gpu)\n",
    "\n",
    "\n",
    "def test_all_frames(model, queryloader, galleryloader, use_gpu):\n",
    "    model = nn.DataParallel(model)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        evaluation(model, args, queryloader, galleryloader, use_gpu)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "VideoReID IAUNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "047d1e8b1beb42a9b0702cf6e1685e69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb0e1eb4f92f496e853e99c3c1ca942a",
      "max": 102502400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ce2ccc5169084fa68ee16cba96c3c791",
      "value": 102502400
     }
    },
    "0b99cc3c3df64399b4fdd31517cee86c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7dd2296ec8074bad905b67ad05234cb0",
       "IPY_MODEL_d208e12ae6b24f709aa68f13cab79794",
       "IPY_MODEL_3e5a6874253e4bbdb559036a0df4ee67"
      ],
      "layout": "IPY_MODEL_f69933f53d6b475689ead0c415664d96"
     }
    },
    "0bad18fe09c348fb82a261d95a1c46ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "253fc2ff27fe4065be4aa5baeb449a73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6206218c04254c9a8e64fcd487436ea5",
       "IPY_MODEL_047d1e8b1beb42a9b0702cf6e1685e69",
       "IPY_MODEL_5466dd887a804eb783ee110a2be4336a"
      ],
      "layout": "IPY_MODEL_d61552d8f96444239dd8cd85e3617ced"
     }
    },
    "3bf977baad4049a99efa216113229993": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3e5a6874253e4bbdb559036a0df4ee67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c02c7561feb04392a7affc019d670e89",
      "placeholder": "​",
      "style": "IPY_MODEL_e07dae26bf514fca808da6e01c869cc3",
      "value": " 97.8M/97.8M [00:00&lt;00:00, 132MB/s]"
     }
    },
    "5466dd887a804eb783ee110a2be4336a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_92f71c0ac6e34b27abeaafb1d8224978",
      "placeholder": "​",
      "style": "IPY_MODEL_3bf977baad4049a99efa216113229993",
      "value": " 97.8M/97.8M [00:01&lt;00:00, 92.0MB/s]"
     }
    },
    "548d1de03b054c0997ffcd4daf037def": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "59ff1e04732c4ab3bf9517d8b88654c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6206218c04254c9a8e64fcd487436ea5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c28a5b74ed674b5daf0527337dbc25fb",
      "placeholder": "​",
      "style": "IPY_MODEL_64844084f50b4b099b91c45f8e2124ba",
      "value": "100%"
     }
    },
    "64844084f50b4b099b91c45f8e2124ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7dd2296ec8074bad905b67ad05234cb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_59ff1e04732c4ab3bf9517d8b88654c4",
      "placeholder": "​",
      "style": "IPY_MODEL_548d1de03b054c0997ffcd4daf037def",
      "value": "100%"
     }
    },
    "84498de0b3084be9ba473e203c0e9296": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92f71c0ac6e34b27abeaafb1d8224978": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c02c7561feb04392a7affc019d670e89": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c28a5b74ed674b5daf0527337dbc25fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce2ccc5169084fa68ee16cba96c3c791": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d208e12ae6b24f709aa68f13cab79794": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84498de0b3084be9ba473e203c0e9296",
      "max": 102502400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0bad18fe09c348fb82a261d95a1c46ac",
      "value": 102502400
     }
    },
    "d61552d8f96444239dd8cd85e3617ced": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e07dae26bf514fca808da6e01c869cc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f69933f53d6b475689ead0c415664d96": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb0e1eb4f92f496e853e99c3c1ca942a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
