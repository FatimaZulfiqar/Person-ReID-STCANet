{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a jupyter notebook file for the implementation of paper **\"Multi-Camera Person Re-Identification using Spatiotemporal Context Modeling\"**<br><br>\n",
    "Author: Fatima Zulfiqar, Usama Ijaz Bajwa, Rana Hammad Raza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0GrxOq7AeiG"
   },
   "source": [
    "# Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T15:18:13.384268Z",
     "iopub.status.busy": "2021-08-01T15:18:13.383879Z",
     "iopub.status.idle": "2021-08-01T15:18:13.482819Z",
     "shell.execute_reply": "2021-08-01T15:18:13.481991Z",
     "shell.execute_reply.started": "2021-08-01T15:18:13.384185Z"
    },
    "executionInfo": {
     "elapsed": 330,
     "status": "ok",
     "timestamp": 1641574477275,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "Fb2OX4T_AeiG"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import imdb\n",
    "\n",
    "\n",
    "class BaseImgDataset(object):\n",
    "    def __init__(self):\n",
    "        self.train_lmdb_path = None\n",
    "        self.query_lmdb_path = None\n",
    "        self.gallery_lmdb_path = None\n",
    "\n",
    "    def generate_lmdb(self):\n",
    "        assert isinstance(self.train, list)\n",
    "        assert isinstance(self.query, list)\n",
    "        assert isinstance(self.gallery, list)\n",
    "        \n",
    "        print(\"Reminder: this function is under development, some datasets might not be applicable yet\")\n",
    "\n",
    "        self.train_lmdb_path = osp.join(self.dataset_dir, 'train_lmdb')\n",
    "        self.query_lmdb_path = osp.join(self.dataset_dir, 'query_lmdb')\n",
    "        self.gallery_lmdb_path = osp.join(self.dataset_dir, 'gallery_lmdb')\n",
    "\n",
    "        def _write_lmdb(write_path, data_list):\n",
    "            if osp.exists(write_path):\n",
    "                return\n",
    "            \n",
    "            print(\"Generating lmdb files to '{}'\".format(write_path))\n",
    "            \n",
    "            num_data = len(data_list)\n",
    "            max_map_size = int(num_data * 500**2 * 3) # be careful with this\n",
    "            env = lmdb.open(write_path, map_size=max_map_size)\n",
    "            \n",
    "            for img_path, pid, camid in data_list:\n",
    "                with env.begin(write=True) as txn:\n",
    "                    with open(img_path, 'rb') as imgf:\n",
    "                        imgb = imgf.read()\n",
    "                    txn.put(img_path, imgb)\n",
    "\n",
    "        _write_lmdb(self.train_lmdb_path, self.train)\n",
    "        _write_lmdb(self.query_lmdb_path, self.query)\n",
    "        _write_lmdb(self.gallery_lmdb_path, self.gallery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTX3m3rZAeiH"
   },
   "source": [
    "# Load Market1501 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-08-01T15:18:16.380656Z",
     "iopub.status.busy": "2021-08-01T15:18:16.380329Z",
     "iopub.status.idle": "2021-08-01T15:20:02.612938Z",
     "shell.execute_reply": "2021-08-01T15:20:02.611948Z",
     "shell.execute_reply.started": "2021-08-01T15:18:16.380626Z"
    },
    "executionInfo": {
     "elapsed": 6737,
     "status": "ok",
     "timestamp": 1641574487117,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "SYulFo0ZAeiI",
    "outputId": "6e734a01-248e-41b6-b89b-f49c1237bd53"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import sys\n",
    "import urllib\n",
    "import tarfile\n",
    "import zipfile\n",
    "import os.path as osp\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import h5py\n",
    "#from scipy.misc import imsave\n",
    "\n",
    "#from base import BaseImgDataset\n",
    "\n",
    "\n",
    "class Market1501_seg(BaseImgDataset):\n",
    "    \"\"\"\n",
    "    Market1501\n",
    "\n",
    "    Reference:\n",
    "    Zheng et al. Scalable Person Re-identification: A Benchmark. ICCV 2015.\n",
    "\n",
    "    URL: http://www.liangzheng.org/Project/project_reid.html\n",
    "    \n",
    "    Dataset statistics:\n",
    "    # identities: 1501 (+1 for background)\n",
    "    # images: 12936 (train) + 3368 (query) + 15913 (gallery)\n",
    "    \"\"\"\n",
    "    dataset_dir = '../Market-1501-v15.09.15' #path to dataset\n",
    "\n",
    "    def __init__(self, verbose=True, use_lmdb=False, **kwargs):\n",
    "        super(Market1501_seg, self).__init__()\n",
    "        self.train_dir = osp.join(self.dataset_dir, 'bounding_box_train')\n",
    "        self.query_dir = osp.join(self.dataset_dir, 'query')\n",
    "        self.gallery_dir = osp.join(self.dataset_dir, 'bounding_box_test')\n",
    "\n",
    "        self.train_seg_dir = osp.join(self.dataset_dir, 'Market1501_train_seg_part4')\n",
    "\n",
    "        self._check_before_run()\n",
    "\n",
    "        train, num_train_pids, num_train_imgs = self._process_dir(self.train_dir, self.train_seg_dir, relabel=True)\n",
    "        query, num_query_pids, num_query_imgs = self._process_dir(self.query_dir, relabel=False)\n",
    "        gallery, num_gallery_pids, num_gallery_imgs = self._process_dir(self.gallery_dir, relabel=False)\n",
    "        num_total_pids = num_train_pids + num_query_pids\n",
    "        num_total_imgs = num_train_imgs + num_query_imgs + num_gallery_imgs\n",
    "\n",
    "        if verbose:\n",
    "            print(\"=> Market1501 loaded\")\n",
    "            print(\"Dataset statistics:\")\n",
    "            print(\"  ------------------------------\")\n",
    "            print(\"  subset   | # ids | # images\")\n",
    "            print(\"  ------------------------------\")\n",
    "            print(\"  train    | {:5d} | {:8d}\".format(num_train_pids, num_train_imgs))\n",
    "            print(\"  query    | {:5d} | {:8d}\".format(num_query_pids, num_query_imgs))\n",
    "            print(\"  gallery  | {:5d} | {:8d}\".format(num_gallery_pids, num_gallery_imgs))\n",
    "            print(\"  ------------------------------\")\n",
    "            print(\"  total    | {:5d} | {:8d}\".format(num_total_pids, num_total_imgs))\n",
    "            print(\"  ------------------------------\")\n",
    "\n",
    "        self.train = train\n",
    "        self.query = query\n",
    "        self.gallery = gallery\n",
    "\n",
    "        self.num_train_pids = num_train_pids\n",
    "        self.num_query_pids = num_query_pids\n",
    "        self.num_gallery_pids = num_gallery_pids\n",
    "\n",
    "        if use_lmdb:\n",
    "            self.generate_lmdb()\n",
    "\n",
    "    def _check_before_run(self):\n",
    "        \"\"\"Check if all files are available before going deeper\"\"\"\n",
    "        if not osp.exists(self.dataset_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.dataset_dir))\n",
    "        if not osp.exists(self.train_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.train_dir))\n",
    "        if not osp.exists(self.query_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.query_dir))\n",
    "        if not osp.exists(self.gallery_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.gallery_dir))\n",
    "        if not osp.exists(self.train_seg_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.train_dir))\n",
    "\n",
    "    def _process_dir(self, dir_path, seg_dir_path=None, relabel=False):\n",
    "        img_paths = glob.glob(osp.join(dir_path, '*.jpg'))\n",
    "        pattern = re.compile(r'([-\\d]+)_c(\\d)')\n",
    "\n",
    "        pid_container = set()\n",
    "        for img_path in img_paths:\n",
    "            pid, _ = map(int, pattern.search(img_path).groups())\n",
    "            if pid == -1: continue  # junk images are just ignored\n",
    "            pid_container.add(pid)\n",
    "        pid2label = {pid:label for label, pid in enumerate(pid_container)}\n",
    "\n",
    "        dataset = []\n",
    "        for img_path in img_paths:\n",
    "            pid, camid = map(int, pattern.search(img_path).groups())\n",
    "            if pid == -1: continue  # junk images are just ignored\n",
    "            assert 0 <= pid <= 1501  # pid == 0 means background\n",
    "            assert 1 <= camid <= 6\n",
    "            camid -= 1 # index starts from 0\n",
    "            if relabel: pid = pid2label[pid]\n",
    "\n",
    "            if seg_dir_path is not None:\n",
    "                img_name = img_path.split('/')[-1].split('.')[0]\n",
    "                seg_dir = osp.join(seg_dir_path, img_name)\n",
    "                head_path = osp.join(seg_dir, 'head.png')\n",
    "                upper_body_path = osp.join(seg_dir, 'upper_clothes.png')\n",
    "                lower_body_path = osp.join(seg_dir, 'lower_clothes.png')\n",
    "                shoes_path = osp.join(seg_dir, 'shoes.png')\n",
    "                foreground_path = osp.join(seg_dir, 'foreground.png')\n",
    "\n",
    "                assert os.path.exists(head_path)\n",
    "                assert os.path.exists(upper_body_path)\n",
    "                assert os.path.exists(lower_body_path)\n",
    "                assert os.path.exists(shoes_path)\n",
    "                assert os.path.exists(foreground_path)\n",
    "                img_path = [img_path, head_path, upper_body_path, lower_body_path, shoes_path, foreground_path]\n",
    "\n",
    "            dataset.append((img_path, pid, camid))\n",
    "\n",
    "        num_pids = len(pid_container)\n",
    "        num_imgs = len(dataset)\n",
    "        return dataset, num_pids, num_imgs\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    Market1501_seg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VsCAmvGAeiL"
   },
   "source": [
    "# Load DukeMTMC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1247,
     "status": "ok",
     "timestamp": 1628166119939,
     "user": {
      "displayName": "Owais Khan",
      "photoUrl": "",
      "userId": "17477675400592215484"
     },
     "user_tz": -300
    },
    "id": "geKQLklUAeiL",
    "outputId": "3496bb5c-22c1-4f60-c66b-d547c98d7819"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import sys\n",
    "import urllib\n",
    "import tarfile\n",
    "import zipfile\n",
    "import os.path as osp\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import h5py\n",
    "#from scipy.misc import imsave\n",
    "\n",
    "#from base import BaseImgDataset\n",
    "\n",
    "\n",
    "class DukeMTMC_seg(BaseImgDataset):\n",
    "    \"\"\"\n",
    "    DukeMTMC\n",
    "\n",
    "    Reference:\n",
    "    Zheng et al. Scalable Person Re-identification: A Benchmark. ICCV 2015.\n",
    "\n",
    "    URL: http://www.liangzheng.org/Project/project_reid.html\n",
    "    \n",
    "    Dataset statistics:\n",
    "    # identities: 1501 (+1 for background)\n",
    "    # images: 12936 (train) + 3368 (query) + 15913 (gallery)\n",
    "    \"\"\"\n",
    "    dataset_dir = '.../DukeMTMC-reID' # path to dataset\n",
    "\n",
    "    def __init__(self, verbose=True, use_lmdb=False, **kwargs):\n",
    "        super(DukeMTMC_seg, self).__init__()\n",
    "        self.train_dir = osp.join(self.dataset_dir, 'bounding_box_train')\n",
    "        self.query_dir = osp.join(self.dataset_dir, 'query')\n",
    "        self.gallery_dir = osp.join(self.dataset_dir, 'bounding_box_test')\n",
    "\n",
    "        self.train_seg_dir = osp.join(self.dataset_dir, 'DukeMTMC_train_seg_part4')\n",
    "\n",
    "        self._check_before_run()\n",
    "\n",
    "        train, num_train_pids, num_train_imgs = self._process_dir(self.train_dir, self.train_seg_dir, relabel=True)\n",
    "        query, num_query_pids, num_query_imgs = self._process_dir(self.query_dir, relabel=False)\n",
    "        gallery, num_gallery_pids, num_gallery_imgs = self._process_dir(self.gallery_dir, relabel=False)\n",
    "        num_total_pids = num_train_pids + num_query_pids\n",
    "        num_total_imgs = num_train_imgs + num_query_imgs + num_gallery_imgs\n",
    "\n",
    "        if verbose:\n",
    "            print(\"=> DukeMTMC-ReID loaded\")\n",
    "            print(\"Dataset statistics:\")\n",
    "            print(\"  ------------------------------\")\n",
    "            print(\"  subset   | # ids | # images\")\n",
    "            print(\"  ------------------------------\")\n",
    "            print(\"  train    | {:5d} | {:8d}\".format(num_train_pids, num_train_imgs))\n",
    "            print(\"  query    | {:5d} | {:8d}\".format(num_query_pids, num_query_imgs))\n",
    "            print(\"  gallery  | {:5d} | {:8d}\".format(num_gallery_pids, num_gallery_imgs))\n",
    "            print(\"  ------------------------------\")\n",
    "            print(\"  total    | {:5d} | {:8d}\".format(num_total_pids, num_total_imgs))\n",
    "            print(\"  ------------------------------\")\n",
    "\n",
    "        self.train = train\n",
    "        self.query = query\n",
    "        self.gallery = gallery\n",
    "\n",
    "        self.num_train_pids = num_train_pids\n",
    "        self.num_query_pids = num_query_pids\n",
    "        self.num_gallery_pids = num_gallery_pids\n",
    "\n",
    "        if use_lmdb:\n",
    "            self.generate_lmdb()\n",
    "\n",
    "    def _check_before_run(self):\n",
    "        \"\"\"Check if all files are available before going deeper\"\"\"\n",
    "        if not osp.exists(self.dataset_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.dataset_dir))\n",
    "        if not osp.exists(self.train_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.train_dir))\n",
    "        if not osp.exists(self.query_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.query_dir))\n",
    "        if not osp.exists(self.gallery_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.gallery_dir))\n",
    "        if not osp.exists(self.train_seg_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.train_dir))\n",
    "\n",
    "    def _process_dir(self, dir_path, seg_dir_path=None, relabel=False):\n",
    "        img_paths = glob.glob(osp.join(dir_path, '*.jpg'))\n",
    "        pattern = re.compile(r'([-\\d]+)_c(\\d)')\n",
    "\n",
    "        pid_container = set()\n",
    "        for img_path in img_paths:\n",
    "            pid, _ = map(int, pattern.search(img_path).groups())\n",
    "            #rint(pid)\n",
    "            if pid == -1: continue  # junk images are just ignored\n",
    "            pid_container.add(pid)\n",
    "        pid2label = {pid:label for label, pid in enumerate(pid_container)}\n",
    "\n",
    "        dataset = []\n",
    "        for img_path in img_paths:\n",
    "            pid, camid = map(int, pattern.search(img_path).groups())\n",
    "            #print(pid)\n",
    "            if pid == -1: continue  # junk images are just ignored\n",
    "            #rint(pid)\n",
    "            assert 0 <= pid <= 7140                           \n",
    "            assert 1 <= camid <= 8\n",
    "            camid -= 1 # index starts from 0\n",
    "            if relabel: pid = pid2label[pid]\n",
    "\n",
    "            if seg_dir_path is not None:\n",
    "                img_name = img_path.split('/')[-1].split('.')[0]\n",
    "                seg_dir = osp.join(seg_dir_path, img_name)\n",
    "                head_path = osp.join(seg_dir, 'head.png')\n",
    "                upper_body_path = osp.join(seg_dir, 'upper_clothes.png')\n",
    "                lower_body_path = osp.join(seg_dir, 'lower_clothes.png')\n",
    "                shoes_path = osp.join(seg_dir, 'shoes.png')\n",
    "                foreground_path = osp.join(seg_dir, 'foreground.png')\n",
    "\n",
    "                assert os.path.exists(head_path)\n",
    "                assert os.path.exists(upper_body_path)\n",
    "                assert os.path.exists(lower_body_path)\n",
    "                assert os.path.exists(shoes_path)\n",
    "                assert os.path.exists(foreground_path)\n",
    "                img_path = [img_path, head_path, upper_body_path, lower_body_path, shoes_path, foreground_path]\n",
    "\n",
    "            dataset.append((img_path, pid, camid))\n",
    "\n",
    "        num_pids = len(pid_container)\n",
    "        #rint(\"No of ID's: \",num_pids)\n",
    "        num_imgs = len(dataset)\n",
    "        #rint(\"No of Images: \",num_imgs)\n",
    "        return dataset, num_pids, num_imgs\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    DukeMTMC_seg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UFt7KBEAeiP"
   },
   "source": [
    "# Load MSMT17 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1792,
     "status": "ok",
     "timestamp": 1628671139542,
     "user": {
      "displayName": "Owais Khan",
      "photoUrl": "",
      "userId": "17477675400592215484"
     },
     "user_tz": -300
    },
    "id": "RRffbZhwAeiR",
    "outputId": "166fbe3b-543f-4530-d8a4-fd8a4e256d33"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import sys\n",
    "import urllib\n",
    "import tarfile\n",
    "import zipfile\n",
    "import os.path as osp\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "#from ..dataset import ImageDataset\n",
    "\n",
    "# Log\n",
    "# 22.01.2019\n",
    "# - add v2\n",
    "# - v1 and v2 differ in dir names\n",
    "# - note that faces in v2 are blurred\n",
    "TRAIN_DIR_KEY = 'train_dir'\n",
    "TEST_DIR_KEY = 'test_dir'\n",
    "VERSION_DICT = {\n",
    "    'MSMT17_V1': {\n",
    "        TRAIN_DIR_KEY: 'train',\n",
    "        TEST_DIR_KEY: 'test',\n",
    "    },\n",
    "    'MSMT17_V2': {\n",
    "        TRAIN_DIR_KEY: 'mask_train_v2',\n",
    "        TEST_DIR_KEY: 'mask_test_v2',\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "class MSMT17_seg(BaseImgDataset):\n",
    "    \"\"\"MSMT17.\n",
    "    Reference:\n",
    "        Wei et al. Person Transfer GAN to Bridge Domain Gap for Person Re-Identification. CVPR 2018.\n",
    "    URL: `<http://www.pkuvmc.com/publications/msmt17.html>`_\n",
    "    \n",
    "    Dataset statistics:\n",
    "        - identities: 4101.\n",
    "        - images: 32621 (train) + 11659 (query) + 82161 (gallery).\n",
    "        - cameras: 15.\n",
    "    \"\"\"\n",
    "    dataset_dir = '/content/' # set path to dataset\n",
    "    \n",
    "    def __init__(self, verbose=True, use_lmdb=False, **kwargs):\n",
    "        super(MSMT17_seg, self).__init__()\n",
    "\n",
    "        has_main_dir = False\n",
    "        \n",
    "        for main_dir in VERSION_DICT:\n",
    "            if osp.exists(osp.join(self.dataset_dir, main_dir)):\n",
    "                train_dir = VERSION_DICT[main_dir][TRAIN_DIR_KEY]\n",
    "                test_dir = VERSION_DICT[main_dir][TEST_DIR_KEY]\n",
    "                has_main_dir = True        \n",
    "                break\n",
    "        \n",
    "        assert has_main_dir, 'Dataset folder not found'\n",
    "\n",
    "        self.train_dir = osp.join(self.dataset_dir, main_dir, train_dir)\n",
    "        self.test_dir = osp.join(self.dataset_dir, main_dir, test_dir)\n",
    "        self.train_seg_dir = osp.join(self.dataset_dir, main_dir, train_dir, 'MSMT17_train_seg_part4')\n",
    "        \n",
    "        self.list_train_path = osp.join(self.dataset_dir, main_dir, 'list_train.txt')\n",
    "        self.list_train_seg_path = osp.join(self.dataset_dir, main_dir, 'list_train.txt')\n",
    "        self.list_val_path = osp.join(self.dataset_dir, main_dir, 'list_val.txt')\n",
    "        self.list_query_path = osp.join(self.dataset_dir, main_dir, 'list_query.txt')\n",
    "        self.list_gallery_path = osp.join(self.dataset_dir, main_dir, 'list_gallery.txt')\n",
    "\n",
    "        #required_files = [self.dataset_dir, self.train_dir, self.test_dir, self.train_seg_dir]\n",
    "        self._check_before_run() # or pass required_files variable that is declared above\n",
    "        \n",
    "        train, num_train_pids, num_train_imgs = self.process_dir(self.train_dir, self.list_train_path, self.train_seg_dir, relabel = True )\n",
    "        val, num_val_pids, num_val_imgs = self.process_dir(self.train_dir, self.list_val_path, relabel = False)\n",
    "        query, num_query_pids, num_query_imgs = self.process_dir(self.test_dir, self.list_query_path, relabel = False)\n",
    "        gallery, num_gallery_pids, num_gallery_imgs = self.process_dir(self.test_dir, self.list_gallery_path, relabel = False)\n",
    "        \n",
    "        num_total_pids = num_train_pids + num_query_pids\n",
    "        num_total_imgs = num_train_imgs + num_val_imgs + num_query_imgs + num_gallery_imgs\n",
    "\n",
    "        # Note: to fairly compare with published methods on the conventional ReID setting,\n",
    "        #       do not add val images to the training set.\n",
    "        if 'combineall' in kwargs and kwargs['combineall']:\n",
    "            train += val\n",
    "\n",
    "        if verbose:\n",
    "            print(\"=> MSMT17 loaded\")\n",
    "            print(\"Dataset statistics:\")\n",
    "            print(\"  ------------------------------\")\n",
    "            print(\"  subset   | # ids | # images\")\n",
    "            print(\"  ------------------------------\")\n",
    "            print(\"  train    | {:5d} | {:8d}\".format(num_train_pids, num_train_imgs))\n",
    "            print(\"  val      | {:5d} | {:8d}\".format(num_val_pids, num_val_imgs))\n",
    "            print(\"  query    | {:5d} | {:8d}\".format(num_query_pids, num_query_imgs))\n",
    "            print(\"  gallery  | {:5d} | {:8d}\".format(num_gallery_pids, num_gallery_imgs))\n",
    "            print(\"  ------------------------------\")\n",
    "            print(\"  total    | {:5d} | {:8d}\".format(num_total_pids, num_total_imgs))\n",
    "            print(\"  ------------------------------\")\n",
    "\n",
    "        self.train = train\n",
    "        self.val = val\n",
    "        self.query = query\n",
    "        self.gallery = gallery\n",
    "\n",
    "        self.num_train_pids = num_train_pids\n",
    "        self.num_val_pids = num_val_pids\n",
    "        self.num_query_pids = num_query_pids\n",
    "        self.num_gallery_pids = num_gallery_pids\n",
    "\n",
    "        if use_lmdb:\n",
    "            self.generate_lmdb()\n",
    "\n",
    "    def _check_before_run(self):\n",
    "        \"\"\"Check if all files are available before going deeper\"\"\"\n",
    "        if not osp.exists(self.dataset_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.dataset_dir))\n",
    "        if not osp.exists(self.train_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.train_dir))\n",
    "        if not osp.exists(self.test_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.test_dir))\n",
    "        if not osp.exists(self.list_val_path):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.list_val_path))\n",
    "        if not osp.exists(self.list_query_path):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.list_query_path))\n",
    "        if not osp.exists(self.list_gallery_path):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.list_gallery_path))\n",
    "        if not osp.exists(self.train_seg_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.train_seg_dir)) # train_dir\n",
    "\n",
    "    def process_dir(self, dir_path, list_path, seg_dir_path=None, relabel=False ): \n",
    "        with open(list_path, 'r') as txt:\n",
    "            lines = txt.readlines()\n",
    "\n",
    "        dataset = []\n",
    "        pid_container = set()\n",
    "        for img_idx, img_info in enumerate(lines):\n",
    "            img_path, pid = img_info.split(' ')\n",
    "            pid = int(pid) # no need to relabel\n",
    "            pid_container.add(pid)\n",
    "            camid = int(img_path.split('_')[2]) - 1 # index starts from 0\n",
    "            img_path = osp.join(dir_path, img_path)\n",
    "            \n",
    "            if seg_dir_path is not None:\n",
    "                img_path = img_path.replace('\\\\','/')\n",
    "                img_name = img_path.split('/')[-1].split('.')[0]\n",
    "                seg_dir = osp.join(seg_dir_path, img_name)\n",
    "                head_path = osp.join(seg_dir, 'head.png')\n",
    "                upper_body_path = osp.join(seg_dir, 'upper_clothes.png')\n",
    "                lower_body_path = osp.join(seg_dir, 'lower_clothes.png')\n",
    "                shoes_path = osp.join(seg_dir, 'shoes.png')\n",
    "                foreground_path = osp.join(seg_dir, 'foreground.png')\n",
    "\n",
    "                assert os.path.exists(head_path)\n",
    "                assert os.path.exists(upper_body_path)\n",
    "                assert os.path.exists(lower_body_path)\n",
    "                assert os.path.exists(shoes_path)\n",
    "                assert os.path.exists(foreground_path)\n",
    "                img_path = [img_path, head_path, upper_body_path, lower_body_path, shoes_path, foreground_path]\n",
    "\n",
    "            dataset.append((img_path, pid, camid))\n",
    "\n",
    "\n",
    "        num_pids = len(pid_container)\n",
    "        #rint(\"No of ID's: \",num_pids)\n",
    "        num_imgs = len(dataset)\n",
    "        #rint(\"No of Images: \",num_imgs)\n",
    "        \n",
    "        return dataset, num_pids, num_imgs\n",
    "     \n",
    "if __name__ == '__main__':\n",
    "    MSMT17_seg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhWOUqr1AeiW"
   },
   "source": [
    "# Initialize Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4wijdpJAeiX"
   },
   "source": [
    "run market_seg.py and uncomment market_seg to train for market1501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T15:22:37.648396Z",
     "iopub.status.busy": "2021-08-01T15:22:37.648011Z",
     "iopub.status.idle": "2021-08-01T15:22:37.659201Z",
     "shell.execute_reply": "2021-08-01T15:22:37.657358Z",
     "shell.execute_reply.started": "2021-08-01T15:22:37.648358Z"
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1641574496065,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "_5f3M9ClAeiX"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "__imgreid_factory = {\n",
    "    'market1501_seg': Market1501_seg#,\n",
    "   #'DukeMTMC_seg': DukeMTMC_seg#,\n",
    "   #'MSMT17_seg': MSMT17_seg\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def get_names():\n",
    "    return list(__imgreid_factory.keys())\n",
    "\n",
    "\n",
    "def init_imgreid_dataset(name, **kwargs):\n",
    "    if name not in list(__imgreid_factory.keys()):\n",
    "        raise KeyError(\"Invalid dataset, got '{}', but expected to be one of {}\".format(name, list(__imgreid_factory.keys())))\n",
    "    return __imgreid_factory[name](**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOgfkTmwAeiX"
   },
   "source": [
    "# models Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7n_127BuAeiX"
   },
   "source": [
    "# Spatial Attention Module (SAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T15:22:39.740224Z",
     "iopub.status.busy": "2021-08-01T15:22:39.739896Z",
     "iopub.status.idle": "2021-08-01T15:22:40.418792Z",
     "shell.execute_reply": "2021-08-01T15:22:40.417822Z",
     "shell.execute_reply.started": "2021-08-01T15:22:39.740192Z"
    },
    "executionInfo": {
     "elapsed": 6264,
     "status": "ok",
     "timestamp": 1641574506469,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "gICcoJ3CAeiY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class Gconv(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Gconv, self).__init__()\n",
    "        fsm_blocks = []\n",
    "        fsm_blocks.append(nn.Conv2d(in_channels * 2, in_channels, 1))\n",
    "        fsm_blocks.append(nn.BatchNorm2d(in_channels))\n",
    "        fsm_blocks.append(nn.ReLU(inplace=True))\n",
    "        self.fsm = nn.Sequential(*fsm_blocks)\n",
    "        # init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, W, x):\n",
    "        bs, n, c = x.size()\n",
    "\n",
    "        x_neighbor = torch.bmm(W, x)  \n",
    "        x = torch.cat([x, x_neighbor], 2) \n",
    "        x = x.view(-1, x.size(2), 1, 1) \n",
    "        x = self.fsm(x) \n",
    "        x = x.view(bs, n, c)\n",
    "        return x \n",
    "\n",
    "\n",
    "class Wcompute(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Wcompute, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        edge_block = []\n",
    "        edge_block.append(nn.Conv2d(in_channels * 2, 1, 1))\n",
    "        edge_block.append(nn.BatchNorm2d(1))\n",
    "        self.relation = nn.Sequential(*edge_block)\n",
    "\n",
    "        #init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "        \n",
    "\n",
    "    def forward(self, x, W_id, y):\n",
    "        bs, N, C = x.size()\n",
    "\n",
    "        W1 = x.unsqueeze(2) \n",
    "        W2 = torch.transpose(W1, 1, 2) \n",
    "        W_new = torch.abs(W1 - W2)\n",
    "        W_new = torch.transpose(W_new, 1, 3) \n",
    "        y = y.view(bs, C, 1, 1).expand_as(W_new)\n",
    "        W_new = torch.cat((W_new, y), 1) \n",
    "\n",
    "        W_new = self.relation(W_new) \n",
    "        W_new = torch.transpose(W_new, 1, 3) \n",
    "        W_new = W_new.squeeze(3) \n",
    "\n",
    "        W_new = W_new - W_id.expand_as(W_new) * 1e8\n",
    "        W_new = F.softmax(W_new, dim=2)\n",
    "        return W_new\n",
    "\n",
    "\n",
    "class SAM(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SAM, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.module_w = Wcompute(in_channels)\n",
    "        self.module_l = Gconv(in_channels)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        bs, N, C = x.size()\n",
    "\n",
    "        W_init = torch.eye(N).unsqueeze(0) \n",
    "        W_init = W_init.repeat(bs, 1, 1).cuda() \n",
    "        W = self.module_w(x, W_init, y) \n",
    "        s = self.module_l(W, x) \n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7A0tDYUAeiY"
   },
   "source": [
    "# Interconnection, Accumulation Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T15:22:40.420872Z",
     "iopub.status.busy": "2021-08-01T15:22:40.420487Z",
     "iopub.status.idle": "2021-08-01T15:22:40.447481Z",
     "shell.execute_reply": "2021-08-01T15:22:40.446542Z",
     "shell.execute_reply.started": "2021-08-01T15:22:40.420835Z"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1641574506470,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "ahkyR5HJAeiY"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_grid(h, w):\n",
    "    x = np.linspace(0, w-1, w)\n",
    "    y = np.linspace(0, h-1, h)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    xv = xv.flatten()\n",
    "    yv = yv.flatten()\n",
    "    return xv, yv\n",
    "\n",
    "def generate_gaussian(height, width, alpha_x, alpha_y):\n",
    "    Dis = np.zeros((height*width, height*width))\n",
    "    xv, yv = generate_grid(height, width)\n",
    "    for i in range(0, width):\n",
    "        for j in range(0, height):\n",
    "            d = (np.square(xv - i))/ (2 * alpha_x**2)  + (np.square(yv - j)) / (2 * alpha_y**2)\n",
    "            Dis[i+j*width] = -1 *  d \n",
    "    Dis = torch.from_numpy(Dis).float()\n",
    "    Dis = F.softmax(Dis, dim=-1)\n",
    "    return Dis\n",
    "\n",
    "# Interonnection and accumulation operation\n",
    "class _IABlockND(nn.Module):\n",
    "    def __init__(self, in_channels, height, width,\n",
    "            alpha_x, alpha_y):\n",
    "        super(_IABlockND, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        conv_nd = nn.Conv2d\n",
    "        max_pool = nn.MaxPool2d\n",
    "        bn = nn.BatchNorm2d\n",
    "\n",
    "        self.Dis = generate_gaussian(height=height, width=width, alpha_x=alpha_x, alpha_y=alpha_y)\n",
    "        self.W1 = bn(self.in_channels)\n",
    "        self.W2 = bn(self.in_channels)\n",
    "        \n",
    "        # init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, conv_nd):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, bn):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        nn.init.constant_(self.W1.weight.data, 0.0)\n",
    "        nn.init.constant_(self.W1.bias.data, 0.0)\n",
    "        nn.init.constant_(self.W2.weight.data, 0.0)\n",
    "        nn.init.constant_(self.W2.bias.data, 0.0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: (b, c, h, w)\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        g_x = x.view(batch_size, self.in_channels, -1)\n",
    "        g_x = g_x.permute(0, 2, 1)\n",
    "\n",
    "        f_cluster = []\n",
    "        f_loc = torch.unsqueeze(self.Dis.cuda(), 0)\n",
    "        f_loc = f_loc.expand(batch_size, -1, -1)\n",
    "        f_cluster.append(torch.unsqueeze(f_loc, 1))\n",
    "\n",
    "        theta_x = x.view(batch_size, self.in_channels, -1)\n",
    "        theta_x = theta_x.permute(0, 2, 1) #[B, H*W, C]\n",
    "        phi_x = x.view(batch_size, self.in_channels, -1)\n",
    "        f = torch.matmul(theta_x, phi_x) #[B, H*W, H*W]\n",
    "        f = f / np.sqrt(self.in_channels)\n",
    "        f = F.softmax(f, dim=-1)\n",
    "        f_cluster.append(torch.unsqueeze(f, 1))\n",
    "        \n",
    "        f_cluster = torch.cat(f_cluster, 1)\n",
    "        f = torch.prod(f_cluster, dim=1)\n",
    "        f = F.softmax(f, dim=-1)\n",
    "\n",
    "        y = torch.matmul(f, g_x)\n",
    "        y = y.permute(0, 2, 1).contiguous()\n",
    "        y = y.view(batch_size, self.in_channels, *x.size()[2:])\n",
    "        y = self.W1(y)\n",
    "        z = y + x\n",
    "\n",
    "        x = z\n",
    "        g_x = x.view(batch_size, self.in_channels, -1) #[B, c, h*w]\n",
    "        theta_x = g_x #[B, c, h*w]\n",
    "        phi_x = g_x.permute(0, 2, 1) #[B, h*w, c]\n",
    "        f = torch.matmul(theta_x, phi_x) #[B, c, c]\n",
    "        f = F.softmax(f, dim=-1)\n",
    "        y = torch.matmul(f, g_x)\n",
    "        y = y.view(batch_size, self.in_channels, *x.size()[2:])\n",
    "        y = self.W2(y)\n",
    "        z = y + x\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "\n",
    "class IABlock2D(_IABlockND):\n",
    "    def __init__(self, in_channels, height, width, alpha_x, alpha_y, **kwargs):\n",
    "        super(IABlock2D, self).__init__(in_channels, height=height, width=width,\n",
    "                                alpha_x=alpha_x, alpha_y=alpha_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDn7aDnsAeiZ"
   },
   "source": [
    "# Interconnetion, Accumulation, and Transformation Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T15:22:40.596707Z",
     "iopub.status.busy": "2021-08-01T15:22:40.596446Z",
     "iopub.status.idle": "2021-08-01T15:22:40.621667Z",
     "shell.execute_reply": "2021-08-01T15:22:40.620581Z",
     "shell.execute_reply.started": "2021-08-01T15:22:40.596682Z"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1641574506471,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "zRlABmSHAeiZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Basic convolutional block\"\"\"\n",
    "    def __init__(self, in_c, out_c, k, s=1, p=0):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_c, out_c, k, stride=s, padding=p)\n",
    "        self.bn = nn.BatchNorm2d(out_c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bn(self.conv(x))\n",
    "\n",
    "\n",
    "class SpatialAttn(nn.Module):\n",
    "    \"\"\"Spatial Attention \"\"\"\n",
    "    def __init__(self, in_channels, number):\n",
    "        super(SpatialAttn, self).__init__()\n",
    "        self.conv = ConvBlock(in_channels, number, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x) \n",
    "        a = torch.sigmoid(x)\n",
    "        return a\n",
    "\n",
    "\n",
    "class IAT(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(IAT, self).__init__()\n",
    "\n",
    "        inter_stride = 2\n",
    "        self.in_channels = in_channels\n",
    "        conv_nd = nn.Conv2d\n",
    "        bn = nn.BatchNorm2d\n",
    "        self.inter_channels = in_channels // inter_stride\n",
    "\n",
    "        self.sa = SpatialAttn(in_channels, number=4)\n",
    "\n",
    "        self.g = conv_nd(self.in_channels, self.inter_channels, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.SAM = SAM(self.inter_channels)\n",
    "\n",
    "        self.W1 = nn.Sequential(\n",
    "                conv_nd(self.in_channels, self.in_channels,\n",
    "                    kernel_size=1, stride=1, padding=0, bias=True),\n",
    "                bn(self.in_channels)\n",
    "            )\n",
    "        \n",
    "        self.W2 = nn.Sequential(\n",
    "                conv_nd(self.in_channels, self.in_channels,\n",
    "                    kernel_size=1, stride=1, padding=0, bias=True),\n",
    "                bn(self.in_channels)\n",
    "            )\n",
    "        \n",
    "        # init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, conv_nd):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, bn):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        nn.init.constant_(self.W1[1].weight.data, 0.0)\n",
    "        nn.init.constant_(self.W1[1].bias.data, 0.0)\n",
    "        nn.init.constant_(self.W2[1].weight.data, 0.0)\n",
    "        nn.init.constant_(self.W2[1].bias.data, 0.0)\n",
    "\n",
    "\n",
    "    def reduce_dimension(self, x, global_node):\n",
    "        bs, c = global_node.size()\n",
    "\n",
    "        x = x.transpose(1, 2).unsqueeze(3) \n",
    "        x = torch.cat((x, global_node.view(bs, c, 1, 1)), 2) \n",
    "        x = self.g(x).squeeze(3) \n",
    "\n",
    "        global_node = x[:,:,-1] \n",
    "        x = x[:,:,:-1].transpose(1, 2) \n",
    "        return x, global_node\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CAM\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        g_x = x.view(batch_size, self.in_channels, -1)\n",
    "        theta_x = g_x \n",
    "        phi_x = g_x.permute(0, 2, 1) \n",
    "        f = torch.matmul(theta_x, phi_x) \n",
    "        f = F.softmax(f, dim=-1)\n",
    "        y = torch.matmul(f, g_x)\n",
    "        y = y.view(batch_size, self.in_channels, *x.size()[2:])\n",
    "        y = self.W1(y)\n",
    "        z = y + x\n",
    "\n",
    "        # SAM\n",
    "        x = z\n",
    "        inputs = x\n",
    "        b, c, h, w = x.size()\n",
    "        u = x.view(b, c, -1).mean(2)\n",
    "\n",
    "        a = self.sa(x) \n",
    "        x = torch.bmm(a.view(b, -1, h * w), x.view(b, c, -1).transpose(1, 2)) \n",
    "        x, u = self.reduce_dimension(x, u)\n",
    "        y = self.SAM(x, u) \n",
    "\n",
    "        y = torch.mean(y, 1) #[b, c//2]\n",
    "        u = torch.cat((y, u), 1) \n",
    "\n",
    "        y = self.W2(u.view(u.size(0), u.size(1), 1, 1))\n",
    "        z = y + inputs\n",
    "        return z, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgtyMaTiAeia"
   },
   "source": [
    "# ResNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T15:22:41.040946Z",
     "iopub.status.busy": "2021-08-01T15:22:41.040613Z",
     "iopub.status.idle": "2021-08-01T15:22:41.080577Z",
     "shell.execute_reply": "2021-08-01T15:22:41.079526Z",
     "shell.execute_reply.started": "2021-08-01T15:22:41.040916Z"
    },
    "executionInfo": {
     "elapsed": 630,
     "status": "ok",
     "timestamp": 1641574509562,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "vURGlxfeAeia"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50_s1', 'resnet101',\n",
    "           'resnet152']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3])\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet18_s1(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34_s1(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50_s1(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet101_s1(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet152(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBU-4ZjkAeic"
   },
   "source": [
    "# STCANet2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T15:22:41.478253Z",
     "iopub.status.busy": "2021-08-01T15:22:41.477907Z",
     "iopub.status.idle": "2021-08-01T15:22:41.599546Z",
     "shell.execute_reply": "2021-08-01T15:22:41.598688Z",
     "shell.execute_reply.started": "2021-08-01T15:22:41.478221Z"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1641574509564,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "LjjGw0omAeic"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "\n",
    "class STCANet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(STCANet, self).__init__()\n",
    "        resnet50 = resnet50_s1(pretrained=True)\n",
    "\n",
    "        self.conv1 = resnet50.conv1\n",
    "        self.bn1 = resnet50.bn1\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = resnet50.maxpool\n",
    "\n",
    "        self.layer1 = resnet50.layer1\n",
    "        self.layer2 = resnet50.layer2\n",
    "        self.layer3 = resnet50.layer3 \n",
    "        self.layer4 = resnet50.layer4 \n",
    "\n",
    "        self.IAT2 = IAT(512) \n",
    "        self.IAT3 = IAT(1024)\n",
    "\n",
    "        self.feat_dim = 2048\n",
    "        self.bn = nn.BatchNorm1d(self.feat_dim)\n",
    "        self.classifier = nn.Linear(self.feat_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.conv1(x))\n",
    "        x = self.maxpool(self.relu(x))\n",
    "\n",
    "        x1 = self.layer1(x) \n",
    "\n",
    "        x2 = self.layer2(x1) \n",
    "        x2, a2 = self.IAT2(x2) \n",
    "\n",
    "        x3 = self.layer3(x2) \n",
    "        x3, a3 = self.IAT3(x3) \n",
    "\n",
    "        x4 = self.layer4(x3) \n",
    "\n",
    "        f = F.avg_pool2d(x4, x4.size()[2:])\n",
    "        f = f.view(f.size(0), -1)\n",
    "        f = self.bn(f)\n",
    "        y = self.classifier(f)\n",
    "\n",
    "        a_head = [a2[:,0:1], a3[:,0:1]]\n",
    "        a_upper = [a2[:,1:2], a3[:,1:2]]\n",
    "        a_lower = [a2[:,2:3], a3[:,2:3]]\n",
    "        a_shoes = [a2[:,3:4], a3[:,3:4]]\n",
    "\n",
    "        return y, f, a_head, a_upper, a_lower, a_shoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AooGxog_Aeie"
   },
   "source": [
    "# IAResnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T15:22:42.333662Z",
     "iopub.status.busy": "2021-08-01T15:22:42.333287Z",
     "iopub.status.idle": "2021-08-01T15:22:42.349559Z",
     "shell.execute_reply": "2021-08-01T15:22:42.348518Z",
     "shell.execute_reply.started": "2021-08-01T15:22:42.333630Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1641574517945,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "1xvMG7JFAeif"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "\n",
    "class IAResNet50_location(nn.Module):\n",
    "\n",
    "    def __init__(self, last_s1, num_classes,  **kwargs):\n",
    "\n",
    "        super(IAResNet50_location, self).__init__()\n",
    "\n",
    "        if not last_s1:\n",
    "            resnet50 = torchvision.models.resnet50(pretrained=True)\n",
    "        else:\n",
    "            resnet50 = resnet50_s1(pretrained=True)\n",
    "        \n",
    "        self.conv1 = resnet50.conv1\n",
    "        self.bn1 = resnet50.bn1\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = resnet50.maxpool\n",
    "\n",
    "        self.layer1 = self._inflate_reslayer(resnet50.layer1)\n",
    "        self.layer2 = self._inflate_reslayer(resnet50.layer2, IA_idx=[3], height=32,\n",
    "                                    width=16, alpha_x=10, alpha_y=20, IA_channels=512)\n",
    "        self.layer3 = self._inflate_reslayer(resnet50.layer3, IA_idx=[5], height=16,\n",
    "                                    width=8, alpha_x=5, alpha_y=10, IA_channels=1024)\n",
    "        self.layer4 = self._inflate_reslayer(resnet50.layer4)\n",
    "\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(2048)\n",
    "        self.classifier = nn.Linear(2048, num_classes)\n",
    "\n",
    "\n",
    "    def _inflate_reslayer(self, reslayer, height=0, width=0,\n",
    "                    alpha_x=0, alpha_y=0, IA_idx=[], IA_channels=0):\n",
    "        reslayers = []\n",
    "        for i, layer2d in enumerate(reslayer):\n",
    "            reslayers.append(layer2d)\n",
    "\n",
    "            if i in IA_idx:\n",
    "                IA_block = IABlock2D(in_channels=IA_channels, height=height,\n",
    "                        width=width, alpha_x=alpha_x, alpha_y=alpha_y)\n",
    "                reslayers.append(IA_block)\n",
    "\n",
    "        return nn.Sequential(*reslayers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        f = F.avg_pool2d(x, x.size()[2:])\n",
    "        f = f.view(f.size(0), -1)\n",
    "        f = self.bn(f)\n",
    "        if not self.training:\n",
    "            return f\n",
    "        y = self.classifier(f)\n",
    "\n",
    "        return y, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CK3cTVcAeid"
   },
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T15:22:41.905931Z",
     "iopub.status.busy": "2021-08-01T15:22:41.905559Z",
     "iopub.status.idle": "2021-08-01T15:22:41.914714Z",
     "shell.execute_reply": "2021-08-01T15:22:41.913617Z",
     "shell.execute_reply.started": "2021-08-01T15:22:41.905896Z"
    },
    "executionInfo": {
     "elapsed": 375,
     "status": "ok",
     "timestamp": 1641576528518,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "decOTWvxAeie"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "__model_factory = {\n",
    "        'STCANet': STCANet,\n",
    "}\n",
    "\n",
    "\n",
    "def get_names():\n",
    "    return list(__model_factory.keys())\n",
    "\n",
    "\n",
    "def init_model(name, *args, **kwargs):\n",
    "    if name not in list(__model_factory.keys()):\n",
    "        raise KeyError(\"Unknown model: {}\".format(name))\n",
    "    return __model_factory[name](*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AisO9uFjAeig"
   },
   "source": [
    "# Metrics Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvs-YWr3Aeig"
   },
   "source": [
    "# distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T15:22:43.287792Z",
     "iopub.status.busy": "2021-08-01T15:22:43.287451Z",
     "iopub.status.idle": "2021-08-01T15:22:43.300469Z",
     "shell.execute_reply": "2021-08-01T15:22:43.299327Z",
     "shell.execute_reply.started": "2021-08-01T15:22:43.287746Z"
    },
    "executionInfo": {
     "elapsed": 570,
     "status": "ok",
     "timestamp": 1641574519862,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "9WduigSJAeig"
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def compute_distance_matrix(input1, input2, metric): # default metric = 'euclidean'\n",
    "    \n",
    "    \"\"\"A wrapper function for computing distance matrix.\n",
    "    Args:\n",
    "        input1 (torch.Tensor): 2-D feature matrix.\n",
    "        input2 (torch.Tensor): 2-D feature matrix.\n",
    "        metric (str, optional): \"euclidean\" or \"cosine\".\n",
    "            Default is \"euclidean\".\n",
    "    Returns:\n",
    "        torch.Tensor: distance matrix.\n",
    "    Examples::\n",
    "       >>> from torchreid import metrics\n",
    "       >>> input1 = torch.rand(10, 2048)\n",
    "       >>> input2 = torch.rand(100, 2048)\n",
    "       >>> distmat = metrics.compute_distance_matrix(input1, input2)\n",
    "       >>> distmat.size() # (10, 100)\n",
    "    \"\"\"\n",
    "    # check input\n",
    "    assert isinstance(input1, torch.Tensor)\n",
    "    assert isinstance(input2, torch.Tensor)\n",
    "    assert input1.dim() == 2, 'Expected 2-D tensor, but got {}-D'.format(\n",
    "        input1.dim()\n",
    "    )\n",
    "    assert input2.dim() == 2, 'Expected 2-D tensor, but got {}-D'.format(\n",
    "        input2.dim()\n",
    "    )\n",
    "    assert input1.size(1) == input2.size(1)\n",
    "\n",
    "    if metric == 'euclidean':\n",
    "        distmat = euclidean_squared_distance(input1, input2)\n",
    "    elif metric == 'cosine':\n",
    "        distmat = cosine_distance(input1, input2)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Unknown distance metric: {}. '\n",
    "            'Please choose either \"euclidean\" or \"cosine\"'.format(metric)\n",
    "        )\n",
    "\n",
    "    return distmat\n",
    "\n",
    "\n",
    "def euclidean_squared_distance(input1, input2):\n",
    "    \"\"\"Computes euclidean squared distance.\n",
    "    Args:\n",
    "        input1 (torch.Tensor): 2-D feature matrix.\n",
    "        input2 (torch.Tensor): 2-D feature matrix.\n",
    "    Returns:\n",
    "        torch.Tensor: distance matrix.\n",
    "    \"\"\"\n",
    "    m, n = input1.size(0), input2.size(0)\n",
    "    mat1 = torch.pow(input1, 2).sum(dim=1, keepdim=True).expand(m, n)\n",
    "    mat2 = torch.pow(input2, 2).sum(dim=1, keepdim=True).expand(n, m).t()\n",
    "    distmat = mat1 + mat2\n",
    "    distmat.addmm_(input1, input2.t(), beta=1, alpha=-2)\n",
    "    return distmat\n",
    "\n",
    "\n",
    "def cosine_distance(input1, input2):\n",
    "    \"\"\"Computes cosine distance.\n",
    "    Args:\n",
    "        input1 (torch.Tensor): 2-D feature matrix.\n",
    "        input2 (torch.Tensor): 2-D feature matrix.\n",
    "    Returns:\n",
    "        torch.Tensor: distance matrix.\n",
    "    \"\"\"\n",
    "    input1_normed = F.normalize(input1, p=2, dim=1)\n",
    "    input2_normed = F.normalize(input2, p=2, dim=1)\n",
    "    distmat = 1 - torch.mm(input1_normed, input2_normed.t())\n",
    "    return distmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIBR4kDEAeih"
   },
   "source": [
    "# Utils Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YBXIGvVAeih"
   },
   "source": [
    "# averagemeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T15:22:44.261106Z",
     "iopub.status.busy": "2021-08-01T15:22:44.260774Z",
     "iopub.status.idle": "2021-08-01T15:22:44.267782Z",
     "shell.execute_reply": "2021-08-01T15:22:44.266554Z",
     "shell.execute_reply.started": "2021-08-01T15:22:44.261076Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1641574528568,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "YSmi7Fs1Aeih"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value.\n",
    "       \n",
    "       Code imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-IDgYImAeih"
   },
   "source": [
    "# iotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T15:22:44.907269Z",
     "iopub.status.busy": "2021-08-01T15:22:44.906952Z",
     "iopub.status.idle": "2021-08-01T15:22:44.918164Z",
     "shell.execute_reply": "2021-08-01T15:22:44.917186Z",
     "shell.execute_reply.started": "2021-08-01T15:22:44.907239Z"
    },
    "executionInfo": {
     "elapsed": 4387,
     "status": "ok",
     "timestamp": 1641574538387,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "37qvelJ6Aeii"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import errno\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def mkdir_if_missing(directory):\n",
    "    if not osp.exists(directory):\n",
    "        try:\n",
    "            os.makedirs(directory)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "\n",
    "def check_isfile(path):\n",
    "    isfile = osp.isfile(path)\n",
    "    if not isfile:\n",
    "        print(\"=> Warning: no file found at '{}' (ignored)\".format(path))\n",
    "    return isfile\n",
    "\n",
    "\n",
    "def read_json(fpath):\n",
    "    with open(fpath, 'r') as f:\n",
    "        obj = json.load(f)\n",
    "    return obj\n",
    "\n",
    "\n",
    "def write_json(obj, fpath):\n",
    "    mkdir_if_missing(osp.dirname(fpath))\n",
    "    with open(fpath, 'w') as f:\n",
    "        json.dump(obj, f, indent=4, separators=(',', ': '))\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best=False, fpath='checkpoint.pth.tar'):\n",
    "    if len(osp.dirname(fpath)) != 0:\n",
    "        mkdir_if_missing(osp.dirname(fpath))\n",
    "    torch.save(state, fpath)\n",
    "    if is_best:\n",
    "        shutil.copy(fpath, osp.join(osp.dirname(fpath), 'best_model.pth.tar'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AKPiwDKAeii"
   },
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T15:22:45.529705Z",
     "iopub.status.busy": "2021-08-01T15:22:45.529224Z",
     "iopub.status.idle": "2021-08-01T15:22:45.540001Z",
     "shell.execute_reply": "2021-08-01T15:22:45.539017Z",
     "shell.execute_reply.started": "2021-08-01T15:22:45.529670Z"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1641574539735,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "0RpgDVOHAeii"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "#from .iotools import mkdir_if_missing\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "    \"\"\"\n",
    "    Write console output to external text file.\n",
    "    Code imported from https://github.com/Cysu/open-reid/blob/master/reid/utils/logging.py.\n",
    "    \"\"\"\n",
    "    def __init__(self, fpath=None, mode='w'):\n",
    "        self.console = sys.stdout\n",
    "        self.file = None\n",
    "        if fpath is not None:\n",
    "            mkdir_if_missing(osp.dirname(fpath))\n",
    "            self.file = open(fpath, mode)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "    def __enter__(self):\n",
    "        pass\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.close()\n",
    "\n",
    "    def write(self, msg):\n",
    "        self.console.write(msg)\n",
    "        if self.file is not None:\n",
    "            self.file.write(msg)\n",
    "\n",
    "    def flush(self):\n",
    "        self.console.flush()\n",
    "        if self.file is not None:\n",
    "            self.file.flush()\n",
    "            os.fsync(self.file.fileno())\n",
    "\n",
    "    def close(self):\n",
    "        self.console.close()\n",
    "        if self.file is not None:\n",
    "            self.file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syTFroR6Aeii"
   },
   "source": [
    "# Reidtools (Visulaization of ranked results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7625rR8Su-c"
   },
   "source": [
    "## New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 606,
     "status": "ok",
     "timestamp": 1641574540336,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "TgZTEOl4St0B"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os.path as osp\n",
    "import cv2\n",
    "\n",
    "#from .tools import mkdir_if_missing\n",
    "\n",
    "__all__ = ['visualize_ranked_results']\n",
    "\n",
    "GRID_SPACING = 10\n",
    "QUERY_EXTRA_SPACING = 90\n",
    "BW = 5 # border width\n",
    "GREEN = (0, 255, 0)\n",
    "RED = (0, 0, 255)\n",
    "\n",
    "\n",
    "def visualize_ranked_results(\n",
    "    distmat, dataset, data_type, width=128, height=256, save_dir='', topk=10\n",
    "):\n",
    "    \"\"\"Visualizes ranked results.\n",
    "    Supports both image-reid and video-reid.\n",
    "    For image-reid, ranks will be plotted in a single figure. For video-reid, ranks will be\n",
    "    saved in folders each containing a tracklet.\n",
    "    Args:\n",
    "        distmat (numpy.ndarray): distance matrix of shape (num_query, num_gallery).\n",
    "        dataset (tuple): a 2-tuple containing (query, gallery), each of which contains\n",
    "            tuples of (img_path(s), pid, camid, dsetid).\n",
    "        data_type (str): \"image\" or \"video\".\n",
    "        width (int, optional): resized image width. Default is 128.\n",
    "        height (int, optional): resized image height. Default is 256.\n",
    "        save_dir (str): directory to save output images.\n",
    "        topk (int, optional): denoting top-k images in the rank list to be visualized.\n",
    "            Default is 10.\n",
    "    \"\"\"\n",
    "    num_q, num_g = distmat.shape\n",
    "    mkdir_if_missing(save_dir)\n",
    "\n",
    "    print('# query: {}\\n# gallery {}'.format(num_q, num_g))\n",
    "    print('Visualizing top-{} ranks ...'.format(topk))\n",
    "\n",
    "    #query, gallery = dataset\n",
    "    #assert num_q == len(query)\n",
    "    #assert num_g == len(gallery)\n",
    "    assert num_q == len(dataset.query)\n",
    "    assert num_g == len(dataset.gallery)\n",
    "    indices = np.argsort(distmat, axis=1)\n",
    "\n",
    "    def _cp_img_to(src, dst, rank, prefix, matched=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: image path or tuple (for vidreid)\n",
    "            dst: target directory\n",
    "            rank: int, denoting ranked position, starting from 1\n",
    "            prefix: string\n",
    "            matched: bool\n",
    "        \"\"\"\n",
    "        if isinstance(src, (tuple, list)):\n",
    "            if prefix == 'gallery':\n",
    "                suffix = 'TRUE' if matched else 'FALSE'\n",
    "                dst = osp.join(\n",
    "                    dst, prefix + '_top' + str(rank).zfill(3)\n",
    "                ) + '_' + suffix\n",
    "            else:\n",
    "                dst = osp.join(dst, prefix + '_top' + str(rank).zfill(3))\n",
    "            mkdir_if_missing(dst)\n",
    "            for img_path in src:\n",
    "                shutil.copy(img_path, dst)\n",
    "        else:\n",
    "            dst = osp.join(\n",
    "                dst, prefix + '_top' + str(rank).zfill(3) + '_name_' +\n",
    "                osp.basename(src)\n",
    "            )\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "    for q_idx in range(num_q):\n",
    "        qimg_path, qpid, qcamid = dataset.query[q_idx][:3]\n",
    "        qimg_path_name = qimg_path[0] if isinstance(\n",
    "            qimg_path, (tuple, list)\n",
    "        ) else qimg_path\n",
    "\n",
    "        if data_type == 'image':\n",
    "            qimg = cv2.imread(qimg_path)\n",
    "            qimg = cv2.resize(qimg, (width, height))\n",
    "            qimg = cv2.copyMakeBorder(\n",
    "                qimg, BW, BW, BW, BW, cv2.BORDER_CONSTANT, value=(0, 0, 0)\n",
    "            )\n",
    "            # resize twice to ensure that the border width is consistent across images\n",
    "            qimg = cv2.resize(qimg, (width, height))\n",
    "            num_cols = topk + 1\n",
    "            grid_img = 255 * np.ones(\n",
    "                (\n",
    "                    height,\n",
    "                    num_cols*width + topk*GRID_SPACING + QUERY_EXTRA_SPACING, 3\n",
    "                ),\n",
    "                dtype=np.uint8\n",
    "            )\n",
    "            grid_img[:, :width, :] = qimg\n",
    "        else:\n",
    "            qdir = osp.join(\n",
    "                save_dir, osp.basename(osp.splitext(qimg_path_name)[0])\n",
    "            )\n",
    "            mkdir_if_missing(qdir)\n",
    "            _cp_img_to(qimg_path, qdir, rank=0, prefix='query')\n",
    "\n",
    "        rank_idx = 1\n",
    "        for g_idx in indices[q_idx, :]:\n",
    "            gimg_path, gpid, gcamid = dataset.gallery[g_idx][:3]\n",
    "            invalid = (qpid == gpid) & (qcamid == gcamid)\n",
    "\n",
    "            if not invalid:\n",
    "                matched = gpid == qpid\n",
    "                if data_type == 'image':\n",
    "                    border_color = GREEN if matched else RED\n",
    "                    gimg = cv2.imread(gimg_path)\n",
    "                    gimg = cv2.resize(gimg, (width, height))\n",
    "                    gimg = cv2.copyMakeBorder(\n",
    "                        gimg,\n",
    "                        BW,\n",
    "                        BW,\n",
    "                        BW,\n",
    "                        BW,\n",
    "                        cv2.BORDER_CONSTANT,\n",
    "                        value=border_color\n",
    "                    )\n",
    "                    gimg = cv2.resize(gimg, (width, height))\n",
    "                    start = rank_idx*width + rank_idx*GRID_SPACING + QUERY_EXTRA_SPACING\n",
    "                    end = (\n",
    "                        rank_idx+1\n",
    "                    ) * width + rank_idx*GRID_SPACING + QUERY_EXTRA_SPACING\n",
    "                    grid_img[:, start:end, :] = gimg\n",
    "                else:\n",
    "                    _cp_img_to(\n",
    "                        gimg_path,\n",
    "                        qdir,\n",
    "                        rank=rank_idx,\n",
    "                        prefix='gallery',\n",
    "                        matched=matched\n",
    "                    )\n",
    "\n",
    "                rank_idx += 1\n",
    "                if rank_idx > topk:\n",
    "                    break\n",
    "\n",
    "        if data_type == 'image':\n",
    "            imname = osp.basename(osp.splitext(qimg_path_name)[0])\n",
    "            cv2.imwrite(osp.join(save_dir, imname + '.jpg'), grid_img)\n",
    "\n",
    "        if (q_idx+1) % 100 == 0:\n",
    "            print('- done {}/{}'.format(q_idx + 1, num_q))\n",
    "\n",
    "    print('Done. Images have been saved to \"{}\" ...'.format(save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnPXhIvQSzBx"
   },
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T15:22:46.144790Z",
     "iopub.status.busy": "2021-08-01T15:22:46.144449Z",
     "iopub.status.idle": "2021-08-01T15:22:46.158258Z",
     "shell.execute_reply": "2021-08-01T15:22:46.156990Z",
     "shell.execute_reply.started": "2021-08-01T15:22:46.144760Z"
    },
    "executionInfo": {
     "elapsed": 517,
     "status": "ok",
     "timestamp": 1641555878282,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "LFA6nmj9Aeij"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "\n",
    "#from .iotools import mkdir_if_missing\n",
    "\n",
    "\n",
    "def visualize_ranked_results(distmat, dataset, save_dir='log/ranked_results', topk=20):\n",
    "    \"\"\"\n",
    "    Visualize ranked results\n",
    "\n",
    "    Support both imgreid and vidreid\n",
    "\n",
    "    Args:\n",
    "    - distmat: distance matrix of shape (num_query, num_gallery).\n",
    "    - dataset: has dataset.query and dataset.gallery, both are lists of (img_path, pid, camid);\n",
    "               for imgreid, img_path is a string, while for vidreid, img_path is a tuple containing\n",
    "               a sequence of strings.\n",
    "    - save_dir: directory to save output images.\n",
    "    - topk: int, denoting top-k images in the rank list to be visualized.\n",
    "    \"\"\"\n",
    "    num_q, num_g = distmat.shape\n",
    "\n",
    "    print(\"Visualizing top-{} ranks in '{}' ...\".format(topk, save_dir))\n",
    "    print(\"# query: {}. # gallery {}\".format(num_q, num_g))\n",
    "    \n",
    "    assert num_q == len(dataset.query)\n",
    "    assert num_g == len(dataset.gallery)\n",
    "    \n",
    "    indices = np.argsort(distmat, axis=1)\n",
    "    mkdir_if_missing(save_dir)\n",
    "\n",
    "    def _cp_img_to(src, dst, rank, prefix):\n",
    "        \"\"\"\n",
    "        - src: image path or tuple (for vidreid)\n",
    "        - dst: target directory\n",
    "        - rank: int, denoting ranked position, starting from 1\n",
    "        - prefix: string\n",
    "        \"\"\"\n",
    "        if isinstance(src, tuple):\n",
    "            dst = osp.join(dst, prefix + '_top' + str(rank).zfill(3))\n",
    "            mkdir_if_missing(dst)\n",
    "            for img_path in src:\n",
    "                shutil.copy(img_path, dst)\n",
    "        else:\n",
    "            dst = osp.join(dst, prefix + '_top' + str(rank).zfill(3) + '_name_' + osp.basename(src))\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "    for q_idx in range(num_q):\n",
    "        qimg_path, qpid, qcamid = dataset.query[q_idx]\n",
    "        qdir = osp.join(save_dir, 'query' + str(q_idx + 1).zfill(5))\n",
    "        mkdir_if_missing(qdir)\n",
    "        _cp_img_to(qimg_path, qdir, rank=0, prefix='query')\n",
    "\n",
    "        rank_idx = 1\n",
    "        for g_idx in indices[q_idx,:]:\n",
    "            gimg_path, gpid, gcamid = dataset.gallery[g_idx]\n",
    "            invalid = (qpid == gpid) & (qcamid == gcamid)\n",
    "            if not invalid:\n",
    "                _cp_img_to(gimg_path, qdir, rank=rank_idx, prefix='gallery')\n",
    "                rank_idx += 1\n",
    "                if rank_idx > topk:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCsG0AxaAeij"
   },
   "source": [
    "# torchtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T15:22:46.797213Z",
     "iopub.status.busy": "2021-08-01T15:22:46.796846Z",
     "iopub.status.idle": "2021-08-01T15:22:46.805476Z",
     "shell.execute_reply": "2021-08-01T15:22:46.804089Z",
     "shell.execute_reply.started": "2021-08-01T15:22:46.797177Z"
    },
    "executionInfo": {
     "elapsed": 2161,
     "status": "ok",
     "timestamp": 1641574560310,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "DVkBwDTaAeik"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, base_lr, epoch, stepsize, gamma=0.1):\n",
    "    # decay learning rate by 'gamma' for every 'stepsize'\n",
    "    lr = base_lr * (gamma ** (epoch // stepsize))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def set_bn_to_eval(m):\n",
    "    # 1. no update for running mean and var\n",
    "    # 2. scale and shift parameters are still trainable\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('BatchNorm') != -1:\n",
    "        m.eval()\n",
    "\n",
    "\n",
    "def count_num_param(model):\n",
    "    num_param = sum(p.numel() for p in model.parameters()) / 1e+06\n",
    "    if hasattr(model, 'classifier') and isinstance(model.classifier, nn.Module):\n",
    "        # we ignore the classifier because it is unused at test time\n",
    "        num_param -= sum(p.numel() for p in model.classifier.parameters()) / 1e+06\n",
    "    return num_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqOE-7EUAeik"
   },
   "source": [
    "# Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T15:22:47.996198Z",
     "iopub.status.busy": "2021-08-01T15:22:47.995782Z",
     "iopub.status.idle": "2021-08-01T15:22:48.019218Z",
     "shell.execute_reply": "2021-08-01T15:22:48.018183Z",
     "shell.execute_reply.started": "2021-08-01T15:22:47.996157Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1641574562025,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "e9_klM4KAeil"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "q_g_dist: query-gallery distance matrix, numpy array, shape [num_query, num_gallery]\n",
    "q_q_dist: query-query distance matrix, numpy array, shape [num_query, num_query]\n",
    "g_g_dist: gallery-gallery distance matrix, numpy array, shape [num_gallery, num_gallery]\n",
    "k1, k2, lambda_value: parameters, the original paper is (k1=20, k2=6, lambda_value=0.3)\n",
    "Returns:\n",
    "  final_dist: re-ranked distance, numpy array, shape [num_query, num_gallery]\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "import numpy as np\n",
    "\n",
    "__all__ = ['re_ranking']\n",
    "\n",
    "\n",
    "def re_ranking(q_g_dist, q_q_dist, g_g_dist, k1=20, k2=6, lambda_value=0.3):\n",
    "\n",
    "    # The following naming, e.g. gallery_num, is different from outer scope.\n",
    "    # Don't care about it.\n",
    "    original_dist = np.concatenate(\n",
    "        [\n",
    "            np.concatenate([q_q_dist, q_g_dist], axis=1),\n",
    "            np.concatenate([q_g_dist.T, g_g_dist], axis=1)\n",
    "        ],\n",
    "        axis=0\n",
    "    )\n",
    "    original_dist = np.power(original_dist, 2).astype(np.float32)\n",
    "    original_dist = np.transpose(\n",
    "        1. * original_dist / np.max(original_dist, axis=0)\n",
    "    )\n",
    "    V = np.zeros_like(original_dist).astype(np.float32)\n",
    "    initial_rank = np.argsort(original_dist).astype(np.int32)\n",
    "\n",
    "    query_num = q_g_dist.shape[0]\n",
    "    gallery_num = q_g_dist.shape[0] + q_g_dist.shape[1]\n",
    "    all_num = gallery_num\n",
    "\n",
    "    for i in range(all_num):\n",
    "        # k-reciprocal neighbors\n",
    "        forward_k_neigh_index = initial_rank[i, :k1 + 1]\n",
    "        backward_k_neigh_index = initial_rank[forward_k_neigh_index, :k1 + 1]\n",
    "        fi = np.where(backward_k_neigh_index == i)[0]\n",
    "        k_reciprocal_index = forward_k_neigh_index[fi]\n",
    "        k_reciprocal_expansion_index = k_reciprocal_index\n",
    "        for j in range(len(k_reciprocal_index)):\n",
    "            candidate = k_reciprocal_index[j]\n",
    "            candidate_forward_k_neigh_index = initial_rank[\n",
    "                candidate, :int(np.around(k1 / 2.)) + 1]\n",
    "            candidate_backward_k_neigh_index = initial_rank[\n",
    "                candidate_forward_k_neigh_index, :int(np.around(k1 / 2.)) + 1]\n",
    "            fi_candidate = np.where(\n",
    "                candidate_backward_k_neigh_index == candidate\n",
    "            )[0]\n",
    "            candidate_k_reciprocal_index = candidate_forward_k_neigh_index[\n",
    "                fi_candidate]\n",
    "            if len(\n",
    "                np.\n",
    "                intersect1d(candidate_k_reciprocal_index, k_reciprocal_index)\n",
    "            ) > 2. / 3 * len(candidate_k_reciprocal_index):\n",
    "                k_reciprocal_expansion_index = np.append(\n",
    "                    k_reciprocal_expansion_index, candidate_k_reciprocal_index\n",
    "                )\n",
    "\n",
    "        k_reciprocal_expansion_index = np.unique(k_reciprocal_expansion_index)\n",
    "        weight = np.exp(-original_dist[i, k_reciprocal_expansion_index])\n",
    "        V[i, k_reciprocal_expansion_index] = 1. * weight / np.sum(weight)\n",
    "    original_dist = original_dist[:query_num, ]\n",
    "    if k2 != 1:\n",
    "        V_qe = np.zeros_like(V, dtype=np.float32)\n",
    "        for i in range(all_num):\n",
    "            V_qe[i, :] = np.mean(V[initial_rank[i, :k2], :], axis=0)\n",
    "        V = V_qe\n",
    "        del V_qe\n",
    "    del initial_rank\n",
    "    invIndex = []\n",
    "    for i in range(gallery_num):\n",
    "        invIndex.append(np.where(V[:, i] != 0)[0])\n",
    "\n",
    "    jaccard_dist = np.zeros_like(original_dist, dtype=np.float32)\n",
    "\n",
    "    for i in range(query_num):\n",
    "        temp_min = np.zeros(shape=[1, gallery_num], dtype=np.float32)\n",
    "        indNonZero = np.where(V[i, :] != 0)[0]\n",
    "        indImages = []\n",
    "        indImages = [invIndex[ind] for ind in indNonZero]\n",
    "        for j in range(len(indNonZero)):\n",
    "            temp_min[0, indImages[j]] = temp_min[0, indImages[j]] + np.minimum(\n",
    "                V[i, indNonZero[j]], V[indImages[j], indNonZero[j]]\n",
    "            )\n",
    "        jaccard_dist[i] = 1 - temp_min / (2.-temp_min)\n",
    "\n",
    "    final_dist = jaccard_dist * (1-lambda_value) + original_dist*lambda_value\n",
    "    del original_dist\n",
    "    del V\n",
    "    del jaccard_dist\n",
    "    final_dist = final_dist[:query_num, query_num:]\n",
    "    \n",
    "    return final_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfe3ouRDAeim"
   },
   "source": [
    "# Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T15:22:49.184485Z",
     "iopub.status.busy": "2021-08-01T15:22:49.184096Z",
     "iopub.status.idle": "2021-08-01T15:22:49.202350Z",
     "shell.execute_reply": "2021-08-01T15:22:49.201315Z",
     "shell.execute_reply.started": "2021-08-01T15:22:49.184448Z"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1641574563865,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "NZCRBctrAeim"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import imdb\n",
    "import io\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def read_image(img_path, mode='RGB'):\n",
    "    \"\"\"Keep reading image until succeed.\n",
    "    This can avoid IOError incurred by heavy IO process.\"\"\"\n",
    "    got_img = False\n",
    "    if not osp.exists(img_path):\n",
    "        raise IOError(\"{} does not exist\".format(img_path))\n",
    "    while not got_img:\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(mode)\n",
    "            got_img = True\n",
    "        except IOError:\n",
    "            print(\"IOError incurred when reading '{}'. Will redo. Don't worry. Just chill.\".format(img_path))\n",
    "            pass\n",
    "    return img\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Image Person ReID Dataset\"\"\"\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, pid, camid = self.dataset[index]\n",
    "        img = read_image(img_path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, pid, camid\n",
    "\n",
    "\n",
    "class ImageDataset_seg(Dataset):\n",
    "    \"\"\"Image Person ReID Dataset\"\"\"\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, pid, camid = self.dataset[index]\n",
    "        img_path, head_path, upper_body_path, lower_body_path, shoes_path, foreground_path = path\n",
    "        \n",
    "        img = read_image(img_path, mode='RGB')\n",
    "        head = read_image(head_path, mode='L')\n",
    "        upper_body = read_image(upper_body_path, mode='L')\n",
    "        lower_body = read_image(lower_body_path, mode='L')\n",
    "        shoes = read_image(shoes_path, mode='L')\n",
    "        foreground = read_image(foreground_path, mode='L')\n",
    "\n",
    "        sequence = [img, head, upper_body, lower_body, shoes, foreground]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            self.transform.randomize_parameters()\n",
    "            sequence = [self.transform(img) for img in sequence]\n",
    "\n",
    "        sequence = torch.cat(sequence, 0) #[3+1+1+1, h, w]\n",
    "        \n",
    "        return sequence, pid, camid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgQ-H9EKAein"
   },
   "source": [
    "# evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-08-01T15:22:49.925737Z",
     "iopub.status.busy": "2021-08-01T15:22:49.925259Z",
     "iopub.status.idle": "2021-08-01T15:22:49.955204Z",
     "shell.execute_reply": "2021-08-01T15:22:49.953960Z",
     "shell.execute_reply.started": "2021-08-01T15:22:49.925695Z"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1641574570927,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "sOEyWINEAein",
    "outputId": "0633c878-424d-4f56-af1d-377df6c13b87"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from eval_lib.cython_eval import eval_market1501_wrap\n",
    "    CYTHON_EVAL_AVAI = True\n",
    "    print(\"Cython evaluation is AVAILABLE\")\n",
    "except ImportError:\n",
    "    CYTHON_EVAL_AVAI = False\n",
    "    print(\"Warning: Cython evaluation is UNAVAILABLE\")\n",
    "\n",
    "\n",
    "def eval_cuhk03(distmat, q_pids, g_pids, q_camids, g_camids, max_rank, N=100):\n",
    "    \"\"\"Evaluation with cuhk03 metric\n",
    "    Key: one image for each gallery identity is randomly sampled for each query identity.\n",
    "    Random sampling is performed N times (default: N=100).\n",
    "    \"\"\"\n",
    "    num_q, num_g = distmat.shape\n",
    "    if num_g < max_rank:\n",
    "        max_rank = num_g\n",
    "        print(\"Note: number of gallery samples is quite small, got {}\".format(num_g))\n",
    "    indices = np.argsort(distmat, axis=1)\n",
    "    matches = (g_pids[indices] == q_pids[:, np.newaxis]).astype(np.int32)\n",
    "\n",
    "    # compute cmc curve for each query\n",
    "    all_cmc = []\n",
    "    all_AP = []\n",
    "    num_valid_q = 0. # number of valid query\n",
    "    for q_idx in range(num_q):\n",
    "        # get query pid and camid\n",
    "        q_pid = q_pids[q_idx]\n",
    "        q_camid = q_camids[q_idx]\n",
    "\n",
    "        # remove gallery samples that have the same pid and camid with query\n",
    "        order = indices[q_idx]\n",
    "        remove = (g_pids[order] == q_pid) & (g_camids[order] == q_camid)\n",
    "        keep = np.invert(remove)\n",
    "\n",
    "        # compute cmc curve\n",
    "        orig_cmc = matches[q_idx][keep] # binary vector, positions with value 1 are correct matches\n",
    "        if not np.any(orig_cmc):\n",
    "            # this condition is true when query identity does not appear in gallery\n",
    "            continue\n",
    "\n",
    "        kept_g_pids = g_pids[order][keep]\n",
    "        g_pids_dict = defaultdict(list)\n",
    "        for idx, pid in enumerate(kept_g_pids):\n",
    "            g_pids_dict[pid].append(idx)\n",
    "\n",
    "        cmc, AP = 0., 0.\n",
    "        for repeat_idx in range(N):\n",
    "            mask = np.zeros(len(orig_cmc), dtype=np.bool)\n",
    "            for _, idxs in g_pids_dict.items():\n",
    "                # randomly sample one image for each gallery person\n",
    "                rnd_idx = np.random.choice(idxs)\n",
    "                mask[rnd_idx] = True\n",
    "            masked_orig_cmc = orig_cmc[mask]\n",
    "            _cmc = masked_orig_cmc.cumsum()\n",
    "            _cmc[_cmc > 1] = 1\n",
    "            cmc += _cmc[:max_rank].astype(np.float32)\n",
    "            # compute AP\n",
    "            num_rel = masked_orig_cmc.sum()\n",
    "            tmp_cmc = masked_orig_cmc.cumsum()\n",
    "            tmp_cmc = [x / (i+1.) for i, x in enumerate(tmp_cmc)]\n",
    "            tmp_cmc = np.asarray(tmp_cmc) * masked_orig_cmc\n",
    "            AP += tmp_cmc.sum() / num_rel\n",
    "        cmc /= N\n",
    "        AP /= N\n",
    "        all_cmc.append(cmc)\n",
    "        all_AP.append(AP)\n",
    "        num_valid_q += 1.\n",
    "\n",
    "    assert num_valid_q > 0, \"Error: all query identities do not appear in gallery\"\n",
    "\n",
    "    all_cmc = np.asarray(all_cmc).astype(np.float32)\n",
    "    all_cmc = all_cmc.sum(0) / num_valid_q\n",
    "    mAP = np.mean(all_AP)\n",
    "\n",
    "    return all_cmc, mAP\n",
    "\n",
    "\n",
    "def eval_market1501(distmat, q_pids, g_pids, q_camids, g_camids, max_rank):\n",
    "    \"\"\"Evaluation with market1501 metric\n",
    "    Key: for each query identity, its gallery images from the same camera view are discarded.\n",
    "    \"\"\"\n",
    "    num_q, num_g = distmat.shape\n",
    "    if num_g < max_rank:\n",
    "        max_rank = num_g\n",
    "        print(\"Note: number of gallery samples is quite small, got {}\".format(num_g))\n",
    "    indices = np.argsort(distmat, axis=1)\n",
    "    matches = (g_pids[indices] == q_pids[:, np.newaxis]).astype(np.int32)\n",
    "\n",
    "    # compute cmc curve for each query\n",
    "    all_cmc = []\n",
    "    all_AP = []\n",
    "    num_valid_q = 0. # number of valid query\n",
    "    for q_idx in range(num_q):\n",
    "        # get query pid and camid\n",
    "        q_pid = q_pids[q_idx]\n",
    "        q_camid = q_camids[q_idx]\n",
    "\n",
    "        # remove gallery samples that have the same pid and camid with query\n",
    "        order = indices[q_idx]\n",
    "        remove = (g_pids[order] == q_pid) & (g_camids[order] == q_camid)\n",
    "        keep = np.invert(remove)\n",
    "\n",
    "        # compute cmc curve\n",
    "        orig_cmc = matches[q_idx][keep] # binary vector, positions with value 1 are correct matches\n",
    "        if not np.any(orig_cmc):\n",
    "            # this condition is true when query identity does not appear in gallery\n",
    "            continue\n",
    "\n",
    "        cmc = orig_cmc.cumsum()\n",
    "        cmc[cmc > 1] = 1\n",
    "\n",
    "        all_cmc.append(cmc[:max_rank])\n",
    "        num_valid_q += 1.\n",
    "\n",
    "        # compute average precision\n",
    "        # reference: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision\n",
    "        num_rel = orig_cmc.sum()\n",
    "        tmp_cmc = orig_cmc.cumsum()\n",
    "        tmp_cmc = [x / (i+1.) for i, x in enumerate(tmp_cmc)]\n",
    "        tmp_cmc = np.asarray(tmp_cmc) * orig_cmc\n",
    "        AP = tmp_cmc.sum() / num_rel\n",
    "        all_AP.append(AP)\n",
    "\n",
    "    assert num_valid_q > 0, \"Error: all query identities do not appear in gallery\"\n",
    "\n",
    "    all_cmc = np.asarray(all_cmc).astype(np.float32)\n",
    "    all_cmc = all_cmc.sum(0) / num_valid_q\n",
    "    mAP = np.mean(all_AP)\n",
    "\n",
    "    return all_cmc, mAP\n",
    "\n",
    "\n",
    "def evaluate_model(distmat, q_pids, g_pids, q_camids, g_camids, max_rank=50, use_metrics_cuhk03=False, use_cython=True):\n",
    "    if use_metrics_cuhk03:\n",
    "        return eval_cuhk03(distmat, q_pids, g_pids, q_camids, g_camids, max_rank)\n",
    "    else:\n",
    "        if use_cython and CYTHON_EVAL_AVAI:\n",
    "            return eval_market1501_wrap(distmat, q_pids, g_pids, q_camids, g_camids, max_rank)\n",
    "        else:\n",
    "            return eval_market1501(distmat, q_pids, g_pids, q_camids, g_camids, max_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38xGnbbVAeip"
   },
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T15:22:51.084383Z",
     "iopub.status.busy": "2021-08-01T15:22:51.083611Z",
     "iopub.status.idle": "2021-08-01T15:22:51.120776Z",
     "shell.execute_reply": "2021-08-01T15:22:51.119668Z",
     "shell.execute_reply.started": "2021-08-01T15:22:51.084337Z"
    },
    "executionInfo": {
     "elapsed": 626,
     "status": "ok",
     "timestamp": 1641574571550,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "jJjtWP5cAeiq"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def DeepSupervision(criterion, xs, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    - criterion: loss function\n",
    "    - xs: tuple of inputs\n",
    "    - y: ground truth\n",
    "    \"\"\"\n",
    "    loss = 0.\n",
    "    for x in xs:\n",
    "        loss += criterion(x, y)\n",
    "    loss /= len(xs)\n",
    "    return loss\n",
    "\n",
    "\n",
    "class MaskLoss(nn.Module):\n",
    "    \"\"\"L2 or L1 loss or cross entropy loss with average with all elements.\n",
    "    \"\"\"\n",
    "    def __init__(self, mode='l2'):\n",
    "        super(MaskLoss, self).__init__()\n",
    "        if mode == 'l2':\n",
    "            self.loss = nn.MSELoss()\n",
    "        elif mode == 'l1':\n",
    "            self.loss = nn.L1Loss()\n",
    "        elif mode == 'ce':\n",
    "            self.loss = nn.BCELoss()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - inputs: prediction spatial map with shape (batch_size, 1, h, w)\n",
    "        - targets: ground truth labels with shape (batch_size, 1, h1, w1)\n",
    "        \"\"\"\n",
    "        b, c, h, w = inputs.size()\n",
    "        targets = F.interpolate(targets, (h, w), mode='bilinear', align_corners=True)\n",
    "        inputs = inputs.view(b, -1)\n",
    "        targets = targets.view(b, -1)\n",
    "        return self.loss(inputs, targets)\n",
    "\n",
    "\n",
    "class CrossEntropyLabelSmooth(nn.Module):\n",
    "    \"\"\"Cross entropy loss with label smoothing regularizer.\n",
    "\n",
    "    Reference:\n",
    "    Szegedy et al. Rethinking the Inception Architecture for Computer Vision. CVPR 2016.\n",
    "    Equation: y = (1 - epsilon) * y + epsilon / K.\n",
    "\n",
    "    Args:\n",
    "    - num_classes (int): number of classes.\n",
    "    - epsilon (float): weight.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, epsilon=0.1, use_gpu=True):\n",
    "        super(CrossEntropyLabelSmooth, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.epsilon = epsilon\n",
    "        self.use_gpu = use_gpu\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - inputs: prediction matrix (before softmax) with shape (batch_size, num_classes)\n",
    "        - targets: ground truth labels with shape (num_classes)\n",
    "        \"\"\"\n",
    "        log_probs = self.logsoftmax(inputs)\n",
    "        targets = torch.zeros(log_probs.size()).scatter_(1, targets.unsqueeze(1).data.cpu(), 1)\n",
    "        if self.use_gpu: targets = targets.cuda()\n",
    "        targets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes\n",
    "        loss = (- targets * log_probs).mean(0).sum()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"Triplet loss with hard positive/negative mining.\n",
    "\n",
    "    Reference:\n",
    "    Hermans et al. In Defense of the Triplet Loss for Person Re-Identification. arXiv:1703.07737.\n",
    "\n",
    "    Code imported from https://github.com/Cysu/open-reid/blob/master/reid/loss/triplet.py.\n",
    "\n",
    "    Args:\n",
    "    - margin (float): margin for triplet.\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=0.3, distance='euclidean', use_gpu=True):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        if distance not in ['euclidean', 'cosine']:\n",
    "            raise KeyError('Unsupported distance: {}'.format(distance))\n",
    "        self.distance = distance\n",
    "        self.margin = margin\n",
    "        self.use_gpu = use_gpu\n",
    "        self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - inputs: feature matrix with shape (batch_size, feat_dim)\n",
    "        - targets: ground truth labels with shape (num_classes)\n",
    "        \"\"\"\n",
    "        n = inputs.size(0)\n",
    "        \n",
    "        # Compute pairwise distance, replace by the official when merged\n",
    "        if self.distance == 'euclidean':\n",
    "            dist = torch.pow(inputs, 2).sum(dim=1, keepdim=True).expand(n, n)\n",
    "            dist = dist + dist.t()\n",
    "            dist.addmm_(1, -2, inputs, inputs.t())\n",
    "            dist = dist.clamp(min=1e-12).sqrt()  # for numerical stability\n",
    "        elif self.distance == 'cosine':\n",
    "            fnorm = torch.norm(inputs, p=2, dim=1, keepdim=True)\n",
    "            l2norm = inputs.div(fnorm.expand_as(inputs))\n",
    "            dist = - torch.mm(l2norm, l2norm.t())\n",
    "\n",
    "        if self.use_gpu: targets = targets.cuda()\n",
    "        \n",
    "        # For each anchor, find the hardest positive and negative\n",
    "        mask = targets.expand(n, n).eq(targets.expand(n, n).t())\n",
    "        dist_ap, dist_an = [], []\n",
    "        for i in range(n):\n",
    "            dist_ap.append(dist[i][mask[i]].max().unsqueeze(0))\n",
    "            dist_an.append(dist[i][mask[i] == 0].min().unsqueeze(0))\n",
    "        dist_ap = torch.cat(dist_ap)\n",
    "        dist_an = torch.cat(dist_an)\n",
    "        \n",
    "        # Compute ranking hinge loss\n",
    "        y = torch.ones_like(dist_an)\n",
    "        loss = self.ranking_loss(dist_an, dist_ap, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtnVPVOFAeit"
   },
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T15:22:51.841870Z",
     "iopub.status.busy": "2021-08-01T15:22:51.841536Z",
     "iopub.status.idle": "2021-08-01T15:22:51.849562Z",
     "shell.execute_reply": "2021-08-01T15:22:51.848586Z",
     "shell.execute_reply.started": "2021-08-01T15:22:51.841840Z"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1641574571552,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "-_EhtIJvAeit"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def init_optim(optim, params, lr, weight_decay):\n",
    "    if optim == 'adam':\n",
    "        return torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
    "    elif optim == 'amsgrad':\n",
    "        return torch.optim.Adam(params, lr=lr, weight_decay=weight_decay, amsgrad=True)\n",
    "    elif optim == 'sgd':\n",
    "        return torch.optim.SGD(params, lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    elif optim == 'rmsprop':\n",
    "        return torch.optim.RMSprop(params, lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise KeyError(\"Unsupported optimizer: {}\".format(optim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFpku83UAeit"
   },
   "source": [
    "# samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T15:22:52.645799Z",
     "iopub.status.busy": "2021-08-01T15:22:52.645459Z",
     "iopub.status.idle": "2021-08-01T15:22:52.656388Z",
     "shell.execute_reply": "2021-08-01T15:22:52.655544Z",
     "shell.execute_reply.started": "2021-08-01T15:22:52.645771Z"
    },
    "executionInfo": {
     "elapsed": 1884,
     "status": "ok",
     "timestamp": 1641574577375,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "fnPSV18wAeiu"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "\n",
    "class RandomIdentitySampler(Sampler):\n",
    "    \"\"\"\n",
    "    Randomly sample N identities, then for each identity,\n",
    "    randomly sample K instances, therefore batch size is N*K.\n",
    "\n",
    "    Args:\n",
    "    - data_source (Dataset): dataset to sample from.\n",
    "    - num_instances (int): number of instances per identity.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_source, num_instances=4):\n",
    "        self.data_source = data_source\n",
    "        self.num_instances = num_instances\n",
    "        self.index_dic = defaultdict(list)\n",
    "        for index, (_, pid, _) in enumerate(data_source):\n",
    "            self.index_dic[pid].append(index)\n",
    "        self.pids = list(self.index_dic.keys())\n",
    "        self.num_identities = len(self.pids)\n",
    "\n",
    "        # compute number of examples in an epoch\n",
    "        self.length = 0\n",
    "        for pid in self.pids:\n",
    "            idxs = self.index_dic[pid]\n",
    "            num = len(idxs)\n",
    "            if num < self.num_instances:\n",
    "                num = self.num_instances\n",
    "            self.length += num - num % self.num_instances\n",
    "\n",
    "    def __iter__(self):\n",
    "        list_container = []\n",
    "\n",
    "        for pid in self.pids:\n",
    "            idxs = copy.deepcopy(self.index_dic[pid])\n",
    "            if len(idxs) < self.num_instances:\n",
    "                idxs = np.random.choice(idxs, size=self.num_instances, replace=True)\n",
    "            random.shuffle(idxs)\n",
    "            batch_idxs = []\n",
    "            for idx in idxs:\n",
    "                batch_idxs.append(idx)\n",
    "                if len(batch_idxs) == self.num_instances:\n",
    "                    list_container.append(batch_idxs)\n",
    "                    batch_idxs = []\n",
    "\n",
    "        random.shuffle(list_container)\n",
    "\n",
    "        ret = []\n",
    "        for batch_idxs in list_container:\n",
    "            ret.extend(batch_idxs)\n",
    "\n",
    "        return iter(ret)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZUC4lkqAeiu"
   },
   "source": [
    "# Spatial transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T15:22:53.542968Z",
     "iopub.status.busy": "2021-08-01T15:22:53.542634Z",
     "iopub.status.idle": "2021-08-01T15:22:53.610643Z",
     "shell.execute_reply": "2021-08-01T15:22:53.609780Z",
     "shell.execute_reply.started": "2021-08-01T15:22:53.542936Z"
    },
    "executionInfo": {
     "elapsed": 442,
     "status": "ok",
     "timestamp": 1641574586473,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "vTLVq5_WAeiu"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import random\n",
    "import math\n",
    "import numbers\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageOps\n",
    "try:\n",
    "    import accimage\n",
    "except ImportError:\n",
    "    accimage = None\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    \"\"\"Composes several transforms together.\n",
    "    Args:\n",
    "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>> ])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms:\n",
    "            img = t(img)\n",
    "        return img\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        for t in self.transforms:\n",
    "            t.randomize_parameters()\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert a ``PIL.Image`` or ``numpy.ndarray`` to tensor.\n",
    "    Converts a PIL.Image or numpy.ndarray (H x W x C) in the range\n",
    "    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, norm_value=255):\n",
    "        self.norm_value = norm_value\n",
    "\n",
    "    def __call__(self, pic):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pic (PIL.Image or numpy.ndarray): Image to be converted to tensor.\n",
    "        Returns:\n",
    "            Tensor: Converted image.\n",
    "        \"\"\"\n",
    "        if isinstance(pic, np.ndarray):\n",
    "            # handle numpy array\n",
    "            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n",
    "            # backward compatibility\n",
    "            return img.float().div(self.norm_value)\n",
    "\n",
    "        if accimage is not None and isinstance(pic, accimage.Image):\n",
    "            nppic = np.zeros(\n",
    "                [pic.channels, pic.height, pic.width], dtype=np.float32)\n",
    "            pic.copyto(nppic)\n",
    "            return torch.from_numpy(nppic)\n",
    "\n",
    "        # handle PIL Image\n",
    "        if pic.mode == 'I':\n",
    "            img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
    "        elif pic.mode == 'I;16':\n",
    "            img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n",
    "        else:\n",
    "            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
    "        # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n",
    "        if pic.mode == 'YCbCr':\n",
    "            nchannel = 3\n",
    "        elif pic.mode == 'I;16':\n",
    "            nchannel = 1\n",
    "        else:\n",
    "            nchannel = len(pic.mode)\n",
    "        img = img.view(pic.size[1], pic.size[0], nchannel)\n",
    "        # put it from HWC to CHW format\n",
    "        # yikes, this transpose takes 80% of the loading time/CPU\n",
    "        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
    "        if isinstance(img, torch.ByteTensor):\n",
    "            return img.float().div(self.norm_value)\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    \"\"\"Normalize an tensor image with mean and standard deviation.\n",
    "    Given mean: (R, G, B) and std: (R, G, B),\n",
    "    will normalize each channel of the torch.*Tensor, i.e.\n",
    "    channel = (channel - mean) / std\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n",
    "        std (sequence): Sequence of standard deviations for R, G, B channels\n",
    "            respecitvely.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        # TODO: make efficient\n",
    "        if tensor.size(0) == 1:\n",
    "            return tensor\n",
    "\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class NormalizeSub(object):\n",
    "    \"\"\"Normalize an tensor image with mean and standard deviation.\n",
    "    Given mean: (R, G, B) and std: (R, G, B),\n",
    "    will normalize each channel of the torch.*Tensor, i.e.\n",
    "    channel = (channel - mean) / std\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n",
    "        std (sequence): Sequence of standard deviations for R, G, B channels\n",
    "            respecitvely.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        # TODO: make efficient\n",
    "        if tensor.size(0) == 1:\n",
    "            mean = [0.5]\n",
    "            std = [0.25]\n",
    "            for t, m, s in zip(tensor, mean, std):\n",
    "                t.sub_(m).div_(s)\n",
    "            return tensor\n",
    "\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Scale(object):\n",
    "    \"\"\"Rescale the input PIL.Image to the given size.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size. If size is a sequence like\n",
    "            (w, h), output size will be matched to this. If size is an int,\n",
    "            smaller edge of the image will be matched to this number.\n",
    "            i.e, if height > width, then image will be rescaled to\n",
    "            (size * height / width, size)\n",
    "        interpolation (int, optional): Desired interpolation. Default is\n",
    "            ``PIL.Image.BILINEAR``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        assert isinstance(size,\n",
    "                          int) or (isinstance(size, collections.Iterable) and\n",
    "                                   len(size) == 2)\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be scaled.\n",
    "        Returns:\n",
    "            PIL.Image: Rescaled image.\n",
    "        \"\"\"\n",
    "        if isinstance(self.size, int):\n",
    "            w, h = img.size\n",
    "            if (w <= h and w == self.size) or (h <= w and h == self.size):\n",
    "                return img\n",
    "            if w < h:\n",
    "                ow = self.size\n",
    "                oh = int(self.size * h / w)\n",
    "                return img.resize((ow, oh), self.interpolation)\n",
    "            else:\n",
    "                oh = self.size\n",
    "                ow = int(self.size * w / h)\n",
    "                return img.resize((ow, oh), self.interpolation)\n",
    "        else:\n",
    "            return img.resize(self.size[::-1], self.interpolation)\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crops the given PIL.Image at a random location.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an\n",
    "            int instead of sequence like (h, w), a square crop (size, size) is\n",
    "            made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be cropped.\n",
    "        Returns:\n",
    "            PIL.Image: Cropped image.\n",
    "        \"\"\"\n",
    "        w, h = img.size\n",
    "        th, tw = self.size\n",
    "\n",
    "        x1 = int(round(self.tl_x * (w - tw)))\n",
    "        y1 = int(round(self.tl_y * (h - th)))\n",
    "        return img.crop((x1, y1, x1 + tw, y1 + th))\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        self.tl_x = random.random()\n",
    "        self.tl_y = random.random()\n",
    "\n",
    "        \n",
    "class CenterCrop(object):\n",
    "    \"\"\"Crops the given PIL.Image at the center.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an\n",
    "            int instead of sequence like (h, w), a square crop (size, size) is\n",
    "            made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be cropped.\n",
    "        Returns:\n",
    "            PIL.Image: Cropped image.\n",
    "        \"\"\"\n",
    "        w, h = img.size\n",
    "        th, tw = self.size\n",
    "        x1 = int(round((w - tw) / 2.))\n",
    "        y1 = int(round((h - th) / 2.))\n",
    "        return img.crop((x1, y1, x1 + tw, y1 + th))\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class CornerCrop(object):\n",
    "\n",
    "    def __init__(self, size, crop_position=None):\n",
    "        self.size = size\n",
    "        if crop_position is None:\n",
    "            self.randomize = True\n",
    "        else:\n",
    "            self.randomize = False\n",
    "        self.crop_position = crop_position\n",
    "        self.crop_positions = ['c', 'tl', 'tr', 'bl', 'br']\n",
    "\n",
    "    def __call__(self, img):\n",
    "        image_width = img.size[0]\n",
    "        image_height = img.size[1]\n",
    "\n",
    "        if self.crop_position == 'c':\n",
    "            th, tw = (self.size, self.size)\n",
    "            x1 = int(round((image_width - tw) / 2.))\n",
    "            y1 = int(round((image_height - th) / 2.))\n",
    "            x2 = x1 + tw\n",
    "            y2 = y1 + th\n",
    "        elif self.crop_position == 'tl':\n",
    "            x1 = 0\n",
    "            y1 = 0\n",
    "            x2 = self.size\n",
    "            y2 = self.size\n",
    "        elif self.crop_position == 'tr':\n",
    "            x1 = image_width - self.size\n",
    "            y1 = 0\n",
    "            x2 = image_width\n",
    "            y2 = self.size\n",
    "        elif self.crop_position == 'bl':\n",
    "            x1 = 0\n",
    "            y1 = image_height - self.size\n",
    "            x2 = self.size\n",
    "            y2 = image_height\n",
    "        elif self.crop_position == 'br':\n",
    "            x1 = image_width - self.size\n",
    "            y1 = image_height - self.size\n",
    "            x2 = image_width\n",
    "            y2 = image_height\n",
    "\n",
    "        img = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "        return img\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        if self.randomize:\n",
    "            self.crop_position = self.crop_positions[random.randint(\n",
    "                0,\n",
    "                len(self.crop_positions) - 1)]\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    \"\"\"Horizontally flip the given PIL.Image randomly with a probability of 0.5.\"\"\"\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be flipped.\n",
    "        Returns:\n",
    "            PIL.Image: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        if self.p < 0.5:\n",
    "            return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        return img\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        self.p = random.random()\n",
    "\n",
    "\n",
    "class MultiScaleCornerCrop(object):\n",
    "    \"\"\"Crop the given PIL.Image to randomly selected size.\n",
    "    A crop of size is selected from scales of the original size.\n",
    "    A position of cropping is randomly selected from 4 corners and 1 center.\n",
    "    This crop is finally resized to given size.\n",
    "    Args:\n",
    "        scales: cropping scales of the original size\n",
    "        size: size of the smaller edge\n",
    "        interpolation: Default: PIL.Image.BILINEAR\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 scales,\n",
    "                 size,\n",
    "                 interpolation=Image.BILINEAR,\n",
    "                 crop_positions=['c', 'tl', 'tr', 'bl', 'br']):\n",
    "        self.scales = scales\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "        self.crop_positions = crop_positions\n",
    "\n",
    "    def __call__(self, img):\n",
    "        min_length = min(img.size[0], img.size[1])\n",
    "        crop_size = int(min_length * self.scale)\n",
    "\n",
    "        image_width = img.size[0]\n",
    "        image_height = img.size[1]\n",
    "\n",
    "        if self.crop_position == 'c':\n",
    "            center_x = image_width // 2\n",
    "            center_y = image_height // 2\n",
    "            box_half = crop_size // 2\n",
    "            x1 = center_x - box_half\n",
    "            y1 = center_y - box_half\n",
    "            x2 = center_x + box_half\n",
    "            y2 = center_y + box_half\n",
    "        elif self.crop_position == 'tl':\n",
    "            x1 = 0\n",
    "            y1 = 0\n",
    "            x2 = crop_size\n",
    "            y2 = crop_size\n",
    "        elif self.crop_position == 'tr':\n",
    "            x1 = image_width - crop_size\n",
    "            y1 = 0\n",
    "            x2 = image_width\n",
    "            y2 = crop_size\n",
    "        elif self.crop_position == 'bl':\n",
    "            x1 = 0\n",
    "            y1 = image_height - crop_size\n",
    "            x2 = crop_size\n",
    "            y2 = image_height\n",
    "        elif self.crop_position == 'br':\n",
    "            x1 = image_width - crop_size\n",
    "            y1 = image_height - crop_size\n",
    "            x2 = image_width\n",
    "            y2 = image_height\n",
    "\n",
    "        img = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "        return img.resize((self.size, self.size), self.interpolation)\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        self.scale = self.scales[random.randint(0, len(self.scales) - 1)]\n",
    "        self.crop_position = self.crop_positions[random.randint(\n",
    "            0,\n",
    "            len(self.scales) - 1)]\n",
    "\n",
    "\n",
    "class MultiScaleRandomCrop(object):\n",
    "\n",
    "    def __init__(self, scales, size, interpolation=Image.BILINEAR):\n",
    "        self.scales = scales\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img):\n",
    "        min_length = min(img.size[0], img.size[1])\n",
    "        crop_size = int(min_length * self.scale)\n",
    "\n",
    "        image_width = img.size[0]\n",
    "        image_height = img.size[1]\n",
    "\n",
    "        x1 = self.tl_x * (image_width - crop_size)\n",
    "        y1 = self.tl_y * (image_height - crop_size)\n",
    "        x2 = x1 + crop_size\n",
    "        y2 = y1 + crop_size\n",
    "\n",
    "        img = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "        return img.resize((self.size, self.size), self.interpolation)\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        self.scale = self.scales[random.randint(0, len(self.scales) - 1)]\n",
    "        self.tl_x = random.random()\n",
    "        self.tl_y = random.random()\n",
    "\n",
    "\n",
    "class Random2DTranslation(object):\n",
    "    \"\"\"\n",
    "    With a probability, first increase image size to (1 + 1/8), and then perform random crop.\n",
    "\n",
    "    Args:\n",
    "        height (int): target height.\n",
    "        width (int): target width.\n",
    "        p (float): probability of performing this transformation. Default: 0.5.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, p=0.5, interpolation=Image.BILINEAR):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "        self.height, self.width = self.size\n",
    "        self.p = p\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be cropped.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image: Cropped image.\n",
    "        \"\"\"\n",
    "        if not self.cropping:\n",
    "            return img.resize((self.width, self.height), self.interpolation)\n",
    "        \n",
    "        new_width, new_height = int(round(self.width * 1.125)), int(round(self.height * 1.125))\n",
    "        resized_img = img.resize((new_width, new_height), self.interpolation)\n",
    "        x_maxrange = new_width - self.width\n",
    "        y_maxrange = new_height - self.height\n",
    "        x1 = int(round(self.tl_x * x_maxrange))\n",
    "        y1 = int(round(self.tl_y * y_maxrange))\n",
    "        return resized_img.crop((x1, y1, x1 + self.width, y1 + self.height))\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        self.cropping = random.uniform(0, 1) < self.p\n",
    "        self.tl_x = random.random()\n",
    "        self.tl_y = random.random()\n",
    "\n",
    "\n",
    "class RandomErasing(object):\n",
    "    \"\"\" Randomly selects a rectangle region in an image and erases its pixels.\n",
    "        'Random Erasing Data Augmentation' by Zhong et al.\n",
    "        See https://arxiv.org/pdf/1708.04896.pdf\n",
    "    Args:\n",
    "         probability: The probability that the Random Erasing operation will be performed.\n",
    "         sl: Minimum proportion of erased area against input image.\n",
    "         sh: Maximum proportion of erased area against input image.\n",
    "         r1: Minimum aspect ratio of erased area.\n",
    "         mean: Erasing value. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, probability = 0.5, sl = 0.02, sh = 0.4, r1 = 0.3, mean=[0.4914, 0.4822, 0.4465]):\n",
    "        self.probability = probability\n",
    "        self.mean = mean\n",
    "        self.sl = sl\n",
    "        self.sh = sh\n",
    "        self.r1 = r1\n",
    "       \n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        img: [C, H, W]\n",
    "        \"\"\"\n",
    "\n",
    "        if img.size(0) == 1:\n",
    "            return img\n",
    "\n",
    "        if random.uniform(0, 1) > self.probability:\n",
    "            return img\n",
    "\n",
    "        for attempt in range(100):\n",
    "            area = img.size()[1] * img.size()[2]\n",
    "       \n",
    "            target_area = random.uniform(self.sl, self.sh) * area\n",
    "            aspect_ratio = random.uniform(self.r1, 1/self.r1)\n",
    "\n",
    "            h = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "            w = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "\n",
    "            if w < img.size()[2] and h < img.size()[1]:\n",
    "                x1 = random.randint(0, img.size()[1] - h)\n",
    "                y1 = random.randint(0, img.size()[2] - w)\n",
    "                if img.size()[0] == 3:\n",
    "                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n",
    "                    img[1, x1:x1+h, y1:y1+w] = self.mean[1]\n",
    "                    img[2, x1:x1+h, y1:y1+w] = self.mean[2]\n",
    "                else:\n",
    "                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n",
    "                return img\n",
    "\n",
    "        return img\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VbNduYaWp0d"
   },
   "source": [
    "# Train for MSMT17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "073d84ef4b0b4eadbea0a6eff29b0b7e",
      "1706b10464434f5c9ec257832302ce53",
      "740e0a748f2d49b99b1277f185a31286",
      "695993973c6a49febcc90eb8d3884d96",
      "3a4db6771dc3471bafdcc495158cccf7",
      "092c3a665fd545c08ed769cdb43eee50",
      "bdf683f21f25460aadf588bac21c56de",
      "fe99daac1bf4459580abbe4c3b1a36cf"
     ]
    },
    "id": "GKlJF9xgI6xn",
    "outputId": "d8a2d79f-70d9-45f4-87d1-4919c184b2a5"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.transforms as T\n",
    "\n",
    "parser = \"Train image model with cross entropy loss and hard triplet loss\"\n",
    "# Datasets\n",
    "dataset = 'MSMT17_seg' # default = 'market1501_seg' 'MSMT17_seg', 'DukeMTMC_seg'\n",
    "workers = 4 # default = 4 type = int help = number of data loading workers (default: 4)\"\n",
    "height = 256 # default = 256, \n",
    "width = 128 # default = 128\n",
    "split_id = 0 # default = 0, help = split index \n",
    "\n",
    "    \n",
    "# CUHK03-specific setting\n",
    "cuhk03_labeled = False \n",
    "cuhk03_classic_split = False\n",
    "use_metric_cuhk03 = False\n",
    "\n",
    "# Optimization options\n",
    "optim = 'adam' # default = 'adam'\n",
    "max_epoch = 60 # default = 60    \n",
    "start_epoch = 0 # default = 0 , help = manual epoch number (usefult for restart)   \n",
    "train_batch = 64 # default = 64\n",
    "test_batch = 32 # default = 32    \n",
    "lr = 0.00035 # learning rate, default = 0.00035, type = float\n",
    "stepsize = [20, 40] # stepsize to decay rate    \n",
    "gamma = 0.1 # learning rate decay, default = 0.1    \n",
    "weight_decay = 5e-04  #default = 5e-04\n",
    "margin = 0.3 # margin for triplet loss    \n",
    "num_instances = 4\n",
    "\n",
    "# Architecture\n",
    "arch = 'STCANet'\n",
    "save_dir = './result/market/STCANet' # path to save model checkpoints\n",
    "\n",
    "# spatial attention loss\n",
    "mode = 'ce' #  help='mask loss mode, in {l1, l2, ce}\n",
    "alpha = 0.5 # the weight for the spatial attention loss\n",
    "\n",
    "# data process\n",
    "flip_cnt = 1\n",
    "\n",
    "# Miscs\n",
    "distance = 'cosine'\n",
    "seed = 1\n",
    "resume = ''\n",
    "evaluate = True\n",
    "rerank = False #default = False\n",
    "eval_step = 10    \n",
    "start_eval = 0\n",
    "gpu_devices = '2' # GPU device default = 2 or 3\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = gpu_devices\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "\n",
    "    if not evaluate:\n",
    "        sys.stdout = Logger(osp.join(save_dir, 'log_train.txt'))\n",
    "    else:\n",
    "        sys.stdout = Logger(osp.join(save_dir, 'log_test.txt'), mode='a')\n",
    "    #print(\"==========\\nArgs:{}\\n==========\".format(args))\n",
    "\n",
    "    if use_gpu:\n",
    "        print(\"Currently using GPU {}\".format(gpu_devices))\n",
    "        cudnn.benchmark = True\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    else:\n",
    "        print(\"Currently using CPU (GPU is highly recommended)\")\n",
    "\n",
    "    #print(\"Initializing dataset {}\".format(dataset))\n",
    "    datasets = init_imgreid_dataset(\n",
    "        name= dataset, split_id=split_id,\n",
    "        cuhk03_labeled=cuhk03_labeled, cuhk03_classic_split=cuhk03_classic_split,\n",
    "    )\n",
    "\n",
    "    transform_train = Compose([\n",
    "        Scale((height, width), interpolation=3),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        RandomErasing(0.5),\n",
    "    ])\n",
    "\n",
    "    transform_test = T.Compose([\n",
    "        T.Resize((height, width), interpolation=3),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    pin_memory = True if use_gpu else False\n",
    "    #ImageDataset_segs = ImageDataset_seg()\n",
    "    trainloader = DataLoader(\n",
    "        ImageDataset_seg(datasets.train, transform=transform_train),\n",
    "        sampler=RandomIdentitySampler(datasets.train, num_instances=num_instances),\n",
    "        batch_size=train_batch, num_workers=workers,\n",
    "        pin_memory=pin_memory, drop_last=True,\n",
    "    )\n",
    "    #ImageDatasets = ImageDataset()\n",
    "    queryloader = DataLoader(\n",
    "        ImageDataset(datasets.query, transform=transform_test),\n",
    "        batch_size=test_batch, shuffle=False, num_workers=workers,\n",
    "        pin_memory=pin_memory, drop_last=False,\n",
    "    )\n",
    "\n",
    "    galleryloader = DataLoader(\n",
    "        ImageDataset(datasets.gallery, transform=transform_test),\n",
    "        batch_size=test_batch, shuffle=False, num_workers=workers,\n",
    "        pin_memory=pin_memory, drop_last=False,\n",
    "    )\n",
    "\n",
    "    print(\"Initializing model: {}\".format(arch))\n",
    "    model = init_model(name=arch,  num_classes=datasets.num_train_pids) \n",
    "    \n",
    "    print(model)\n",
    "    print(\"Model size: {:.3f} M\".format(count_num_param(model)))\n",
    "    \n",
    "    criterion_xent = CrossEntropyLabelSmooth(num_classes=datasets.num_train_pids, use_gpu=use_gpu)\n",
    "    criterion_htri = TripletLoss(margin=margin, distance=distance)\n",
    "    criterion_mask = MaskLoss(mode=mode) \n",
    "    \n",
    "    optimizer = init_optim(optim, model.parameters(), lr, weight_decay)\n",
    "    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=stepsize, gamma=gamma)\n",
    "\n",
    "    if resume:\n",
    "        if check_isfile(resume):\n",
    "            checkpoint = torch.load(resume)\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            start_epochs = checkpoint['epoch']\n",
    "            rank1 = checkpoint['rank1']\n",
    "            print(\"Loaded checkpoint from '{}'\".format(resume))\n",
    "            print(\"- start_epoch: {}\\n- rank1: {}\".format(start_epochs, rank1))\n",
    "\n",
    "    if use_gpu:\n",
    "        model = nn.DataParallel(model).cuda()\n",
    "\n",
    "    if evaluate:\n",
    "        print(\"Evaluate only\")\n",
    "        test(model, queryloader, galleryloader, use_gpu = use_gpu)\n",
    "        return\n",
    "    #if cross_validate:\n",
    "     #   print(\"Cross Dataset Validation\")\n",
    "      #  test(m, queryloader, galleryloader, use_gpu = use_gpu)\n",
    "      #  return\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_time = 0\n",
    "    best_rank1 = -np.inf\n",
    "    best_epoch = 0\n",
    "    print(\"==> Start training\")\n",
    "\n",
    "    for epoch in range(start_epoch, max_epoch):\n",
    "        scheduler.step()\n",
    "\n",
    "        start_train_time = time.time()\n",
    "        train(epoch, model, criterion_xent, criterion_htri, criterion_mask, optimizer, trainloader)\n",
    "        train_time += round(time.time() - start_train_time)\n",
    "        \n",
    "        if (epoch + 1) % eval_step == 0 or epoch == 0:\n",
    "            print(\"==> Test\")\n",
    "            use_gpu = True\n",
    "            rank1 = test(model, queryloader, galleryloader, use_gpu = use_gpu)\n",
    "            is_best = rank1 > best_rank1\n",
    "            \n",
    "            if is_best:\n",
    "                best_rank1 = rank1\n",
    "                best_epoch = epoch + 1\n",
    "\n",
    "            if use_gpu:\n",
    "                state_dict = model.module.state_dict()\n",
    "            else:\n",
    "                state_dict = model.state_dict()\n",
    "            \n",
    "            save_checkpoint({\n",
    "                'state_dict': state_dict,\n",
    "                'rank1': rank1,\n",
    "                'epoch': epoch,\n",
    "            }, is_best, osp.join(save_dir, 'checkpoint_ep' + str(epoch + 1) + '.pth.tar'))\n",
    "\n",
    "    print(\"==> Best Rank-1 {:.1%}, achieved at epoch {}\".format(best_rank1, best_epoch))\n",
    "\n",
    "    elapsed = round(time.time() - start_time)\n",
    "    elapsed = str(datetime.timedelta(seconds=elapsed))\n",
    "    train_time = str(datetime.timedelta(seconds=train_time))\n",
    "    print(\"Finished. Total elapsed time (h:m:s): {}. Training time (h:m:s): {}.\".format(elapsed, train_time))\n",
    "    #print(\"==========\\nArgs:{}\\n==========\".format(args))\n",
    "\n",
    "\n",
    "def train(epoch, model, criterion_xent, criterion_htri, criterion_mask, optimizer, trainloader, use_gpu=True):\n",
    "    xent_losses = AverageMeter()\n",
    "    htri_losses = AverageMeter()\n",
    "    mask_losses = AverageMeter()\n",
    "    accs = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for batch_idx, (sequences, pids, _) in enumerate(trainloader):\n",
    "        if use_gpu:\n",
    "            sequences, pids = sequences.cuda(), pids.cuda()\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        imgs = sequences[:, :3] \n",
    "        head_masks = sequences[:, 3:4] \n",
    "        upper_masks = sequences[:, 4:5] \n",
    "        lower_masks = sequences[:, 5:6] \n",
    "        shoes_masks = sequences[:, 6:7] \n",
    "        \n",
    "        outputs, features, a_head, a_upper, a_lower, a_shoes = model(imgs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        xent_loss = criterion_xent(outputs, pids)\n",
    "        htri_loss = criterion_htri(features, pids)\n",
    "        loss = xent_loss + htri_loss\n",
    "\n",
    "        head_loss = DeepSupervision(criterion_mask, a_head, head_masks)\n",
    "        upper_loss = DeepSupervision(criterion_mask, a_upper, upper_masks)\n",
    "        lower_loss = DeepSupervision(criterion_mask, a_lower, lower_masks)\n",
    "        shoes_loss = DeepSupervision(criterion_mask, a_shoes, shoes_masks)\n",
    "        mask_loss = (head_loss + upper_loss + lower_loss + shoes_loss) / 4.0\n",
    "\n",
    "        total_loss = loss + alpha * mask_loss\n",
    "\n",
    "        # backward + optimize\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        accs.update(torch.sum(preds == pids.data).float()/pids.size(0), pids.size(0))\n",
    "        xent_losses.update(xent_loss.item(), pids.size(0))\n",
    "        htri_losses.update(htri_loss.item(), pids.size(0))\n",
    "        mask_losses.update(mask_loss.item(), pids.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "    print('Epoch{0} '\n",
    "          'Time:{batch_time.sum:.1f}s '\n",
    "          'Data:{data_time.sum:.1f}s '\n",
    "          'xentLoss:{xent_loss.avg:.4f} '\n",
    "          'triLoss:{tri_loss.avg:.4f} '\n",
    "          'MaskLoss:{mask_loss.avg:.4f} '\n",
    "          'Acc:{acc.avg:.2%} '.format(\n",
    "           epoch+1, batch_time=batch_time,\n",
    "           data_time=data_time, xent_loss=xent_losses,\n",
    "           tri_loss=htri_losses, mask_loss=mask_losses, acc=accs))\n",
    "\n",
    "\n",
    "def fliplr(img, use_gpu):\n",
    "    '''flip horizontal'''\n",
    "    inv_idx = torch.arange(img.size(3)-1, -1, -1).long()\n",
    "    if use_gpu: inv_idx = inv_idx.cuda()\n",
    "    img_flip = img.index_select(3, inv_idx)\n",
    "    return img_flip\n",
    "\n",
    "\n",
    "def test(model, queryloader, galleryloader, use_gpu = True, ranks=[1, 5, 10, 20], rerank = rerank):\n",
    "    batch_time = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        qf, q_pids, q_camids = [], [], []\n",
    "        for batch_idx, (imgs, pids, camids) in enumerate(queryloader):\n",
    "            end = time.time()\n",
    "\n",
    "            n, c, h, w = imgs.size()\n",
    "            features = torch.FloatTensor(n, model.module.feat_dim).zero_()\n",
    "            for i in range(flip_cnt):\n",
    "                if (i==1):\n",
    "                    imgs = fliplr(imgs, use_gpu = use_gpu)\n",
    "                f = model(imgs)[1]\n",
    "                f = f.data.cpu()\n",
    "                features = features + f\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "\n",
    "            qf.append(features)\n",
    "            q_pids.extend(pids)\n",
    "            q_camids.extend(camids)\n",
    "        qf = torch.cat(qf, 0)\n",
    "        q_pids = np.asarray(q_pids)\n",
    "        q_camids = np.asarray(q_camids)\n",
    "\n",
    "        print(\"Extracted features for query set, obtained {}-by-{} matrix\".format(qf.size(0), qf.size(1)))\n",
    "\n",
    "        gf, g_pids, g_camids = [], [], []\n",
    "        for batch_idx, (imgs, pids, camids) in enumerate(galleryloader):\n",
    "\n",
    "            end = time.time()\n",
    "\n",
    "            n, c, h, w = imgs.size()\n",
    "            features = torch.FloatTensor(n, model.module.feat_dim).zero_()\n",
    "            for i in range(flip_cnt):\n",
    "                if (i==1):\n",
    "                    imgs = fliplr(imgs, use_gpu = True)\n",
    "                if use_gpu: imgs = imgs.cuda()\n",
    "                f = model(imgs)[1]\n",
    "                f = f.data.cpu()\n",
    "                features = features + f\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "\n",
    "            gf.append(features)\n",
    "            g_pids.extend(pids)\n",
    "            g_camids.extend(camids)\n",
    "        gf = torch.cat(gf, 0)\n",
    "        g_pids = np.asarray(g_pids)\n",
    "        g_camids = np.asarray(g_camids)\n",
    "\n",
    "        print(\"Extracted features for gallery set, obtained {}-by-{} matrix\".format(gf.size(0), gf.size(1)))\n",
    "    \n",
    "    print(\"==> BatchTime(s)/BatchSize(img): {:.3f}/{}\".format(batch_time.avg, test_batch))\n",
    "    \n",
    "    #uncomment this and comment below dismat code before rerank\n",
    "    distmat = compute_distance_matrix(qf, gf, distance)\n",
    "    \n",
    "    #m, n = qf.size(0), gf.size(0)\n",
    "    #distmat = torch.zeros((m, n))\n",
    "    #if distance == 'euclidean':\n",
    "     #   distmat = torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\n",
    "      #            torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\n",
    "       # distmat.addmm_(1, -2, qf, gf.t())\n",
    "    \n",
    "    #else:\n",
    "     #   q_norm = torch.norm(qf, p=2, dim=1, keepdim=True)\n",
    "      #  g_norm = torch.norm(gf, p=2, dim=1, keepdim=True)\n",
    "       # qf = qf.div(q_norm.expand_as(qf))\n",
    "       # gf = gf.div(g_norm.expand_as(gf))\n",
    "       # distmat = - torch.mm(qf, gf.t())\n",
    "    \n",
    "    distmat = distmat.numpy()\n",
    "    \n",
    "    if rerank:\n",
    "        print('Applying person re-ranking ...')\n",
    "        distmat_qq = compute_distance_matrix(qf, qf, distance)\n",
    "        distmat_gg = compute_distance_matrix(gf, gf, distance)\n",
    "        distmat = re_ranking(distmat, distmat_qq, distmat_gg)\n",
    "    \n",
    "    print(\"Computing CMC and mAP\")\n",
    "    use_metric_cuhk03 = False\n",
    "    cmc, mAP = evaluate_model(distmat, q_pids, g_pids, q_camids, g_camids, use_metrics_cuhk03=use_metric_cuhk03)\n",
    "\n",
    "    print(\"Results ----------\")\n",
    "    print(\"mAP: {:.1%}\".format(mAP))\n",
    "    print(\"CMC curve\")\n",
    "    for r in ranks:\n",
    "        print(\"Rank-{:<3}: {:.1%}\".format(r, cmc[r-1]))\n",
    "    print(\"------------------\")\n",
    "\n",
    "    return cmc[0]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zi2WTRNE0O7-"
   },
   "source": [
    "# Visualize Ranked Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "8395988ff3614135a490f40a55697981",
      "44f16670076e4010b53c0f92b92d7638",
      "d4673b50aa2f479f8c845203aa453fa8",
      "b32cc4881fce4a218bd369d2d9da85e2",
      "1119a849165b47c48e4adaa6af3fc25d",
      "95e040781a1441a39dad0d34cf58252a",
      "5b7e7131d91544abb0f67646c442f629",
      "f528d0b2f489458dba43fcec34e5230c",
      "7ad310124eb74562835a07bea40fd9bf",
      "d4b8330a10224bcb999a881f4b957f8a",
      "edeaa1b897d240d3a70377519cb7ec56"
     ]
    },
    "executionInfo": {
     "elapsed": 525773,
     "status": "ok",
     "timestamp": 1641549248192,
     "user": {
      "displayName": "Fatima Zulfiqar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11615102525674283235"
     },
     "user_tz": -300
    },
    "id": "F0M477c20THw",
    "outputId": "2eaea1b8-da28-4535-a184-53b472a90f60"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.transforms as T\n",
    "\n",
    "parser = \"Train image model with cross entropy loss and hard triplet loss\"\n",
    "# Datasets\n",
    "dataset = 'market1501_seg' # default = 'market1501_seg' 'MSMT17_seg', 'DukeMTMC_seg'\n",
    "workers = 4 # default = 4 type = int help = number of data loading workers (default: 4)\"\n",
    "height = 256 # default = 256, \n",
    "width = 128 # default = 128\n",
    "split_id = 0 # default = 0, help = split index \n",
    "\n",
    "    \n",
    "# CUHK03-specific setting\n",
    "cuhk03_labeled = False \n",
    "cuhk03_classic_split = False\n",
    "use_metric_cuhk03 = False\n",
    "\n",
    "# Optimization options\n",
    "optim = 'adam' # default = 'adam'\n",
    "max_epoch = 60 # default = 60    \n",
    "start_epoch = 0 # default = 0 , help = manual epoch number (usefult for restart)   \n",
    "train_batch = 64 # default = 64\n",
    "test_batch = 32 # default = 32    \n",
    "lr = 0.00035 # learning rate, default = 0.00035, type = float\n",
    "stepsize = [20, 40] # stepsize to decay rate    \n",
    "gamma = 0.1 # learning rate decay, default = 0.1    \n",
    "weight_decay = 5e-04  #default = 5e-04\n",
    "margin = 0.3 # margin for triplet loss    \n",
    "num_instances = 4\n",
    "\n",
    "# Architecture\n",
    "arch = 'STCANet'\n",
    "save_dir = './result/market/STCANet' # path to save model checkpoints\n",
    "\n",
    "# spatial attention loss\n",
    "mode = 'ce' #  help='mask loss mode, in {l1, l2, ce}\n",
    "alpha = 0.5 # the weight for the spatial attention loss\n",
    "\n",
    "# data process\n",
    "flip_cnt = 1\n",
    "\n",
    "# Miscs\n",
    "distance = 'cosine'\n",
    "seed = 1\n",
    "resume = '' # set path of saved model weights\n",
    "evaluate = True # evaluate only\n",
    "rerank = False #default = False\n",
    "visrank = True\n",
    "eval_step = 10    \n",
    "start_eval = 0\n",
    "gpu_devices = '2' # GPU device default = 2 or 3\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = gpu_devices\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "\n",
    "    if not evaluate:\n",
    "        sys.stdout = Logger(osp.join(save_dir, 'log_train.txt'))\n",
    "    else:\n",
    "        sys.stdout = Logger(osp.join(save_dir, 'log_test.txt'), mode='a')\n",
    "    #print(\"==========\\nArgs:{}\\n==========\".format(args))\n",
    "\n",
    "    if use_gpu:\n",
    "        print(\"Currently using GPU {}\".format(gpu_devices))\n",
    "        cudnn.benchmark = True\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    else:\n",
    "        print(\"Currently using CPU (GPU is highly recommended)\")\n",
    "\n",
    "    #print(\"Initializing dataset {}\".format(dataset))\n",
    "    datasets = init_imgreid_dataset(\n",
    "        name= dataset, split_id=split_id,\n",
    "        cuhk03_labeled=cuhk03_labeled, cuhk03_classic_split=cuhk03_classic_split,\n",
    "    )\n",
    "\n",
    "    transform_train = Compose([\n",
    "        Scale((height, width), interpolation=3),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        RandomErasing(0.5),\n",
    "    ])\n",
    "\n",
    "    transform_test = T.Compose([\n",
    "        T.Resize((height, width), interpolation=3),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    pin_memory = True if use_gpu else False\n",
    "    #ImageDataset_segs = ImageDataset_seg()\n",
    "    trainloader = DataLoader(\n",
    "        ImageDataset_seg(datasets.train, transform=transform_train),\n",
    "        sampler=RandomIdentitySampler(datasets.train, num_instances=num_instances),\n",
    "        batch_size=train_batch, num_workers=workers,\n",
    "        pin_memory=pin_memory, drop_last=True,\n",
    "    )\n",
    "    #ImageDatasets = ImageDataset()\n",
    "    queryloader = DataLoader(\n",
    "        ImageDataset(datasets.query, transform=transform_test),\n",
    "        batch_size=test_batch, shuffle=False, num_workers=workers,\n",
    "        pin_memory=pin_memory, drop_last=False,\n",
    "    )\n",
    "\n",
    "    galleryloader = DataLoader(\n",
    "        ImageDataset(datasets.gallery, transform=transform_test),\n",
    "        batch_size=test_batch, shuffle=False, num_workers=workers,\n",
    "        pin_memory=pin_memory, drop_last=False,\n",
    "    )\n",
    "\n",
    "    print(\"Initializing model: {}\".format(arch))\n",
    "    model = init_model(name=arch,  num_classes=datasets.num_train_pids) \n",
    "    \n",
    "    print(model)\n",
    "    print(\"Model size: {:.3f} M\".format(count_num_param(model)))\n",
    "    \n",
    "    criterion_xent = CrossEntropyLabelSmooth(num_classes=datasets.num_train_pids, use_gpu=use_gpu)\n",
    "    criterion_htri = TripletLoss(margin=margin, distance=distance)\n",
    "    criterion_mask = MaskLoss(mode=mode) \n",
    "    \n",
    "    optimizer = init_optim(optim, model.parameters(), lr, weight_decay)\n",
    "    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=stepsize, gamma=gamma)\n",
    "\n",
    "    if resume:\n",
    "        if check_isfile(resume):\n",
    "            checkpoint = torch.load(resume)\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            start_epochs = checkpoint['epoch']\n",
    "            rank1 = checkpoint['rank1']\n",
    "            print(\"Loaded checkpoint from '{}'\".format(resume))\n",
    "            print(\"- start_epoch: {}\\n- rank1: {}\".format(start_epochs, rank1))\n",
    "\n",
    "    if use_gpu:\n",
    "        model = nn.DataParallel(model).cuda()\n",
    "\n",
    "    if evaluate:\n",
    "        print(\"Evaluate only\")\n",
    "        #test(model, queryloader, galleryloader, use_gpu = use_gpu)\n",
    "        visualization(model, datasets, queryloader, galleryloader, use_gpu)\n",
    "        return\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_time = 0\n",
    "    best_rank1 = -np.inf\n",
    "    best_epoch = 0\n",
    "    print(\"==> Start training\")\n",
    "\n",
    "    for epoch in range(start_epoch, max_epoch):\n",
    "        scheduler.step()\n",
    "\n",
    "        start_train_time = time.time()\n",
    "        train(epoch, model, criterion_xent, criterion_htri, criterion_mask, optimizer, trainloader)\n",
    "        train_time += round(time.time() - start_train_time)\n",
    "        \n",
    "        if (epoch + 1) % eval_step == 0 or epoch == 0:\n",
    "            print(\"==> Test\")\n",
    "            use_gpu = True\n",
    "            rank1 = test(model, queryloader, galleryloader, use_gpu = use_gpu)\n",
    "            is_best = rank1 > best_rank1\n",
    "            \n",
    "            if is_best:\n",
    "                best_rank1 = rank1\n",
    "                best_epoch = epoch + 1\n",
    "\n",
    "            if use_gpu:\n",
    "                state_dict = model.module.state_dict()\n",
    "            else:\n",
    "                state_dict = model.state_dict()\n",
    "            \n",
    "            save_checkpoint({\n",
    "                'state_dict': state_dict,\n",
    "                'rank1': rank1,\n",
    "                'epoch': epoch,\n",
    "            }, is_best, osp.join(save_dir, 'checkpoint_ep' + str(epoch + 1) + '.pth.tar'))\n",
    "\n",
    "    print(\"==> Best Rank-1 {:.1%}, achieved at epoch {}\".format(best_rank1, best_epoch))\n",
    "\n",
    "    elapsed = round(time.time() - start_time)\n",
    "    elapsed = str(datetime.timedelta(seconds=elapsed))\n",
    "    train_time = str(datetime.timedelta(seconds=train_time))\n",
    "    print(\"Finished. Total elapsed time (h:m:s): {}. Training time (h:m:s): {}.\".format(elapsed, train_time))\n",
    "    #print(\"==========\\nArgs:{}\\n==========\".format(args))\n",
    "\n",
    "\n",
    "def train(epoch, model, criterion_xent, criterion_htri, criterion_mask, optimizer, trainloader, use_gpu=True):\n",
    "    xent_losses = AverageMeter()\n",
    "    htri_losses = AverageMeter()\n",
    "    mask_losses = AverageMeter()\n",
    "    accs = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for batch_idx, (sequences, pids, _) in enumerate(trainloader):\n",
    "        if use_gpu:\n",
    "            sequences, pids = sequences.cuda(), pids.cuda()\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        imgs = sequences[:, :3] \n",
    "        head_masks = sequences[:, 3:4] \n",
    "        upper_masks = sequences[:, 4:5] \n",
    "        lower_masks = sequences[:, 5:6] \n",
    "        shoes_masks = sequences[:, 6:7] \n",
    "        \n",
    "        outputs, features, a_head, a_upper, a_lower, a_shoes = model(imgs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        xent_loss = criterion_xent(outputs, pids)\n",
    "        htri_loss = criterion_htri(features, pids)\n",
    "        loss = xent_loss + htri_loss\n",
    "\n",
    "        head_loss = DeepSupervision(criterion_mask, a_head, head_masks)\n",
    "        upper_loss = DeepSupervision(criterion_mask, a_upper, upper_masks)\n",
    "        lower_loss = DeepSupervision(criterion_mask, a_lower, lower_masks)\n",
    "        shoes_loss = DeepSupervision(criterion_mask, a_shoes, shoes_masks)\n",
    "        mask_loss = (head_loss + upper_loss + lower_loss + shoes_loss) / 4.0\n",
    "\n",
    "        total_loss = loss + alpha * mask_loss\n",
    "\n",
    "        # backward + optimize\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        accs.update(torch.sum(preds == pids.data).float()/pids.size(0), pids.size(0))\n",
    "        xent_losses.update(xent_loss.item(), pids.size(0))\n",
    "        htri_losses.update(htri_loss.item(), pids.size(0))\n",
    "        mask_losses.update(mask_loss.item(), pids.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "    print('Epoch{0} '\n",
    "          'Time:{batch_time.sum:.1f}s '\n",
    "          'Data:{data_time.sum:.1f}s '\n",
    "          'xentLoss:{xent_loss.avg:.4f} '\n",
    "          'triLoss:{tri_loss.avg:.4f} '\n",
    "          'MaskLoss:{mask_loss.avg:.4f} '\n",
    "          'Acc:{acc.avg:.2%} '.format(\n",
    "           epoch+1, batch_time=batch_time,\n",
    "           data_time=data_time, xent_loss=xent_losses,\n",
    "           tri_loss=htri_losses, mask_loss=mask_losses, acc=accs))\n",
    "\n",
    "\n",
    "def fliplr(img, use_gpu):\n",
    "    '''flip horizontal'''\n",
    "    inv_idx = torch.arange(img.size(3)-1, -1, -1).long()\n",
    "    if use_gpu: inv_idx = inv_idx.cuda()\n",
    "    img_flip = img.index_select(3, inv_idx)\n",
    "    return img_flip\n",
    "\n",
    "\n",
    "def test(model, queryloader, galleryloader, use_gpu = True, ranks=[1, 5, 10, 20], rerank = rerank):\n",
    "    batch_time = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        qf, q_pids, q_camids = [], [], []\n",
    "        for batch_idx, (imgs, pids, camids) in enumerate(queryloader):\n",
    "            end = time.time()\n",
    "\n",
    "            n, c, h, w = imgs.size()\n",
    "            features = torch.FloatTensor(n, model.module.feat_dim).zero_()\n",
    "            for i in range(flip_cnt):\n",
    "                if (i==1):\n",
    "                    imgs = fliplr(imgs, use_gpu = use_gpu)\n",
    "                f = model(imgs)[1]\n",
    "                f = f.data.cpu()\n",
    "                features = features + f\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "\n",
    "            qf.append(features)\n",
    "            q_pids.extend(pids)\n",
    "            q_camids.extend(camids)\n",
    "        qf = torch.cat(qf, 0)\n",
    "        q_pids = np.asarray(q_pids)\n",
    "        q_camids = np.asarray(q_camids)\n",
    "\n",
    "        print(\"Extracted features for query set, obtained {}-by-{} matrix\".format(qf.size(0), qf.size(1)))\n",
    "\n",
    "        gf, g_pids, g_camids = [], [], []\n",
    "        for batch_idx, (imgs, pids, camids) in enumerate(galleryloader):\n",
    "\n",
    "            end = time.time()\n",
    "\n",
    "            n, c, h, w = imgs.size()\n",
    "            features = torch.FloatTensor(n, model.module.feat_dim).zero_()\n",
    "            for i in range(flip_cnt):\n",
    "                if (i==1):\n",
    "                    imgs = fliplr(imgs, use_gpu = True)\n",
    "                if use_gpu: imgs = imgs.cuda()\n",
    "                f = model(imgs)[1]\n",
    "                f = f.data.cpu()\n",
    "                features = features + f\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "\n",
    "            gf.append(features)\n",
    "            g_pids.extend(pids)\n",
    "            g_camids.extend(camids)\n",
    "        gf = torch.cat(gf, 0)\n",
    "        g_pids = np.asarray(g_pids)\n",
    "        g_camids = np.asarray(g_camids)\n",
    "\n",
    "        print(\"Extracted features for gallery set, obtained {}-by-{} matrix\".format(gf.size(0), gf.size(1)))\n",
    "    \n",
    "    print(\"==> BatchTime(s)/BatchSize(img): {:.3f}/{}\".format(batch_time.avg, test_batch))\n",
    "    \n",
    "    #uncomment this and comment below dismat code before rerank\n",
    "    distmat = compute_distance_matrix(qf, gf, distance)\n",
    "    \n",
    "    #m, n = qf.size(0), gf.size(0)\n",
    "    #distmat = torch.zeros((m, n))\n",
    "    #if distance == 'euclidean':\n",
    "     #   distmat = torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\n",
    "      #            torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\n",
    "       # distmat.addmm_(1, -2, qf, gf.t())\n",
    "    \n",
    "    #else:\n",
    "     #   q_norm = torch.norm(qf, p=2, dim=1, keepdim=True)\n",
    "      #  g_norm = torch.norm(gf, p=2, dim=1, keepdim=True)\n",
    "       # qf = qf.div(q_norm.expand_as(qf))\n",
    "       # gf = gf.div(g_norm.expand_as(gf))\n",
    "       # distmat = - torch.mm(qf, gf.t())\n",
    "    \n",
    "    distmat = distmat.numpy()\n",
    "    \n",
    "    if rerank:\n",
    "        print('Applying person re-ranking ...')\n",
    "        distmat_qq = compute_distance_matrix(qf, qf, distance)\n",
    "        distmat_gg = compute_distance_matrix(gf, gf, distance)\n",
    "        distmat = re_ranking(distmat, distmat_qq, distmat_gg)\n",
    "    \n",
    "  \n",
    "    print(\"Computing CMC and mAP\")\n",
    "    use_metric_cuhk03 = False\n",
    "    cmc, mAP = evaluate_model(distmat, q_pids, g_pids, q_camids, g_camids, use_metrics_cuhk03=use_metric_cuhk03)\n",
    "\n",
    "    print(\"Results ----------\")\n",
    "    print(\"mAP: {:.1%}\".format(mAP))\n",
    "    print(\"CMC curve\")\n",
    "    for r in ranks:\n",
    "        print(\"Rank-{:<3}: {:.1%}\".format(r, cmc[r-1]))\n",
    "    print(\"------------------\")\n",
    "\n",
    "  \n",
    "    return cmc[0]\n",
    "\n",
    "def visualization(model, datasets, queryloader, galleryloader, use_gpu = True, ranks=[1, 5, 10, 20], rerank = rerank):\n",
    "    batch_time = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        qf, q_pids, q_camids = [], [], []\n",
    "        for batch_idx, (imgs, pids, camids) in enumerate(queryloader):\n",
    "            end = time.time()\n",
    "\n",
    "            n, c, h, w = imgs.size()\n",
    "            features = torch.FloatTensor(n, model.module.feat_dim).zero_()\n",
    "            for i in range(flip_cnt):\n",
    "                if (i==1):\n",
    "                    imgs = fliplr(imgs, use_gpu = use_gpu)\n",
    "                f = model(imgs)[1]\n",
    "                f = f.data.cpu()\n",
    "                features = features + f\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "\n",
    "            qf.append(features)\n",
    "            q_pids.extend(pids)\n",
    "            q_camids.extend(camids)\n",
    "        qf = torch.cat(qf, 0)\n",
    "        q_pids = np.asarray(q_pids)\n",
    "        q_camids = np.asarray(q_camids)\n",
    "\n",
    "        print(\"Extracted features for query set, obtained {}-by-{} matrix\".format(qf.size(0), qf.size(1)))\n",
    "\n",
    "        gf, g_pids, g_camids = [], [], []\n",
    "        for batch_idx, (imgs, pids, camids) in enumerate(galleryloader):\n",
    "\n",
    "            end = time.time()\n",
    "\n",
    "            n, c, h, w = imgs.size()\n",
    "            features = torch.FloatTensor(n, model.module.feat_dim).zero_()\n",
    "            for i in range(flip_cnt):\n",
    "                if (i==1):\n",
    "                    imgs = fliplr(imgs, use_gpu = True)\n",
    "                if use_gpu: imgs = imgs.cuda()\n",
    "                f = model(imgs)[1]\n",
    "                f = f.data.cpu()\n",
    "                features = features + f\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "\n",
    "            gf.append(features)\n",
    "            g_pids.extend(pids)\n",
    "            g_camids.extend(camids)\n",
    "        gf = torch.cat(gf, 0)\n",
    "        g_pids = np.asarray(g_pids)\n",
    "        g_camids = np.asarray(g_camids)\n",
    "\n",
    "        print(\"Extracted features for gallery set, obtained {}-by-{} matrix\".format(gf.size(0), gf.size(1)))\n",
    "    \n",
    "    print(\"==> BatchTime(s)/BatchSize(img): {:.3f}/{}\".format(batch_time.avg, test_batch))\n",
    "    \n",
    "    #uncomment this and comment below dismat code before rerank\n",
    "    distmat = compute_distance_matrix(qf, gf, distance)\n",
    "    \n",
    "    #m, n = qf.size(0), gf.size(0)\n",
    "    #distmat = torch.zeros((m, n))\n",
    "    #if distance == 'euclidean':\n",
    "     #   distmat = torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\n",
    "      #            torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\n",
    "       # distmat.addmm_(1, -2, qf, gf.t())\n",
    "    \n",
    "    #else:\n",
    "     #   q_norm = torch.norm(qf, p=2, dim=1, keepdim=True)\n",
    "      #  g_norm = torch.norm(gf, p=2, dim=1, keepdim=True)\n",
    "       # qf = qf.div(q_norm.expand_as(qf))\n",
    "       # gf = gf.div(g_norm.expand_as(gf))\n",
    "       # distmat = - torch.mm(qf, gf.t())\n",
    "    \n",
    "    distmat = distmat.numpy()\n",
    "    \n",
    "    if rerank:\n",
    "        print('Applying person re-ranking ...')\n",
    "        distmat_qq = compute_distance_matrix(qf, qf, distance)\n",
    "        distmat_gg = compute_distance_matrix(gf, gf, distance)\n",
    "        distmat = re_ranking(distmat, distmat_qq, distmat_gg)\n",
    "    \n",
    "    if visrank:\n",
    "      print(\"Visualize ranked results\")\n",
    "      #ImageDataset(datasets.query, transform=transform_test)\n",
    "      visualize_ranked_results(distmat, datasets)\n",
    "      print(\"Done\")\n",
    "      \n",
    "    print(\"Computing CMC and mAP\")\n",
    "    use_metric_cuhk03 = False\n",
    "    cmc, mAP = evaluate_model(distmat, q_pids, g_pids, q_camids, g_camids, use_metrics_cuhk03=use_metric_cuhk03)\n",
    "\n",
    "    print(\"Results ----------\")\n",
    "    print(\"mAP: {:.1%}\".format(mAP))\n",
    "    print(\"CMC curve\")\n",
    "    for r in ranks:\n",
    "        print(\"Rank-{:<3}: {:.1%}\".format(r, cmc[r-1]))\n",
    "    print(\"------------------\")\n",
    "\n",
    "            \n",
    "    return cmc[0]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "qOgfkTmwAeiX",
    "AisO9uFjAeig",
    "fIBR4kDEAeih",
    "hH6H44kBAeim"
   ],
   "machine_shape": "hm",
   "name": "person-reid-iaunet-re-rank.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "073d84ef4b0b4eadbea0a6eff29b0b7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_740e0a748f2d49b99b1277f185a31286",
       "IPY_MODEL_695993973c6a49febcc90eb8d3884d96"
      ],
      "layout": "IPY_MODEL_1706b10464434f5c9ec257832302ce53"
     }
    },
    "092c3a665fd545c08ed769cdb43eee50": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1119a849165b47c48e4adaa6af3fc25d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_edeaa1b897d240d3a70377519cb7ec56",
      "placeholder": "",
      "style": "IPY_MODEL_d4b8330a10224bcb999a881f4b957f8a",
      "value": " 97.8M/97.8M [00:01&lt;00:00, 130MB/s]"
     }
    },
    "1706b10464434f5c9ec257832302ce53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a4db6771dc3471bafdcc495158cccf7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "44f16670076e4010b53c0f92b92d7638": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b7e7131d91544abb0f67646c442f629": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "695993973c6a49febcc90eb8d3884d96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe99daac1bf4459580abbe4c3b1a36cf",
      "placeholder": "",
      "style": "IPY_MODEL_bdf683f21f25460aadf588bac21c56de",
      "value": " 97.8M/97.8M [00:07&lt;00:00, 13.8MB/s]"
     }
    },
    "740e0a748f2d49b99b1277f185a31286": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_092c3a665fd545c08ed769cdb43eee50",
      "max": 102502400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3a4db6771dc3471bafdcc495158cccf7",
      "value": 102502400
     }
    },
    "7ad310124eb74562835a07bea40fd9bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8395988ff3614135a490f40a55697981": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d4673b50aa2f479f8c845203aa453fa8",
       "IPY_MODEL_b32cc4881fce4a218bd369d2d9da85e2",
       "IPY_MODEL_1119a849165b47c48e4adaa6af3fc25d"
      ],
      "layout": "IPY_MODEL_44f16670076e4010b53c0f92b92d7638"
     }
    },
    "95e040781a1441a39dad0d34cf58252a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b32cc4881fce4a218bd369d2d9da85e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ad310124eb74562835a07bea40fd9bf",
      "max": 102502400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f528d0b2f489458dba43fcec34e5230c",
      "value": 102502400
     }
    },
    "bdf683f21f25460aadf588bac21c56de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d4673b50aa2f479f8c845203aa453fa8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b7e7131d91544abb0f67646c442f629",
      "placeholder": "",
      "style": "IPY_MODEL_95e040781a1441a39dad0d34cf58252a",
      "value": "100%"
     }
    },
    "d4b8330a10224bcb999a881f4b957f8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "edeaa1b897d240d3a70377519cb7ec56": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f528d0b2f489458dba43fcec34e5230c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fe99daac1bf4459580abbe4c3b1a36cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
